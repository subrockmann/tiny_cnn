{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os, sys, math, datetime, csv\n",
    "import psutil\n",
    "\n",
    "# import pathlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import PIL\n",
    "\n",
    "import PIL.Image\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%reload_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# see https://github.com/microsoft/pylance-release/issues/1066\n",
    "#from tensorflow import keras\n",
    "keras = tf.keras\n",
    "from keras.layers import (\n",
    "    Input,\n",
    "    Dense,\n",
    "    Add,\n",
    "    Flatten,\n",
    "    Conv2D,\n",
    "    Dropout,\n",
    "    Reshape,\n",
    "    Activation,\n",
    "    DepthwiseConv2D,\n",
    "    MaxPooling2D,\n",
    "    AvgPool2D,\n",
    "    AveragePooling2D,\n",
    "    GlobalAveragePooling2D,\n",
    "    Softmax,\n",
    "    BatchNormalization,\n",
    "    Concatenate,\n",
    "    Permute,\n",
    "    ReLU\n",
    ")\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l2\n",
    "#from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "#from dotenv import load_dotenv\n",
    "#from tf2cv.model_provider import get_model as tf2cv_get_model\n",
    "\n",
    "# Import the necessary MLTK APIs\n",
    "from mltk.core import view_model, summarize_model, profile_model\n",
    "\n",
    "from workbench.config.config import initialize\n",
    "from workbench.utils.utils import create_filepaths, get_file_size, create_model_name, append_dict_to_csv\n",
    "from workbench.tensorflow import set_batchnorm_momentum, set_dropout\n",
    "from workbench.data.data import get_vvw_dataset, get_vvw_minval_dataset, get_lemon_binary_datagen\n",
    "\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "# import deeplake\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %reload_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_1 = 1\n",
    "seed_2 = 15\n",
    "seed_3 = 30\n",
    "seed_4 = 42\n",
    "seed_5 = 75\n",
    "\n",
    "seed = seed_1\n",
    "\n",
    "# set the random seeds#\n",
    "#os.environ[\"TF_CUDNN_DETERMINISTIC\"]= \"1\"\n",
    "# random.seed(seed)\n",
    "# np.random.seed(seed)\n",
    "# tf.random.set_seed(seed) # setting tensorflow global seed\n",
    "tf.keras.utils.set_random_seed(seed)\n",
    "#tf.config.experimental.enable_op_determinism()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "automated = False\n",
    "\n",
    "\n",
    "img_res = 96\n",
    "channels = 3\n",
    "classes = 2\n",
    "alpha = 0.25\n",
    "global dropout_rate\n",
    "dropout_rate = 0.2 # 0.2\n",
    "architecture = \"shufflenet_v1\" #\"mobilenet_v1\"#shufflenet_v2tiny  # \"mobilenet_v1_vvw\"#  \"mobilenet_v3_small_keras\" ##\"shufflenet_v2tiny\" # ##\"mobilenet_v3_large_keras\"  #\"efficientNetB0_keras\" #\"mobilenet_v2_keras\" #\n",
    "loop_depth = 5\n",
    "head = \"GAP\"\n",
    "groups = 2 # [1, 2, 3, 4, 8] used in shuffleNetv1\n",
    "first_layer_channels = 24 # standard 24\n",
    "last_layer_channels = 1024 # standard 1024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (img_res,img_res,channels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_batchnorm_momentum(model, momentum=0.9):\n",
    "    for layer in model.layers:\n",
    "        if type(layer)==type(tf.keras.layers.BatchNormalization()):\n",
    "            #print(layer.momentum)\n",
    "            layer.momentum=momentum\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MobileNet V1 & V2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fast downsampling mobilenet https://github.com/qinzheng93/FD-MobileNet/blob/master/pyvision/models/ImageNet/MobileNet.py  \n",
    "\n",
    "Keras Effnet https://github.com/arthurdouillard/keras-effnet/blob/master/effnet.py  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mobilenet_v1(input_shape, classes, alpha=1, loop_depth=5, head=\"GAP\"):\n",
    "    \"\"\"\n",
    "    This function builds a CNN model according to the MobileNet V1 specification, using the functional API.\n",
    "    The function returns the model.\n",
    "    \"\"\"\n",
    "    global dropout_rate\n",
    "    \n",
    "    # MobileNet V1 Block\n",
    "    def mobilenet_v1_block(x, filters, strides):\n",
    "        # Depthwise convolution\n",
    "        x = DepthwiseConv2D(kernel_size=3, strides=strides, padding=\"same\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = ReLU()(x)  # TODO: option to change to ReLu6 or HardSwish\n",
    "\n",
    "        # Pointwise convolution = standard convolution with kernel size =1\n",
    "        x = Conv2D(filters=filters, kernel_size=1, strides=1)(x)  # strides for pointwise convolution must be 1\n",
    "        x = BatchNormalization()(x)\n",
    "        x = ReLU()(x)  # TODO: option to change to ReLu6 or HardSwish\n",
    "\n",
    "        return x\n",
    "\n",
    "    # Stem of the model\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Conv2D(filters=32 * alpha, kernel_size=3, strides=2, padding=\"same\")(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)  # TODO: option to change to ReLu6 or HardSwish\n",
    "\n",
    "    # Main part of the model\n",
    "    x = mobilenet_v1_block(x, filters=64 * alpha, strides=1)\n",
    "    x = mobilenet_v1_block(x, filters=128 * alpha, strides=2)\n",
    "    x = mobilenet_v1_block(x, filters=128 * alpha, strides=1)\n",
    "    x = mobilenet_v1_block(x, filters=256 * alpha, strides=2)\n",
    "    x = mobilenet_v1_block(x, filters=256 * alpha, strides=1)\n",
    "    x = mobilenet_v1_block(x, filters=512 * alpha, strides=2)\n",
    "\n",
    "    for _ in range(loop_depth):  # TODO: reduce the depth of the net for faster inference\n",
    "        x = mobilenet_v1_block(x, filters=512 * alpha, strides=1)\n",
    "\n",
    "    x = mobilenet_v1_block(x, filters=1024 * alpha, strides=2)\n",
    "    x = mobilenet_v1_block(x, filters=1024 * alpha, strides=1)\n",
    "\n",
    "    if head==\"GAP\":  \n",
    "        x = GlobalAveragePooling2D(keepdims=True)(x) #\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "        x = Conv2D(filters=classes, kernel_size=1, strides=1, padding=\"same\")(x)\n",
    "        #x = Reshape(target_shape=(classes,))(x)\n",
    "        x = Flatten()(x)\n",
    "        outputs = Activation(\"softmax\")(x)\n",
    "\n",
    "    elif head==\"MV1\":\n",
    "   \n",
    "        # use the original implementation from the paper with average pooling and fully-connected layers\n",
    "        x = AvgPool2D(pool_size=x.shape[1:3], strides=1)(x)  # TODO: pool_size is dependent on the input resolution, lower resolutions than 224 might crash the architecture\n",
    "        #x = Flatten()(x)\n",
    "        outputs = Dense(units=classes, activation=\"softmax\")(x)  \n",
    "\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "\n",
    "        ####\n",
    "            # Average pooling, max polling may be used also\n",
    "    # Keras employs GlobalAveragePooling2D \n",
    "    #x = AveragePooling2D(pool_size=x.shape[1:3])(x)\n",
    "    #x = MaxPooling2D(pool_size=x.shape[1:3])(x)\n",
    "\n",
    "    # Keras inserts Dropout() and a pointwise Conv2D() here\n",
    "    # We are staying with the paper base structure\n",
    "\n",
    "    # Flatten, FC layer and classify\n",
    "    #x = Flatten()(x)\n",
    "    #outputs = Dense(classes, activation='softmax')(x)\n",
    "        ####\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs, name=\"mobilenetv1\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "MobilnetV1 from Silican Labs github page:\n",
    "https://github.com/SiliconLabs/platform_ml_models/blob/master/eembc/Person_detection/mobilenet_v1_eembc.py\n",
    "'''\n",
    "\n",
    "def mobilenet_v1_vvw(input_shape, classes, alpha=1, loop_depth=5, global_average_pooling=True):\n",
    "    # Mobilenet parameters\n",
    "    #input_shape = [96,96,3] # resized to 96x96 per EEMBC requirement\n",
    "    #num_classes = 2 # person and non-person\n",
    "    num_filters = 8 # normally 32, but running with alpha=.25 per EEMBC requirement\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = inputs # Keras model uses ZeroPadding2D()\n",
    "\n",
    "    # 1st layer, pure conv\n",
    "    # Keras 2.2 model has padding='valid' and disables bias\n",
    "    x = Conv2D(num_filters,\n",
    "                  kernel_size=3,\n",
    "                  strides=2,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x) # Keras uses ReLU6 instead of pure ReLU\n",
    "\n",
    "    # 2nd layer, depthwise separable conv\n",
    "    # Filter size is always doubled before the pointwise conv\n",
    "    # Keras uses ZeroPadding2D() and padding='valid'\n",
    "    x = DepthwiseConv2D(kernel_size=3,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    num_filters = 2*num_filters\n",
    "    x = Conv2D(num_filters,\n",
    "                  kernel_size=1,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # 3rd layer, depthwise separable conv\n",
    "    x = DepthwiseConv2D(kernel_size=3,\n",
    "                  strides=2,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    num_filters = 2*num_filters\n",
    "    x = Conv2D(num_filters,\n",
    "                  kernel_size=1,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # 4th layer, depthwise separable conv\n",
    "    x = DepthwiseConv2D(kernel_size=3,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(num_filters,\n",
    "                  kernel_size=1,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # 5th layer, depthwise separable conv\n",
    "    x = DepthwiseConv2D(kernel_size=3,\n",
    "                  strides=2,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    num_filters = 2*num_filters\n",
    "    x = Conv2D(num_filters,\n",
    "                  kernel_size=1,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # 6th layer, depthwise separable conv\n",
    "    x = DepthwiseConv2D(kernel_size=3,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(num_filters,\n",
    "                  kernel_size=1,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # 7th layer, depthwise separable conv\n",
    "    x = DepthwiseConv2D(kernel_size=3,\n",
    "                  strides=2,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    num_filters = 2*num_filters\n",
    "    x = Conv2D(num_filters,\n",
    "                  kernel_size=1,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # 8th-12th layers, identical depthwise separable convs\n",
    "    # 8th\n",
    "    x = DepthwiseConv2D(kernel_size=3,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(num_filters,\n",
    "                  kernel_size=1,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # 9th\n",
    "    x = DepthwiseConv2D(kernel_size=3,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(num_filters,\n",
    "                  kernel_size=1,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # 10th\n",
    "    x = DepthwiseConv2D(kernel_size=3,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(num_filters,\n",
    "                  kernel_size=1,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # 11th\n",
    "    x = DepthwiseConv2D(kernel_size=3,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(num_filters,\n",
    "                  kernel_size=1,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # 12th\n",
    "    x = DepthwiseConv2D(kernel_size=3,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(num_filters,\n",
    "                  kernel_size=1,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # 13th layer, depthwise separable conv\n",
    "    x = DepthwiseConv2D(kernel_size=3,\n",
    "                  strides=2,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    num_filters = 2*num_filters\n",
    "    x = Conv2D(num_filters,\n",
    "                  kernel_size=1,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # 14th layer, depthwise separable conv\n",
    "    x = DepthwiseConv2D(kernel_size=3,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(num_filters,\n",
    "                  kernel_size=1,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # Average pooling, max polling may be used also\n",
    "    # Keras employs GlobalAveragePooling2D \n",
    "    x = AveragePooling2D(pool_size=x.shape[1:3])(x)\n",
    "    #x = MaxPooling2D(pool_size=x.shape[1:3])(x)\n",
    "\n",
    "    # Keras inserts Dropout() and a pointwise Conv2D() here\n",
    "    # We are staying with the paper base structure\n",
    "\n",
    "    # Flatten, FC layer and classify\n",
    "    x = Flatten()(x)\n",
    "    outputs = Dense(classes, activation='softmax')(x)\n",
    "\n",
    "    # Instantiate model.\n",
    "   \n",
    "    model = Model(inputs=inputs, outputs=outputs, name=\"mobilenetv1vvw\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### MOBILENET V2 not yet correctly implemented!\n",
    "\n",
    "\n",
    "# Formular to avoid exccessive downsampling\n",
    "# copied from: https://github.com/keras-team/keras/blob/v2.11.0/keras/applications/mobilenet_v2.py#L563\n",
    "\n",
    "def _make_divisible(v, divisor, min_value=None):\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "\n",
    "\n",
    "def mobilenet_v2(input_shape, classes, alpha=1, loop_depth=5, global_average_pooling=True):\n",
    "    \"\"\"\n",
    "    This function builds a CNN model according to the MobileNet V2 specification, using the functional API.\n",
    "    The function returns the model.\n",
    "    \"\"\"\n",
    "\n",
    "    global block_id\n",
    "    block_id = 1\n",
    "    global dropout_rate\n",
    "\n",
    "    # Expansion block\n",
    "    def expansion_block(x, t, filters, block_id):\n",
    "        prefix = f\"block_{block_id}_\"\n",
    "        x = Conv2D(kernel_size=1, filters = filters * t, use_bias=False, name= prefix + \"expand\")(x)\n",
    "        x = BatchNormalization(name = prefix + \"expand_bn\")(x)\n",
    "        x = ReLU(6, name=prefix + \"expand_relu\")(x)\n",
    "        return x\n",
    "\n",
    "    def depthwise_block(x, strides, block_id):\n",
    "        prefix = f\"block_{block_id}_\"\n",
    "        x = DepthwiseConv2D(kernel_size=3, strides=strides, padding=\"same\", use_bias=False, name= prefix + \"dw_conv\")(x)\n",
    "        x = BatchNormalization(name = prefix + \"dw_bn\")(x)\n",
    "        x = ReLU(6, name=prefix + \"dw_relu\")(x)\n",
    "        return x\n",
    "\n",
    "    def projection_block(x, out_channels, block_id):\n",
    "        prefix = f\"block_{block_id}_\"\n",
    "        x = Conv2D(filters=out_channels, kernel_size=1, strides=1, padding=\"same\", use_bias=False, name= prefix + \"compress\")(x)\n",
    "        x = BatchNormalization(name = prefix + \"compress_bn\")(x)\n",
    "        return x\n",
    "\n",
    "    def bottleneck_residual_block(x, t, filters, out_channels, strides):\n",
    "        global block_id\n",
    "        block_id =  block_id +1\n",
    "        y = expansion_block(x, t, filters, block_id)\n",
    "        y = depthwise_block(y, strides, block_id)\n",
    "        y = projection_block(y, out_channels, block_id)\n",
    "\n",
    "        # TODO: Check if this implementation is ok\n",
    "        if y.shape[-1] == x.shape[-1]:\n",
    "            y = Add()([x, y])\n",
    "        return y\n",
    "\n",
    "    # Avoid massive downsampling\n",
    "    first_block_filters = _make_divisible(32 * alpha, 8)\n",
    "\n",
    "    # Stem of the model\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Conv2D(filters=first_block_filters, kernel_size=3, strides=2, padding=\"same\", use_bias=False)(inputs)\n",
    "    x = BatchNormalization(name='conv1_bn')(x)\n",
    "    x = ReLU(6, name = 'conv1_relu')(x)  # TODO: option to change to ReLu6 or HardSwish\n",
    "\n",
    "\n",
    "    # Main part of the model\n",
    "    block_id = 0\n",
    "    x = depthwise_block(x, strides=1, block_id=block_id)\n",
    "    x = projection_block(x, out_channels=16 * alpha, block_id=block_id)\n",
    "\n",
    "    # 2 identical layers\n",
    "    x  = bottleneck_residual_block(x, t=6, filters=x.shape[-1], out_channels=24 * alpha, strides=2) #, block_id=block_id +1)\n",
    "    x  = bottleneck_residual_block(x, t=6, filters=x.shape[-1], out_channels=24 * alpha, strides=1) #, block_id=block_id +1)\n",
    "\n",
    "    # 3 repeated bottle_necks\n",
    "    x = bottleneck_residual_block(x, t=6, filters=x.shape[-1], out_channels=32 * alpha, strides=2) #, block_id=block_id +1)   \n",
    "    x = bottleneck_residual_block(x, t=6, filters=x.shape[-1], out_channels=32 * alpha, strides=1) #, block_id=block_id +1)   \n",
    "    x = bottleneck_residual_block(x, t=6, filters=x.shape[-1], out_channels=32 * alpha, strides=1) #, block_id=block_id +1)   \n",
    "\n",
    "    # 4 repeated bottle_necks\n",
    "    x = bottleneck_residual_block(x, t=6, filters=x.shape[-1], out_channels=64 * alpha, strides=2) #, block_id=block_id +1)   \n",
    "    x = bottleneck_residual_block(x, t=6, filters=x.shape[-1], out_channels=64 * alpha, strides=1) #, block_id=block_id +1)   \n",
    "    x = bottleneck_residual_block(x, t=6, filters=x.shape[-1], out_channels=64 * alpha, strides=1) #, block_id=block_id +1)   \n",
    "    x = bottleneck_residual_block(x, t=6, filters=x.shape[-1], out_channels=64 * alpha, strides=1) #, block_id=block_id +1)   \n",
    "\n",
    "    # 3 repeated bottle_necks\n",
    "    x = bottleneck_residual_block(x, t=6, filters=x.shape[-1], out_channels=96 * alpha, strides=1) #, block_id=block_id +1)   \n",
    "    x = bottleneck_residual_block(x, t=6, filters=x.shape[-1], out_channels=96 * alpha, strides=1) #, block_id=block_id +1)   \n",
    "    x = bottleneck_residual_block(x, t=6, filters=x.shape[-1], out_channels=96 * alpha, strides=1) #, block_id=block_id +1)   \n",
    "\n",
    "    # 3 repeated bottle_necks\n",
    "    x = bottleneck_residual_block(x, t=6, filters=x.shape[-1], out_channels=160 * alpha, strides=2) #, block_id=block_id +1)   \n",
    "    x = bottleneck_residual_block(x, t=6, filters=x.shape[-1], out_channels=160 * alpha, strides=1) #, block_id=block_id +1)   \n",
    "    x = bottleneck_residual_block(x, t=6, filters=x.shape[-1], out_channels=160 * alpha, strides=1) #, block_id=block_id +1)   \n",
    "\n",
    "    x = bottleneck_residual_block(x, t=6, filters=x.shape[-1], out_channels=320 * alpha, strides=1) #, block_id=block_id +1) \n",
    "\n",
    "\n",
    "    # no alpha applied to last conv as stated in the paper:\n",
    "    # if the width multiplier is greater than 1 we increase the number of output\n",
    "    # channels.\n",
    "    if alpha > 1.0:\n",
    "        last_block_filters = _make_divisible(1280 * alpha, 8)\n",
    "    else:\n",
    "        last_block_filters = 1280\n",
    "\n",
    "    # 1*1 conv\n",
    "    x = Conv2D(filters=last_block_filters, kernel_size=1, padding=\"same\", use_bias=False, name=\"last_conv\")(x)\n",
    "    x = BatchNormalization(name='last_bn')(x)\n",
    "    x = ReLU(6,name='last_relu')(x)\n",
    "\n",
    "\n",
    "    if global_average_pooling:  \n",
    "        x = GlobalAveragePooling2D(keepdims=True)(x)\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "        x = Conv2D(filters=classes, kernel_size=1, strides=1, padding=\"same\")(x)\n",
    "        x = Reshape(target_shape=(classes,))(x)\n",
    "        #outputs = Activation(\"softmax\")(x)\n",
    "        outputs = Softmax()(x)\n",
    "\n",
    "\n",
    "    else:\n",
    "        # use the original implementation from the paper with average pooling and fully-connected layers\n",
    "        x = AvgPool2D(pool_size=7, strides=1)(x)  # TODO: pool_size is dependent on the input resolution, lower resolutions than 224 might crash the architecture\n",
    "        outputs = Dense(units=classes, activation=\"softmax\")(x)  # TODO: is there a stride=1 implementation in Dense?\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs, name=\"mobilenetv2\")\n",
    "\n",
    "    # https://github.com/tensorflow/tensorflow/issues/36065\n",
    "    # model in the original version does not train because momentum is set to 0.999 by default\n",
    "    model = set_batchnorm_momentum(model, momentum=0.9)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MY VERSION OF MOBILENET V2\n",
    "\n",
    "# Formular to avoid exccessive downsampling\n",
    "# copied from: https://github.com/keras-team/keras/blob/v2.11.0/keras/applications/mobilenet_v2.py#L563\n",
    "\n",
    "def _make_divisible(v, divisor, min_value=None):\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "\n",
    "\n",
    "def mobilenet_vme(input_shape, classes, alpha=1, loop_depth=5, global_average_pooling=True):\n",
    "    \"\"\"\n",
    "    This function builds a CNN model according to the MobileNet V2 specification, using the functional API.\n",
    "    The function returns the model.\n",
    "    \"\"\"\n",
    "\n",
    "    global block_id\n",
    "    block_id = 1\n",
    "    global dropout_rate\n",
    "\n",
    "    # Expansion block\n",
    "    def expansion_block(x, t, filters, block_id):\n",
    "        prefix = f\"block_{block_id}_\"\n",
    "        x = Conv2D(kernel_size=1, filters = filters * t, use_bias=False, name= prefix + \"expand\")(x)\n",
    "        x = BatchNormalization(name = prefix + \"expand_bn\")(x)\n",
    "        x = ReLU(6, name=prefix + \"expand_relu\")(x)\n",
    "        return x\n",
    "\n",
    "    def depthwise_block(x, strides, block_id):\n",
    "        prefix = f\"block_{block_id}_\"\n",
    "        x = DepthwiseConv2D(kernel_size=3, strides=strides, padding=\"same\", use_bias=False, name= prefix + \"dw_conv\")(x)\n",
    "        x = BatchNormalization(name = prefix + \"dw_bn\")(x)\n",
    "        x = ReLU(6, name=prefix + \"dw_relu\")(x)\n",
    "        return x\n",
    "\n",
    "    def projection_block(x, out_channels, block_id):\n",
    "        prefix = f\"block_{block_id}_\"\n",
    "        x = Conv2D(filters=out_channels, kernel_size=1, strides=1, padding=\"same\", use_bias=False, name= prefix + \"compress\")(x)\n",
    "        x = BatchNormalization(name = prefix + \"compress_bn\")(x)\n",
    "        return x\n",
    "\n",
    "    def bottleneck_residual_block(x, t, filters, out_channels, strides):\n",
    "        global block_id\n",
    "        block_id =  block_id +1\n",
    "        y = expansion_block(x, t, filters, block_id)\n",
    "        y = depthwise_block(y, strides, block_id)\n",
    "        y = projection_block(y, out_channels, block_id)\n",
    "\n",
    "        # TODO: Check if this implementation is ok\n",
    "        if y.shape[-1] == x.shape[-1]:\n",
    "            y = Add()([x, y])\n",
    "        return y\n",
    "\n",
    "    # Avoid massive downsampling\n",
    "    first_block_filters = _make_divisible(32 * alpha, 8)\n",
    "\n",
    "    # Stem of the model\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Conv2D(filters=first_block_filters, kernel_size=3, strides=2, padding=\"same\", use_bias=False)(inputs)\n",
    "    x = BatchNormalization(name='conv1_bn')(x)\n",
    "    x = ReLU(6, name = 'conv1_relu')(x)  # TODO: option to change to ReLu6 or HardSwish\n",
    "\n",
    "\n",
    "    # Main part of the model\n",
    "    block_id = 0\n",
    "    x = depthwise_block(x, strides=1, block_id=block_id)\n",
    "    x = projection_block(x, out_channels=16 * alpha, block_id=block_id)\n",
    "\n",
    "    # 2 identical layers\n",
    "    x  = bottleneck_residual_block(x, t=6, filters=x.shape[-1], out_channels=24 * alpha, strides=2) #, block_id=block_id +1)\n",
    "    x  = bottleneck_residual_block(x, t=6, filters=x.shape[-1], out_channels=24 * alpha, strides=1) #, block_id=block_id +1)\n",
    "\n",
    "    # 3 repeated bottle_necks\n",
    "    x = bottleneck_residual_block(x, t=6, filters=x.shape[-1], out_channels=32 * alpha, strides=2) #, block_id=block_id +1)   \n",
    "    x = bottleneck_residual_block(x, t=6, filters=x.shape[-1], out_channels=32 * alpha, strides=1) #, block_id=block_id +1)   \n",
    "    x = bottleneck_residual_block(x, t=6, filters=x.shape[-1], out_channels=32 * alpha, strides=1) #, block_id=block_id +1)   \n",
    "\n",
    "    # 4 repeated bottle_necks\n",
    "    x = bottleneck_residual_block(x, t=6, filters=x.shape[-1], out_channels=64 * alpha, strides=2) #, block_id=block_id +1)   \n",
    "    x = bottleneck_residual_block(x, t=6, filters=x.shape[-1], out_channels=64 * alpha, strides=1) #, block_id=block_id +1)   \n",
    "    x = bottleneck_residual_block(x, t=6, filters=x.shape[-1], out_channels=64 * alpha, strides=1) #, block_id=block_id +1)   \n",
    "    x = bottleneck_residual_block(x, t=6, filters=x.shape[-1], out_channels=64 * alpha, strides=1) #, block_id=block_id +1)   \n",
    "\n",
    "    # 3 repeated bottle_necks\n",
    "    x = bottleneck_residual_block(x, t=6, filters=x.shape[-1], out_channels=96 * alpha, strides=1) #, block_id=block_id +1)   \n",
    "    x = bottleneck_residual_block(x, t=6, filters=x.shape[-1], out_channels=96 * alpha, strides=1) #, block_id=block_id +1)   \n",
    "    x = bottleneck_residual_block(x, t=6, filters=x.shape[-1], out_channels=96 * alpha, strides=1) #, block_id=block_id +1)   \n",
    "\n",
    "    # 3 repeated bottle_necks\n",
    "    x = bottleneck_residual_block(x, t=6, filters=x.shape[-1], out_channels=160 * alpha, strides=2) #, block_id=block_id +1)   \n",
    "    x = bottleneck_residual_block(x, t=6, filters=x.shape[-1], out_channels=160 * alpha, strides=1) #, block_id=block_id +1)   \n",
    "    x = bottleneck_residual_block(x, t=6, filters=x.shape[-1], out_channels=160 * alpha, strides=1) #, block_id=block_id +1)   \n",
    "\n",
    "    x = bottleneck_residual_block(x, t=6, filters=x.shape[-1], out_channels=320 * alpha, strides=1) #, block_id=block_id +1) \n",
    "\n",
    "\n",
    "    # no alpha applied to last conv as stated in the paper:\n",
    "    # if the width multiplier is greater than 1 we increase the number of output\n",
    "    # channels.\n",
    "    if alpha > 1.0:\n",
    "        last_block_filters = _make_divisible(1280 * alpha, 8)\n",
    "    else:\n",
    "        last_block_filters = 1280\n",
    "\n",
    "    # 1*1 conv\n",
    "    x = Conv2D(filters=last_block_filters, kernel_size=1, padding=\"same\", use_bias=False, name=\"last_conv\")(x)\n",
    "    x = BatchNormalization(name='last_bn')(x)\n",
    "    x = ReLU(6,name='last_relu')(x)\n",
    "\n",
    "\n",
    "    if global_average_pooling:  \n",
    "        x = GlobalAveragePooling2D(keepdims=True)(x)\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "        x = Conv2D(filters=classes, kernel_size=1, strides=1, padding=\"same\")(x)\n",
    "        x = Reshape(target_shape=(classes,))(x)\n",
    "        outputs = Activation(\"softmax\")(x)\n",
    "        #outputs = Softmax()(x)\n",
    "\n",
    "\n",
    "    else:\n",
    "        # use the original implementation from the paper with average pooling and fully-connected layers\n",
    "        x = AvgPool2D(pool_size=7, strides=1)(x)  # TODO: pool_size is dependent on the input resolution, lower resolutions than 224 might crash the architecture\n",
    "        outputs = Dense(units=classes, activation=\"softmax\")(x)  # TODO: is there a stride=1 implementation in Dense?\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs, name=\"mobilenetvme\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # form https://github.com/keras-team/keras/blob/v2.11.0/keras/applications/mobilenet_v2.py#L96-L485\n",
    "# def _inverted_res_block(inputs, expansion, stride, alpha, filters, block_id):\n",
    "#     \"\"\"Inverted ResNet block.\"\"\"\n",
    "#     channel_axis = 1 if backend.image_data_format() == \"channels_first\" else -1\n",
    "\n",
    "#     in_channels = backend.int_shape(inputs)[channel_axis]\n",
    "#     pointwise_conv_filters = int(filters * alpha)\n",
    "#     # Ensure the number of filters on the last 1x1 convolution is divisible by\n",
    "#     # 8.\n",
    "#     pointwise_filters = _make_divisible(pointwise_conv_filters, 8)\n",
    "#     x = inputs\n",
    "#     prefix = f\"block_{block_id}_\"\n",
    "\n",
    "#     if block_id:\n",
    "#         # Expand with a pointwise 1x1 convolution.\n",
    "#         x = layers.Conv2D(\n",
    "#             expansion * in_channels,\n",
    "#             kernel_size=1,\n",
    "#             padding=\"same\",\n",
    "#             use_bias=False,\n",
    "#             activation=None,\n",
    "#             name=prefix + \"expand\",\n",
    "#         )(x)\n",
    "#         x = layers.BatchNormalization(\n",
    "#             axis=channel_axis,\n",
    "#             epsilon=1e-3,\n",
    "#             momentum=0.999,\n",
    "#             name=prefix + \"expand_BN\",\n",
    "#         )(x)\n",
    "#         x = layers.ReLU(6.0, name=prefix + \"expand_relu\")(x)\n",
    "#     else:\n",
    "#         prefix = \"expanded_conv_\"\n",
    "\n",
    "#     # Depthwise 3x3 convolution.\n",
    "#     if stride == 2:\n",
    "#         x = layers.ZeroPadding2D(\n",
    "#             padding=imagenet_utils.correct_pad(x, 3), name=prefix + \"pad\"\n",
    "#         )(x)\n",
    "#     x = layers.DepthwiseConv2D(\n",
    "#         kernel_size=3,\n",
    "#         strides=stride,\n",
    "#         activation=None,\n",
    "#         use_bias=False,\n",
    "#         padding=\"same\" if stride == 1 else \"valid\",\n",
    "#         name=prefix + \"depthwise\",\n",
    "#     )(x)\n",
    "#     x = layers.BatchNormalization(\n",
    "#         axis=channel_axis,\n",
    "#         epsilon=1e-3,\n",
    "#         momentum=0.999,\n",
    "#         name=prefix + \"depthwise_BN\",\n",
    "#     )(x)\n",
    "\n",
    "#     x = layers.ReLU(6.0, name=prefix + \"depthwise_relu\")(x)\n",
    "\n",
    "#     # Project with a pointwise 1x1 convolution.\n",
    "#     x = layers.Conv2D(\n",
    "#         pointwise_filters,\n",
    "#         kernel_size=1,\n",
    "#         padding=\"same\",\n",
    "#         use_bias=False,\n",
    "#         activation=None,\n",
    "#         name=prefix + \"project\",\n",
    "#     )(x)\n",
    "#     x = layers.BatchNormalization(\n",
    "#         axis=channel_axis,\n",
    "#         epsilon=1e-3,\n",
    "#         momentum=0.999,\n",
    "#         name=prefix + \"project_BN\",\n",
    "#     )(x)\n",
    "\n",
    "#     if in_channels == pointwise_filters and stride == 1:\n",
    "#         return layers.Add(name=prefix + \"add\")([inputs, x])\n",
    "#     return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mobilenet_v1_keras(input_shape, classes=classes, alpha=alpha):\n",
    "    global dropout_rate\n",
    "    model = tf.keras.applications.mobilenet.MobileNet(\n",
    "        input_shape=input_shape,\n",
    "        alpha=alpha,\n",
    "        depth_multiplier=1,\n",
    "        dropout=dropout_rate,\n",
    "        include_top=True,\n",
    "        weights=None, #'imagenet'\n",
    "        input_tensor=None,\n",
    "        pooling=None,\n",
    "        classes=classes,\n",
    "        classifier_activation='softmax',\n",
    "        #**kwargs\n",
    "    )\n",
    "\n",
    "    #model._name = model.name + \"_keras\" # model.name cannot be overritten\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mobilenet_v2_keras(input_shape, classes=classes, alpha=alpha):\n",
    "    model = tf.keras.applications.mobilenet_v2.MobileNetV2(\n",
    "        input_shape=input_shape,\n",
    "        alpha=alpha,\n",
    "        include_top=True, # False should be corect\n",
    "        weights=None, #'imagenet'\n",
    "        input_tensor=None,\n",
    "        pooling=\"avg\",\n",
    "        classes=classes,\n",
    "        classifier_activation='softmax',\n",
    "        #**kwargs\n",
    "    )\n",
    "\n",
    "    # https://github.com/tensorflow/tensorflow/issues/36065\n",
    "    # model in the original version does not train because momentum is set to 0.999 by default\n",
    "    model = set_batchnorm_momentum(model, momentum=0.9)\n",
    "    \n",
    "    #model._name = model.name + \"_keras\" # model.name cannot be overritten\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MobileNet V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mobilenet_v3_small_keras(input_shape, classes=classes, alpha=alpha):\n",
    "    global dropout_rate\n",
    "    model = tf.keras.applications.MobileNetV3Small(\n",
    "        input_shape=input_shape,\n",
    "        minimalistic=False, # TODO find out about this parameter\n",
    "        alpha=alpha,\n",
    "        include_top=True,\n",
    "        weights=None,\n",
    "        input_tensor=None,\n",
    "        classes=classes,\n",
    "        classifier_activation=\"softmax\",\n",
    "        dropout_rate=dropout_rate,\n",
    "        include_preprocessing=False\n",
    "        #**kwargs\n",
    "        )\n",
    "    # https://github.com/tensorflow/tensorflow/issues/36065\n",
    "    # model in the original version does not train because momentum is set to 0.999 by default\n",
    "    model = set_batchnorm_momentum(model, momentum=0.9)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mobilenet_v3_large_keras(input_shape, classes=classes, alpha=alpha):\n",
    "    global dropout_rate\n",
    "    model = tf.keras.applications.MobileNetV3Large(\n",
    "        input_shape=input_shape,\n",
    "        minimalistic=False, # TODO find out about this parameter\n",
    "        alpha=alpha,\n",
    "        include_top=True,\n",
    "        weights=None,\n",
    "        input_tensor=None,\n",
    "        classes=classes,\n",
    "        classifier_activation=\"softmax\",\n",
    "        dropout_rate=dropout_rate,\n",
    "        include_preprocessing=False\n",
    "        #**kwargs\n",
    "        )\n",
    "\n",
    "    # https://github.com/tensorflow/tensorflow/issues/36065\n",
    "    # model in the original version does not train because momentum is set to 0.999 by default\n",
    "    model = set_batchnorm_momentum(model, momentum=0.9)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "def efficientNetB0_keras(input_shape, classes= classes, alpha= alpha):\n",
    "    model = tf.keras.applications.efficientnet.EfficientNetB0(\n",
    "        include_top=True,\n",
    "        weights= None, #'imagenet',\n",
    "        input_tensor=None,\n",
    "        input_shape=input_shape,\n",
    "        pooling=None,\n",
    "        classes=classes,\n",
    "        classifier_activation='softmax',\n",
    "        #**kwargs\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ShuffleNets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def shufflenet_v1(input_shape, classes=classes, alpha=alpha, groups=1):\n",
    "def _make_divisible(v, divisor, min_value=None):\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    \n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    # if new_v < 0.9 * v:\n",
    "    #     new_v += divisor\n",
    "    return new_v\n",
    "    \n",
    "\n",
    "\n",
    "def channel_shuffle(x, groups):\n",
    "\n",
    "    n, h, w, c = x.get_shape().as_list()\n",
    "    x = tf.reshape(\n",
    "        x, shape=tf.convert_to_tensor([tf.shape(x)[0], h, w, groups, c // groups])\n",
    "    )\n",
    "    x = tf.transpose(x, tf.convert_to_tensor([0, 1, 2, 4, 3]))\n",
    "    x = tf.reshape(x, shape=tf.convert_to_tensor([tf.shape(x)[0], h, w, c]))\n",
    "    return x\n",
    "    \n",
    "def shuffle_unit(x, groups, channels,strides):\n",
    "\n",
    "    y = x\n",
    "    x = Conv2D(channels//4, kernel_size = 1, strides = (1,1),padding = 'same', groups=groups)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    x = channel_shuffle(x, groups)\n",
    "    \n",
    "    x = DepthwiseConv2D(kernel_size = (3,3), strides = strides, padding = 'same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    if strides == (2,2):\n",
    "        channels = channels - y.shape[-1]\n",
    "    x = Conv2D(channels, kernel_size = 1, strides = (1,1),padding = 'same', groups=groups)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    if strides ==(1,1):\n",
    "        x =Add()([x,y])\n",
    "    if strides == (2,2):\n",
    "        y = AvgPool2D((3,3), strides = (2,2), padding = 'same')(y)\n",
    "        #x = Concatenate([x,y])\n",
    "        x = Concatenate(axis=-1)([x, y]) \n",
    "    \n",
    "    x = ReLU()(x)\n",
    "\n",
    "\n",
    "    return x\n",
    "    \n",
    "def shufflenet_v1(input_shape, classes=classes, alpha=alpha, groups=2):\n",
    "\n",
    "    start_channels = 200\n",
    "    input = Input (input_shape)\n",
    "\n",
    "    x =  Conv2D (24,kernel_size=3,strides = (2,2), padding = 'same', use_bias = True)(input)\n",
    "    x =  BatchNormalization()(x)\n",
    "    x =  ReLU()(x)\n",
    "    \n",
    "    x = MaxPooling2D (pool_size=(3,3), strides = 2, padding='same')(x)\n",
    "\n",
    "    repetitions = [3,7,3]\n",
    "\n",
    "    for i,repetition in enumerate(repetitions):\n",
    "        channels = start_channels * (2**i)\n",
    "\n",
    "        x  = shuffle_unit(x, groups, channels,strides = (2,2))\n",
    "\n",
    "        for i in range(repetition):\n",
    "            x = shuffle_unit(x, groups, channels,strides=(1,1))\n",
    "\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    output = Dense(classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(input, output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def shufflenet_v1(input_shape, classes=classes, alpha=alpha, groups=1):\n",
    "#     # inspired by https://github.com/Haikoitoh/paper-implementation/blob/main/ShuffleNet.ipynb\n",
    "\n",
    "#     # groups = 2 # [1, 2, 3, 4, 8]\n",
    "\n",
    "#     def _make_divisible(v, divisor, min_value=None):\n",
    "#         if min_value is None:\n",
    "#             min_value = divisor\n",
    "#         new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "#         # Make sure that round down does not go down by more than 10%.\n",
    "#         # if new_v < 0.9 * v:\n",
    "#         #     new_v += divisor\n",
    "#         return new_v\n",
    "\n",
    "#     start_channels_dict = {\"1\": [144, 288, 576],\n",
    "#                             \"2\": [200, 400, 800], \n",
    "#                             \"3\": [240,480, 960], \n",
    "#                             \"4\": [272, 544, 1088], \n",
    "#                             \"8\": [384, 768, 1536]}\n",
    "\n",
    "#     start_channels = start_channels_dict[str(groups)][0]\n",
    "#     print(start_channels)\n",
    "\n",
    "#     # def channel_shuffle(x, groups):\n",
    "#     #     batch, width, height, channels = x.get_shape().as_list()\n",
    "#     #     group_ch = channels // groups\n",
    "\n",
    "#     #     x = Reshape([width, height, group_ch, groups])(x)\n",
    "#     #     x = Permute([1, 2, 4, 3])(x)\n",
    "#     #     x = Reshape([width, height, channels])(x)\n",
    "#     #     return x\n",
    "\n",
    "#     def channel_shuffle(x, groups):\n",
    "\n",
    "#         n, h, w, c = x.get_shape().as_list()\n",
    "#         x = tf.reshape(\n",
    "#             x, shape=tf.convert_to_tensor([tf.shape(x)[0], h, w, groups, c // groups])\n",
    "#         )\n",
    "#         x = tf.transpose(x, tf.convert_to_tensor([0, 1, 2, 4, 3]))\n",
    "#         x = tf.reshape(x, shape=tf.convert_to_tensor([tf.shape(x)[0], h, w, c]))\n",
    "#         return x\n",
    "\n",
    "#     def shuffle_unit(x, groups, channels, strides):\n",
    "\n",
    "#         # if strides == (2, 2):\n",
    "#         #     #channels = channels - y.shape[-1]  # TODO: fix downsampling to negative filters\n",
    "#         #     channels = channels //2\n",
    "#         print(f\"Input channels to shuffle_unit: {channels}\")\n",
    "            \n",
    "#         y = x\n",
    "#         x = Conv2D(\n",
    "#             channels, kernel_size=1, strides=(1, 1), padding=\"same\", groups=groups\n",
    "#         )(x)\n",
    "#         x = BatchNormalization()(x)\n",
    "#         x = ReLU()(x)\n",
    "\n",
    "#         x = channel_shuffle(x, groups)\n",
    "\n",
    "#         x = DepthwiseConv2D(kernel_size=(3, 3), strides=strides, padding=\"same\")(x)\n",
    "#         x = BatchNormalization()(x)\n",
    "\n",
    "#         if strides == (2, 2):\n",
    "#             channels = channels - x.shape[-1]  # TODO: fix downsampling to negative filters\n",
    "#             #channels = x.shape[-1]  # TODO: fix downsampling to negative filters\n",
    "#             print(channels)\n",
    "#         x = Conv2D(\n",
    "#             channels, kernel_size=1, strides=(1, 1), padding=\"same\", groups=groups\n",
    "#         )(x)\n",
    "#         x = BatchNormalization()(x)\n",
    "\n",
    "#         if strides == (1, 1):\n",
    "#             x = Add()([x, y])\n",
    "#         if strides == (2, 2):\n",
    "#             y = AvgPool2D((3, 3), strides=(2, 2), padding=\"same\")(y)\n",
    "#             x = Concatenate(axis=-1)([x, y])  # TODO: check if the axis is correct!\n",
    "\n",
    "#         x = ReLU()(x)\n",
    "#         return x\n",
    "    \n",
    "\n",
    "\n",
    "#     # Main architecture\n",
    "\n",
    "#     input = Input(input_shape)\n",
    "\n",
    "#     x = Conv2D(24, kernel_size=3, strides=(2, 2), padding=\"same\", use_bias=True)(input)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = ReLU()(x)\n",
    "\n",
    "#     x = MaxPooling2D(pool_size=(3, 3), strides=2, padding=\"same\")(x)\n",
    "\n",
    "#     repetitions = [3, 7, 3]\n",
    "\n",
    "#     # for i, repetition in enumerate(repetitions):\n",
    "#     #     channels = start_channels * (2**i)\n",
    "\n",
    "#     #     x = shuffle_unit(\n",
    "#     #         x, groups, _make_divisible(channels * alpha, groups), strides=(2, 2)\n",
    "#     #     )\n",
    "\n",
    "#     #     for j in range(repetition):\n",
    "#     #         x = shuffle_unit(\n",
    "#     #             x, groups, _make_divisible(channels * alpha, groups), strides=(1, 1)\n",
    "#     #         )\n",
    "\n",
    "#     channels = start_channels\n",
    "#     repetition = repetitions[0]\n",
    "#     x = shuffle_unit(x, groups, _make_divisible(channels * alpha, groups), strides=(2, 2))\n",
    "\n",
    "#     # for j in range(repetition):\n",
    "#     #     print(f\"{j} - channels {channels}\")\n",
    "#     #     x = shuffle_unit(x, groups, _make_divisible(channels * alpha, groups), strides=(1, 1))\n",
    "\n",
    "#     # channels = channels *2\n",
    "#     # repetition = repetitions[1]\n",
    "#     # x = shuffle_unit(x, groups, _make_divisible(channels * alpha, groups), strides=(2, 2))\n",
    "\n",
    "#     # for j in range(repetition):\n",
    "#     #     x = shuffle_unit(x, groups, _make_divisible(channels * alpha, groups), strides=(1, 1))\n",
    "\n",
    "#     # channels = channels *2\n",
    "#     # repetition = repetitions[2]\n",
    "#     # x = shuffle_unit(x, groups, _make_divisible(channels * alpha, groups), strides=(2, 2))\n",
    "\n",
    "#     # for j in range(repetition):\n",
    "#     #     x = shuffle_unit(x, groups, _make_divisible(channels * alpha, groups), strides=(1, 1))\n",
    "\n",
    "\n",
    "\n",
    "#     x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "#     output = Dense(classes, activation=\"softmax\")(x)\n",
    "\n",
    "#     model = Model(input, output, name=\"shufflenetv1\")\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from keras.activations import *\n",
    "from keras.callbacks import *\n",
    "import keras.backend as K\n",
    "\n",
    "\n",
    "def _stage(tensor, nb_groups, in_channels, out_channels, repeat):\n",
    "    x = _shufflenet_unit(tensor, nb_groups, in_channels, out_channels, 2)\n",
    "\n",
    "    for _ in range(repeat):\n",
    "        x = _shufflenet_unit(x, nb_groups, out_channels, out_channels, 1)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def _pw_group(tensor, nb_groups, in_channels, out_channels):\n",
    "    \"\"\"Pointwise grouped convolution.\"\"\"\n",
    "    nb_chan_per_grp = in_channels // nb_groups\n",
    "\n",
    "    pw_convs = []\n",
    "    for grp in range(nb_groups):\n",
    "        x = Lambda(lambda x: x[:, :, :, nb_chan_per_grp * grp: nb_chan_per_grp * (grp + 1)])(tensor)\n",
    "        grp_out_chan = int(out_channels / nb_groups + 0.5)\n",
    "\n",
    "        pw_convs.append(\n",
    "            Conv2D(grp_out_chan,\n",
    "                   kernel_size=(1, 1),\n",
    "                   padding='same',\n",
    "                   use_bias=False,\n",
    "                   strides=1)(x)\n",
    "        )\n",
    "\n",
    "    return Concatenate(axis=-1)(pw_convs)\n",
    "\n",
    "\n",
    "def _shuffle(x, nb_groups):\n",
    "    def shuffle_layer(x):\n",
    "        _, w, h, n = K.int_shape(x)\n",
    "        nb_chan_per_grp = n // nb_groups\n",
    "\n",
    "        x = K.reshape(x, (-1, w, h, nb_chan_per_grp, nb_groups))\n",
    "        x = K.permute_dimensions(x, (0, 1, 2, 4, 3)) # Transpose only grps and chs\n",
    "        x = K.reshape(x, (-1, w, h, n))\n",
    "\n",
    "        return x\n",
    "\n",
    "    return Lambda(shuffle_layer)(x)\n",
    "\n",
    "\n",
    "def _shufflenet_unit(tensor, nb_groups, in_channels, out_channels, strides, shuffle=True, bottleneck=4):\n",
    "    bottleneck_channels = out_channels // bottleneck\n",
    "\n",
    "    x = _pw_group(tensor, nb_groups, in_channels, bottleneck_channels)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    if shuffle:\n",
    "        x = _shuffle(x, nb_groups)\n",
    "\n",
    "    x = DepthwiseConv2D(kernel_size=(3, 3),\n",
    "                        padding='same',\n",
    "                        use_bias=False,\n",
    "                        strides=strides)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "\n",
    "    x = _pw_group(x, nb_groups, bottleneck_channels,\n",
    "                  out_channels if strides < 2 else out_channels - in_channels)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    if strides < 2:\n",
    "        x = Add()([tensor, x])\n",
    "    else:\n",
    "        avg = AveragePooling2D(pool_size=(3, 3),\n",
    "                               strides=2,\n",
    "                               padding='same')(tensor)\n",
    "\n",
    "        x = Concatenate(axis=-1)([avg, x])\n",
    "\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def _info(nb_groups):\n",
    "    return {\n",
    "        1: [24, 144, 288, 576],\n",
    "        2: [24, 200, 400, 800],\n",
    "        3: [24, 240, 480, 960],\n",
    "        4: [24, 272, 544, 1088],\n",
    "        8: [24, 384, 768, 1536]\n",
    "    }[nb_groups], [None, 3, 7, 3]\n",
    "\n",
    "\n",
    "def ShuffleNet_keras(input_shape, nb_classes, include_top=True, weights=None, nb_groups=8):\n",
    "    x_in = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv2D(24,\n",
    "               kernel_size=(3, 3),\n",
    "               strides=2,\n",
    "               use_bias=False,\n",
    "               padding='same')(x_in)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = MaxPooling2D(pool_size=(3, 3),\n",
    "                     strides=2,\n",
    "                     padding='same')(x)\n",
    "\n",
    "    channels_list, repeat_list = _info(nb_groups)\n",
    "    for i, (out_channels, repeat) in enumerate(zip(channels_list[1:], repeat_list[1:]), start=1):\n",
    "        x = _stage(x, nb_groups, channels_list[i-1], out_channels, repeat)\n",
    "\n",
    "    if include_top:\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "        x = Dense(nb_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=x_in, outputs=x)\n",
    "\n",
    "    if weights is not None:\n",
    "        model.load_weights(weights, by_name=True)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def shufflenet_v11(input_shape, classes=classes, alpha= alpha, groups = 1):\n",
    "#     # inspired by https://github.com/Haikoitoh/paper-implementation/blob/main/ShuffleNet.ipynb\n",
    "\n",
    "#     #groups = 2 # [1, 2, 3, 4, 8]\n",
    "\n",
    "#     start_channels_dict={\n",
    "#         \"1\" : 144,\n",
    "#         \"2\" : 200,\n",
    "#         \"3\" : 240,\n",
    "#         \"4\" : 272,\n",
    "#         \"8\" : 384\n",
    "#     }\n",
    "\n",
    "#     start_channels = start_channels_dict[str(groups)]\n",
    "\n",
    "#     def channel_shuffle(x, groups):  \n",
    "#         batch, width, height, channels = x.get_shape().as_list()\n",
    "#         group_ch = channels // groups\n",
    "\n",
    "#         x = Reshape([width, height, group_ch, groups])(x)\n",
    "#         x = Permute([1, 2, 4, 3])(x)\n",
    "#         x = Reshape([width, height, channels])(x)\n",
    "#         return x\n",
    "\n",
    "#     def shuffle_unit(x, groups, channels, strides):\n",
    "\n",
    "#         y = x\n",
    "#         x = Conv2D(channels, kernel_size = 1, strides = (1,1),padding = 'same', groups=groups)(x)\n",
    "#         x = BatchNormalization()(x)\n",
    "#         x = ReLU()(x)\n",
    "\n",
    "#         x = channel_shuffle(x, groups)\n",
    "        \n",
    "#         x = DepthwiseConv2D(kernel_size = (3,3), strides = strides, padding = 'same')(x)\n",
    "#         x = BatchNormalization()(x)\n",
    "\n",
    "#         if strides == (2,2):\n",
    "#             channels = channels - y.shape[-1]\n",
    "#         x = Conv2D(channels, kernel_size = 1, strides = (1,1),padding = 'same', groups=groups)(x)\n",
    "#         x = BatchNormalization()(x)\n",
    "\n",
    "#         if strides ==(1,1):\n",
    "#             x =Add()([x,y])\n",
    "#         if strides == (2,2):\n",
    "#             y = AvgPool2D((3,3), strides = (2,2), padding = 'same')(y)\n",
    "#             x = Concatenate(axis=-1)([x,y]) # TODO: check if the axis is correct!\n",
    "        \n",
    "#         x = ReLU()(x)\n",
    "#         return x\n",
    "\n",
    "#     # Main architecture\n",
    "\n",
    "#     input = Input (input_shape)\n",
    "\n",
    "#     x =  Conv2D (24,kernel_size=3,strides = (2,2), padding = 'same', use_bias = True)(input)\n",
    "#     x =  BatchNormalization()(x)\n",
    "#     x =  ReLU()(x)\n",
    "    \n",
    "#     x = MaxPooling2D (pool_size=(3,3), strides = 2, padding='same')(x)\n",
    "\n",
    "#     repetitions = [3,7,3]\n",
    "\n",
    "#     for i,repetition in enumerate(repetitions):\n",
    "#         channels = start_channels * (2**i)\n",
    "\n",
    "#         x  = shuffle_unit(x, groups, channels * alpha ,strides = (2,2))\n",
    "\n",
    "#         for i in range(repetition):\n",
    "#             x = shuffle_unit(x, groups, channels * alpha ,strides=(1,1))\n",
    "\n",
    "#     x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "#     output = Dense(classes,activation='softmax')(x)\n",
    "\n",
    "#     model = Model(input, output, name=\"shufflenetv1\")\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shufflenet_v2(input_shape, classes=classes, alpha=alpha, use_bias=False):\n",
    "    \n",
    "\n",
    "    #groups = 2 # [1, 2, 3, 4, 8]\n",
    "\n",
    "    start_channels_dict={  # based on alpha factor\n",
    "        \"0.5\" : 48,\n",
    "        \"1\" : 116,\n",
    "        \"1.5\" : 176,\n",
    "        \"2\" : 244,\n",
    "    }\n",
    "\n",
    "    first_layer_channels = 24\n",
    "    last_layer_channels = 1024\n",
    "\n",
    "    start_channels = start_channels_dict[str(alpha)]\n",
    "\n",
    "    def channel_shuffle(x, groups):  \n",
    "        batch, width, height, channels = x.get_shape().as_list()\n",
    "        group_ch = channels // groups\n",
    "\n",
    "        x = Reshape([width, height, group_ch, groups])(x)\n",
    "        x = Permute([1, 2, 4, 3])(x)\n",
    "        x = Reshape([width, height, channels])(x)\n",
    "        return x\n",
    "\n",
    "    def basic_unit(x, channels):\n",
    "\n",
    "        y,z = tf.split(x, num_or_size_splits=2, axis=-1) # channel split \n",
    "        # batch, width, height, channels = x.get_shape().as_list()\n",
    "        # channel_split = channels//2\n",
    "        # y = x[:, :, :, 0:channel_split+1]\n",
    "        # print(y.shape)\n",
    "        # z = x[:, :, :, channel_split:-1]\n",
    "        # print(z.shape)\n",
    "\n",
    "        y = Conv2D(channels, kernel_size = 1, strides = (1,1) ,padding = 'same', use_bias=use_bias)(y)\n",
    "        y = BatchNormalization()(y)\n",
    "        y = ReLU()(y)\n",
    "        \n",
    "        y = DepthwiseConv2D(kernel_size = (3,3), strides = (1,1), padding = 'same', use_bias=use_bias)(y)\n",
    "        y = BatchNormalization()(y)\n",
    "\n",
    "        y = Conv2D(channels, kernel_size = 1, strides = (1,1),padding = 'same', use_bias=use_bias)(y)\n",
    "        y = BatchNormalization()(y)\n",
    "        y = ReLU()(y)\n",
    "\n",
    "        y = Concatenate(axis=-1)([y,z]) # TODO: check if the axis is correct!\n",
    "        y = channel_shuffle(y, 2)\n",
    "        return y\n",
    "\n",
    "    def down_sampling_unit(x, channels):\n",
    "\n",
    "        y = x\n",
    "        x = Conv2D(channels, kernel_size = 1, strides = (1,1),padding = 'same', use_bias=use_bias)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = ReLU()(x)\n",
    "        \n",
    "        x = DepthwiseConv2D(kernel_size = (3,3), strides = (2,2), padding = 'same', use_bias=use_bias)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "\n",
    "        x = Conv2D(channels, kernel_size = 1, strides = (1,1),padding = 'same', use_bias=use_bias)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = ReLU()(x)\n",
    "\n",
    "        y = DepthwiseConv2D(kernel_size = (3,3), strides = (2,2), padding = 'same', use_bias=use_bias)(y)\n",
    "        y = BatchNormalization()(y)\n",
    "\n",
    "        y = Conv2D(channels, kernel_size = 1, strides = (1,1),padding = 'same', use_bias=use_bias)(y)\n",
    "        y = BatchNormalization()(y)\n",
    "        y = ReLU()(y)\n",
    "\n",
    "        x = Concatenate(axis=-1)([x,y]) # TODO: check if the axis is correct!\n",
    "        x = channel_shuffle(x, 2)\n",
    "        return x\n",
    "\n",
    "    # Main architecture\n",
    "\n",
    "    input = Input (input_shape)\n",
    "\n",
    "    x =  Conv2D (first_layer_channels ,kernel_size=3, strides = (2,2), padding = 'valid', use_bias=use_bias)(input)\n",
    "    x =  BatchNormalization()(x)\n",
    "    x =  ReLU()(x)\n",
    "    \n",
    "    x = MaxPooling2D (pool_size=(3,3), strides = 2, padding='same')(x)\n",
    "\n",
    "    repetitions = [3,7,3]\n",
    "\n",
    "    channels = start_channels\n",
    "\n",
    "    for i,repetition in enumerate(repetitions):\n",
    "\n",
    "        x  = down_sampling_unit(x, channels)\n",
    "\n",
    "        for j in range(repetition):\n",
    "            x = basic_unit(x, channels)\n",
    "\n",
    "        channels = channels * 2 # ShuffleNet V1 *(2**1)\n",
    "\n",
    "    x =  Conv2D (last_layer_channels, kernel_size=1,strides = (1,1), padding = 'same', use_bias=use_bias)(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    output = Dense(classes,activation='softmax')(x)\n",
    "\n",
    "    model = Model(input, output, name=\"shufflenetv2\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shufflenet_v2tiny(input_shape, classes=classes, alpha=alpha, first_layer_channels=24, last_layer_channels=1024, use_bias=False):\n",
    "    \n",
    "\n",
    "    #groups = 2 # [1, 2, 3, 4, 8]\n",
    "\n",
    "    start_channels_dict={  # based on alpha factor\n",
    "        \"0.05\" : 6,\n",
    "        \"0.1\" : 12,\n",
    "        \"0.2\" : 24,\n",
    "        \"0.25\" : 28,\n",
    "        \"0.3\" : 34,        \n",
    "        \"0.5\" : 48, # this is the smallest original architecture alpha\n",
    "        \"1\" : 116,\n",
    "        \"1.5\" : 176,\n",
    "        \"2\" : 244,\n",
    "    }\n",
    "\n",
    "    # first_layer_channels = 24\n",
    "    # last_layer_channels = 1024\n",
    "\n",
    "    start_channels = start_channels_dict[str(alpha)]\n",
    "\n",
    "    def channel_shuffle(x, groups):  \n",
    "        batch, width, height, channels = x.get_shape().as_list()\n",
    "        group_ch = channels // groups\n",
    "\n",
    "        x = Reshape([width, height, group_ch, groups])(x)\n",
    "        x = Permute([1, 2, 4, 3])(x)\n",
    "        x = Reshape([width, height, channels])(x)\n",
    "        return x\n",
    "\n",
    "    #     x = Reshape([width, height, group_ch, groups])(x)\n",
    "    #     x = Permute([1, 2, 4, 3])(x)\n",
    "    #     x = Reshape([width, height, channels])(x)\n",
    "    #     return x\n",
    "\n",
    "    # https://stackoverflow.com/questions/62794840/apply-channel-shuffle-in-tensorflow-or-keras\n",
    "\n",
    "    # trouble version\n",
    "    # def channel_shuffle(x, groups):\n",
    "    #     # g = 2\n",
    "    #     b, h, w, c = x.shape.as_list()\n",
    "    #     x = tf.reshape(x,[-1, h, w, groups, c // groups])\n",
    "    #     x = tf.transpose(x, perm = [0, 1, 2, 4, 3])\n",
    "    #     #x = tf.reverse(x,[-1]) \n",
    "    #     x = tf.reshape(x, [-1, h, w, c])\n",
    "    #     return x\n",
    "\n",
    "\n",
    "    # https://github.com/MG2033/ShuffleNet/blob/master/layers.py#L238\n",
    "    # def channel_shuffle(name, x, num_groups):\n",
    "    #     with tf.variable_scope(name) as scope:\n",
    "    #         n, h, w, c = x.shape.as_list()\n",
    "    #         x_reshaped = tf.reshape(x, [-1, h, w, num_groups, c // num_groups])\n",
    "    #         x_transposed = tf.transpose(x_reshaped, [0, 1, 2, 4, 3])\n",
    "    #         output = tf.reshape(x_transposed, [-1, h, w, c])\n",
    "    #         return output\n",
    "\n",
    "\n",
    "    ## https://github.com/timctho/shufflenet-v2-tensorflow/blob/master/module.py\n",
    "    def channel_shuffle(x, groups):\n",
    "    #with tf.variable_scope('shuffle_unit'):\n",
    "    \n",
    "        n, h, w, c = x.get_shape().as_list()\n",
    "        x = tf.reshape(x, shape=tf.convert_to_tensor([tf.shape(x)[0], h, w, groups, c // groups]))\n",
    "        x = tf.transpose(x, tf.convert_to_tensor([0, 1, 2, 4, 3]))\n",
    "        x = tf.reshape(x, shape=tf.convert_to_tensor([tf.shape(x)[0], h, w, c]))\n",
    "        return x\n",
    "\n",
    "    def basic_unit(x, channels):\n",
    "\n",
    "        #x,y = tf.split(x, num_or_size_splits=2, axis=-1) # channel split \n",
    "\n",
    "        batch, width, height, channels = x.get_shape().as_list()\n",
    "        channel_split = channels//2 \n",
    "        y = x[:, :, :, 0:channel_split]\n",
    "        print(f\"Basic unit y: {y.shape} - channels: {channels}\")\n",
    "        z = x[:, :, :, channel_split-1:-1]\n",
    "        print(f\"Basic unit z: {z.shape} - channels: {channels}\")\n",
    "\n",
    "\n",
    "\n",
    "        y = Conv2D(channels, kernel_size = 1, strides = (1,1) ,padding = 'same', use_bias=use_bias)(y)\n",
    "        y = BatchNormalization()(y)\n",
    "        y = ReLU()(y)\n",
    "        \n",
    "        y = DepthwiseConv2D(kernel_size = (3,3), strides = (1,1), padding = 'same', use_bias=use_bias)(y)\n",
    "        y = BatchNormalization()(y)\n",
    "\n",
    "        y = Conv2D(channels, kernel_size = 1, strides = (1,1),padding = 'same', use_bias=use_bias)(y)\n",
    "        y = BatchNormalization()(y)\n",
    "        y = ReLU()(y)\n",
    "\n",
    "        y = Concatenate(axis=-1)([y,z]) # TODO: check if the axis is correct!\n",
    "        y = channel_shuffle(y, 2) # use 2 groups\n",
    "        return y\n",
    "\n",
    "    def down_sampling_unit(x, channels):\n",
    "\n",
    "        y = x\n",
    "        x = Conv2D(channels, kernel_size = 1, strides = (1,1),padding = 'same', use_bias=use_bias)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = ReLU()(x)\n",
    "        \n",
    "        x = DepthwiseConv2D(kernel_size = (3,3), strides = (2,2), padding = 'same', use_bias=use_bias)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "\n",
    "        x = Conv2D(channels, kernel_size = 1, strides = (1,1),padding = 'same', use_bias=use_bias)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = ReLU()(x)\n",
    "\n",
    "        y = DepthwiseConv2D(kernel_size = (3,3), strides = (2,2), padding = 'same', use_bias=use_bias)(y)\n",
    "        y = BatchNormalization()(y)\n",
    "\n",
    "        y = Conv2D(channels, kernel_size = 1, strides = (1,1),padding = 'same', use_bias=use_bias)(y)\n",
    "        y = BatchNormalization()(y)\n",
    "        y = ReLU()(y)\n",
    "\n",
    "        x = Concatenate(axis=-1)([x,y]) # TODO: check if the axis is correct!\n",
    "        x = channel_shuffle(x, 2)\n",
    "        return x\n",
    "\n",
    "    # Main architecture\n",
    "\n",
    "    input = Input (input_shape)\n",
    "\n",
    "    x =  Conv2D (first_layer_channels ,kernel_size=3, strides = (2,2), padding = 'valid', use_bias=use_bias)(input)\n",
    "    x =  BatchNormalization()(x)\n",
    "    x =  ReLU()(x)\n",
    "    \n",
    "    # exclude pooling layer to avoid model collapse\n",
    "    x = MaxPooling2D (pool_size=(3,3), strides = 2, padding='same')(x)\n",
    "\n",
    "    repetitions = [3,7,3]\n",
    "\n",
    "    channels = start_channels\n",
    "\n",
    "    # stage 2\n",
    "    print(\"stage 2\")\n",
    "    x = down_sampling_unit(x, channels)\n",
    "    for i in range(repetitions[0]):\n",
    "        print(f\"rep {i}, channels {channels}\")\n",
    "        x = basic_unit(x, channels)\n",
    "    \n",
    "    # stage 3\n",
    "    print(\"stage 3\")\n",
    "    channels = channels *2\n",
    "    print(\"channels\")\n",
    "\n",
    "    x = down_sampling_unit(x, channels)\n",
    "    for i in range(repetitions[1]):\n",
    "        print(f\"rep {i}, channels {channels}\")\n",
    "        x = basic_unit(x, channels)\n",
    "\n",
    "    # stage 3\n",
    "    channels = channels *2\n",
    "\n",
    "    x = down_sampling_unit(x, channels)\n",
    "    for i in range(repetitions[2]):\n",
    "        x = basic_unit(x, channels)\n",
    "       \n",
    "\n",
    "    # for i,repetition in enumerate(repetitions):\n",
    "\n",
    "    #     x  = down_sampling_unit(x, channels)\n",
    "\n",
    "    #     for j in range(repetition):\n",
    "    #         x = basic_unit(x, channels)\n",
    "\n",
    "    #     channels = channels * 2 # ShuffleNet V1 *(2**1)\n",
    "\n",
    "    x =  Conv2D (last_layer_channels, kernel_size=1,strides = (1,1), padding = 'same', use_bias=use_bias)(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    output = Dense(classes,activation='softmax')(x)\n",
    "\n",
    "    model = Model(input, output, name=\"shufflenetv2tiny\")\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_seed = 1\n",
    "#train_ds, val_ds, test_ds, class_names = get_vvw_minval_dataset(img_res, img_res, 32, channels, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds, val_ds, test_ds, class_names = get_lemon_binary_datagen(None, img_res, img_res, 32, channels, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model = shufflenet_v1(input_shape, classes=classes, alpha= alpha)\n",
    "# model.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer='adam',\n",
    "#                         loss='sparse_categorical_crossentropy',\n",
    "#                          metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model.fit(train_ds,\n",
    "#         epochs=1, \n",
    "#         validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_val_true = np.concatenate([y for x, y in val_ds], axis=0)\n",
    "# y_val_pred = model.predict(val_ds).argmax(axis=1)\n",
    "\n",
    "# y_test_true = np.concatenate([y for x, y in test_ds], axis=0)\n",
    "# y_test_pred = model.predict(test_ds).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = model.evaluate(test_ds, batch_size=32)\n",
    "# print(\"test loss, test acc:\", results)\n",
    "# wandb.log({\n",
    "#         \"test_loss\" : results[0],\n",
    "#         \"test_accuracy\" : results[1]\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace activation layers: https://stackoverflow.com/questions/66384428/how-to-replace-activation-layer-of-existing-model-in-keras-tensorflow\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.layers import LeakyReLU\n",
    "# l_relu = LeakyReLU(alpha=0.3)\n",
    "\n",
    "# for layer in e_net.layers:\n",
    "#     if (hasattr(layer,'activation'))==True:\n",
    "#           layer.activation = l_relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/66242091/tensorflow-keras-replace-activation-layer-in-pretrained-model\n",
    "\n",
    "# def replace_swish_with_relu(model):\n",
    "#     '''\n",
    "#     Modify passed model by replacing swish activation with relu\n",
    "#     '''\n",
    "#     for layer in tuple(model.layers):\n",
    "#         layer_type = type(layer).__name__\n",
    "#         if hasattr(layer, 'activation') and layer.activation.__name__ == 'swish':\n",
    "#             print(layer_type, layer.activation.__name__)\n",
    "#             if layer_type == \"Conv2D\":\n",
    "#                 # conv layer with swish activation\n",
    "#                 layer.activation = tf.keras.activations.relu\n",
    "#             else:\n",
    "#                 # activation layer\n",
    "#                 layer.activation = tf.keras.activations.relu\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = mobilenet_v1(input_shape, classes=classes, alpha=alpha)\n",
    "#model = mobilenet_v2(input_shape, classes=classes, alpha=alpha)\n",
    "#model = mobilenet_v1_keras(input_shape, classes=classes, alpha=alpha)\n",
    "#model = mobilenet_v2_keras(input_shape, classes=classes, alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(architecture, input_shape, classes, alpha, loop_depth= None,first_layer_channels=None, last_layer_channels=None, use_bias=False, head=head):\n",
    "    global base_model_name\n",
    "    if architecture==\"mobilenet_v1\":\n",
    "        if loop_depth==None:\n",
    "            model = mobilenet_v1(input_shape, classes=classes, alpha=alpha)\n",
    "            variation_code =\"000\"\n",
    "        else:\n",
    "            model = mobilenet_v1(input_shape, classes=classes, alpha=alpha, loop_depth=loop_depth, head=head)\n",
    "            variation_code =\"l\"+str(loop_depth)+\".\"+str(head)\n",
    "        base_model_name = model.name\n",
    "\n",
    "    elif architecture==\"mobilenet_v1_vvw\":\n",
    "        model = mobilenet_v1_vvw(input_shape, classes=classes, alpha=alpha)\n",
    "        variation_code =\"vvw\"\n",
    "        base_model_name = model.name\n",
    "\n",
    "    elif architecture==\"mobilenet_v1_keras\":\n",
    "        model = mobilenet_v1_keras(input_shape, classes=classes, alpha=alpha)\n",
    "        variation_code =\"keras\"\n",
    "        base_model_name = model.name.split(\"_\")[0]+\"v1\"\n",
    "\n",
    "    elif architecture==\"mobilenet_v2\":\n",
    "        if loop_depth==None:\n",
    "            model = mobilenet_v2(input_shape, classes=classes, alpha=alpha)\n",
    "            variation_code =\"000\"\n",
    "        else:\n",
    "            model = mobilenet_v2(input_shape, classes=classes, alpha=alpha, loop_depth=loop_depth)\n",
    "            variation_code =\"l\"+str(loop_depth)\n",
    "        base_model_name = model.name\n",
    "\n",
    "    # my personal mobilenet version    \n",
    "    elif architecture==\"mobilenet_vme\":\n",
    "        if loop_depth==None:\n",
    "            model = mobilenet_vme(input_shape, classes=classes, alpha=alpha)\n",
    "            variation_code =\"000\"\n",
    "        else:\n",
    "            model = mobilenet_vme(input_shape, classes=classes, alpha=alpha, loop_depth=loop_depth)\n",
    "            variation_code =\"l\"+str(loop_depth)\n",
    "        base_model_name = model.name\n",
    "    elif architecture==\"mobilenet_v2_keras\":\n",
    "        model = mobilenet_v2_keras(input_shape, classes=classes, alpha=alpha)\n",
    "        variation_code =\"keras\"\n",
    "        base_model_name = model.name.split(\"_\")[0]#+\"v2\"\n",
    "    elif architecture== \"mobilenet_v3_small_keras\":\n",
    "        model = mobilenet_v3_small_keras(input_shape, classes=classes, alpha=alpha)\n",
    "        variation_code =\"keras\"\n",
    "        base_model_name = model.name.split(\"_\")[0]\n",
    "    elif architecture== \"mobilenet_v3_large_keras\":\n",
    "        model = mobilenet_v3_large_keras(input_shape, classes=classes, alpha=alpha)\n",
    "        variation_code =\"keras\"\n",
    "        base_model_name = model.name.split(\"_\")[0]\n",
    "    elif architecture==\"efficientNetB0_keras\":\n",
    "        model = efficientNetB0_keras(input_shape, classes=classes, alpha=alpha)\n",
    "        variation_code =\"keras\"\n",
    "        base_model_name = \"efficientNetB0\"\n",
    "    elif architecture==\"shufflenet_v1\":\n",
    "        model = shufflenet_v1(input_shape, classes=classes, alpha= alpha, groups=groups)\n",
    "        variation_code =\"g\"+str(groups)\n",
    "        base_model_name = model.name\n",
    "\n",
    "    elif architecture==\"shufflenet_v1keras\":\n",
    "        model = ShuffleNet_keras(input_shape, classes=classes, include_top=True, weights=None, nb_groups=groups)\n",
    "        variation_code==\"g\"+str(groups)\n",
    "        base_model_name = model.name\n",
    "\n",
    "\n",
    "    elif architecture==\"shufflenet_v2\":\n",
    "        model = shufflenet_v2(input_shape, classes=classes, alpha= alpha)\n",
    "        variation_code =\"000\"\n",
    "        base_model_name = model.name\n",
    "\n",
    "    elif architecture==\"shufflenet_v2tiny\":\n",
    "        if (first_layer_channels==None) |(last_layer_channels==None) :\n",
    "            model = shufflenet_v2tiny(input_shape, classes, alpha, first_layer_channels=24, last_layer_channels=1024, use_bias=False) \n",
    "            variation_code =\"000\"\n",
    "        else:\n",
    "            model = shufflenet_v2tiny(input_shape, classes, alpha, first_layer_channels=first_layer_channels, last_layer_channels=last_layer_channels, use_bias=False) \n",
    "            variation_code = \"f\"+str(first_layer_channels)+\"l\"+str(last_layer_channels)\n",
    "        base_model_name = model.name\n",
    "    else:\n",
    "        #raise Exception e:\n",
    "        print(f\"Model architecture {architecture} is not supported.\")\n",
    "\n",
    "    return model, variation_code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"re_lu_14\" \"                 f\"(type ReLU).\n\nAttempt to convert a value (<keras.layers.merging.concatenate.Concatenate object at 0x00000198C4DFF100>) with an unsupported type (<class 'keras.layers.merging.concatenate.Concatenate'>) to a Tensor.\n\nCall arguments received by layer \"re_lu_14\" \"                 f\"(type ReLU):\n  • inputs=<keras.layers.merging.concatenate.Concatenate object at 0x00000198C4DFF100>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [310], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model, variation_code \u001b[39m=\u001b[39m get_model(architecture, input_shape, classes, alpha, loop_depth, first_layer_channels\u001b[39m=\u001b[39;49mfirst_layer_channels, last_layer_channels\u001b[39m=\u001b[39;49mlast_layer_channels)\n\u001b[0;32m      2\u001b[0m base_model_name\n",
      "Cell \u001b[1;32mIn [298], line 57\u001b[0m, in \u001b[0;36mget_model\u001b[1;34m(architecture, input_shape, classes, alpha, loop_depth, first_layer_channels, last_layer_channels, use_bias, head)\u001b[0m\n\u001b[0;32m     55\u001b[0m     base_model_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mefficientNetB0\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     56\u001b[0m \u001b[39melif\u001b[39;00m architecture\u001b[39m==\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mshufflenet_v1\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m---> 57\u001b[0m     model \u001b[39m=\u001b[39m shufflenet_v1(input_shape, classes\u001b[39m=\u001b[39;49mclasses, alpha\u001b[39m=\u001b[39;49m alpha, groups\u001b[39m=\u001b[39;49mgroups)\n\u001b[0;32m     58\u001b[0m     variation_code \u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mg\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(groups)\n\u001b[0;32m     59\u001b[0m     base_model_name \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mname\n",
      "Cell \u001b[1;32mIn [309], line 68\u001b[0m, in \u001b[0;36mshufflenet_v1\u001b[1;34m(input_shape, classes, alpha, groups)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mfor\u001b[39;00m i,repetition \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(repetitions):\n\u001b[0;32m     66\u001b[0m     channels \u001b[39m=\u001b[39m start_channels \u001b[39m*\u001b[39m (\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mi)\n\u001b[1;32m---> 68\u001b[0m     x  \u001b[39m=\u001b[39m shuffle_unit(x, groups, channels,strides \u001b[39m=\u001b[39;49m (\u001b[39m2\u001b[39;49m,\u001b[39m2\u001b[39;49m))\n\u001b[0;32m     70\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(repetition):\n\u001b[0;32m     71\u001b[0m         x \u001b[39m=\u001b[39m shuffle_unit(x, groups, channels,strides\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m))\n",
      "Cell \u001b[1;32mIn [309], line 47\u001b[0m, in \u001b[0;36mshuffle_unit\u001b[1;34m(x, groups, channels, strides)\u001b[0m\n\u001b[0;32m     44\u001b[0m     y \u001b[39m=\u001b[39m AvgPool2D((\u001b[39m3\u001b[39m,\u001b[39m3\u001b[39m), strides \u001b[39m=\u001b[39m (\u001b[39m2\u001b[39m,\u001b[39m2\u001b[39m), padding \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39msame\u001b[39m\u001b[39m'\u001b[39m)(y)\n\u001b[0;32m     45\u001b[0m     x \u001b[39m=\u001b[39m Concatenate([x,y])\n\u001b[1;32m---> 47\u001b[0m x \u001b[39m=\u001b[39m ReLU()(x)\n\u001b[0;32m     50\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32md:\\Miniconda\\envs\\tiny_cnn_6\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32md:\\Miniconda\\envs\\tiny_cnn_6\\lib\\site-packages\\keras\\backend.py:5366\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(x, alpha, max_value, threshold)\u001b[0m\n\u001b[0;32m   5364\u001b[0m     clip_max \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   5365\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 5366\u001b[0m     x \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mrelu(x)\n\u001b[0;32m   5368\u001b[0m \u001b[39mif\u001b[39;00m clip_max:\n\u001b[0;32m   5369\u001b[0m     max_value \u001b[39m=\u001b[39m _constant_to_tensor(max_value, x\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mbase_dtype)\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer \"re_lu_14\" \"                 f\"(type ReLU).\n\nAttempt to convert a value (<keras.layers.merging.concatenate.Concatenate object at 0x00000198C4DFF100>) with an unsupported type (<class 'keras.layers.merging.concatenate.Concatenate'>) to a Tensor.\n\nCall arguments received by layer \"re_lu_14\" \"                 f\"(type ReLU):\n  • inputs=<keras.layers.merging.concatenate.Concatenate object at 0x00000198C4DFF100>"
     ]
    }
   ],
   "source": [
    "model, variation_code = get_model(architecture, input_shape, classes, alpha, loop_depth, first_layer_channels=first_layer_channels, last_layer_channels=last_layer_channels)\n",
    "base_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variation_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for layer in model.layers:\n",
    "#     if type(layer)==type(tf.keras.layers.Dropout(0)):\n",
    "#         print(layer.rate)\n",
    "#         print(layer.trainable)\n",
    "#         layer.rate=0.5\n",
    "#         print(layer.rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for layer in model.layers:\n",
    "#     if type(layer)==type(tf.keras.layers.BatchNormalization()):\n",
    "#         print(layer.momentum)\n",
    "#         print(layer.trainable)\n",
    "#         #layer.momentum=0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.optimizer.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model name\n",
    "\n",
    "model_name = create_model_name(base_model_name, alpha, input_shape, classes, variation_code)\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the filepath structure\n",
    "(\n",
    "    models_path,\n",
    "    models_summary_path,\n",
    "    models_image_path,\n",
    "    models_layer_df_path,\n",
    "    models_tf_path,\n",
    "    models_tflite_path,\n",
    "    models_tflite_opt_path,\n",
    ") = create_filepaths(model_name)\n",
    "\n",
    "# mobilenet_v1 = keras.models.load_model(models_tf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show model in local version of Netron.app\n",
    "\n",
    "if automated == False:\n",
    "    view_model(model, tflite=True, build=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(\n",
    "    model,\n",
    "    to_file=models_image_path,\n",
    "    show_shapes=True,\n",
    "    show_dtype=False,\n",
    "    show_layer_names=True,\n",
    "    rankdir=\"TB\",  # TB for vertical plot, LR for horizontal plot\n",
    "    expand_nested=True,\n",
    "    layer_range=None,\n",
    "    dpi=200,\n",
    "    show_layer_activations=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mltk_summary = summarize_model(model)\n",
    "print(mltk_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model summary to disk\n",
    "\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "with open(models_summary_path, \"w\", encoding='utf-8') as f:\n",
    "    with redirect_stdout(f):\n",
    "        print(mltk_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(models_tf_path)\n",
    "models_tf_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"analyzer_list.txt\", \"a\") as f:\n",
    "    f.write(f\"{model_name}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if automated == False:\n",
    "\n",
    "    ! explorer $models_path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linux playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! wsl ls ./models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! wsl pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! wsl ls ./models -l >model.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! wsl ls ./models -laR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linux_path = models_tflite_path.as_posix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_tf_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tiny_cnn_6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0faa6c31b20b8f809b81d6d7d22a84ccd9f354666f54133d1793fa4c65539801"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
