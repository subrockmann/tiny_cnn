{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, math, datetime, csv\n",
    "import psutil\n",
    "\n",
    "# import pathlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import PIL\n",
    "\n",
    "import PIL.Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# see https://github.com/microsoft/pylance-release/issues/1066\n",
    "#from tensorflow import keras\n",
    "keras = tf.keras\n",
    "from keras.layers import (\n",
    "    Input,\n",
    "    Dense,\n",
    "    Add,\n",
    "    Flatten,\n",
    "    Conv2D,\n",
    "    Dropout,\n",
    "    Reshape,\n",
    "    Activation,\n",
    "    DepthwiseConv2D,\n",
    "    MaxPooling2D,\n",
    "    AvgPool2D,\n",
    "    GlobalAveragePooling2D,\n",
    "    Softmax,\n",
    "    BatchNormalization,\n",
    "    Concatenate,\n",
    "    Permute,\n",
    "    ReLU\n",
    ")\n",
    "\n",
    "from keras.models import Model\n",
    "#from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "#from dotenv import load_dotenv\n",
    "#from tf2cv.model_provider import get_model as tf2cv_get_model\n",
    "\n",
    "# Import the necessary MLTK APIs\n",
    "from mltk.core import view_model, summarize_model, profile_model\n",
    "\n",
    "from workbench.config.config import initialize\n",
    "from workbench.utils.utils import create_filepaths, get_file_size, create_model_name, append_dict_to_csv\n",
    "from workbench.tensorflow import set_batchnorm_momentum\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "# import deeplake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_1 = 1\n",
    "seed_2 = 15\n",
    "seed_3 = 30\n",
    "seed_4 = 42\n",
    "seed_5 = 75\n",
    "\n",
    "seed = seed_1\n",
    "\n",
    "# set the random seeds#\n",
    "#os.environ[\"TF_CUDNN_DETERMINISTIC\"]= \"1\"\n",
    "# random.seed(seed)\n",
    "# np.random.seed(seed)\n",
    "# tf.random.set_seed(seed) # setting tensorflow global seed\n",
    "tf.keras.utils.set_random_seed(seed)\n",
    "#tf.config.experimental.enable_op_determinism()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "img_res = 224\n",
    "channels = 3\n",
    "classes = 3\n",
    "alpha = 1\n",
    "global dropout_rate\n",
    "dropout_rate = 0.001 # 0.2\n",
    "architecture = \"mobilenet_v2_keras\" # \"mobilenet_v3_small_keras\" ##\"shufflenet_v2tiny\" # ##\"mobilenet_v3_large_keras\"  #\"efficientNetB0_keras\" #\"mobilenet_v2_keras\" #\n",
    "loop_depth = 5\n",
    "groups = 1 # [1, 2, 3, 4, 8] used in shuffleNetv1\n",
    "first_layer_channels = 4 # standard 24\n",
    "last_layer_channels = 1024 # standard 1024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (img_res,img_res,channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_stats = {\"img_res\" : img_res,\n",
    "    \"classes\" : classes,\n",
    "    \"channels\" : channels,\n",
    "    \"alpha\" : alpha,\n",
    "    \"dropout_rate\" : dropout_rate,\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_batchnorm_momentum(model, momentum=0.9):\n",
    "    for layer in model.layers:\n",
    "        if type(layer)==type(tf.keras.layers.BatchNormalization()):\n",
    "            #print(layer.momentum)\n",
    "            layer.momentum=momentum\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MobileNet V1 & V2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fast downsampling mobilenet https://github.com/qinzheng93/FD-MobileNet/blob/master/pyvision/models/ImageNet/MobileNet.py  \n",
    "\n",
    "Keras Effnet https://github.com/arthurdouillard/keras-effnet/blob/master/effnet.py  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mobilenet_v1(input_shape, classes, alpha=1, loop_depth=5, global_average_pooling=True):\n",
    "    \"\"\"\n",
    "    This function builds a CNN model according to the MobileNet V1 specification, using the functional API.\n",
    "    The function returns the model.\n",
    "    \"\"\"\n",
    "    global dropout_rate\n",
    "    \n",
    "    # MobileNet V1 Block\n",
    "    def mobilenet_v1_block(x, filters, strides):\n",
    "        # Depthwise convolution\n",
    "        x = DepthwiseConv2D(kernel_size=3, strides=strides, padding=\"same\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = ReLU()(x)  # TODO: option to change to ReLu6 or HardSwish\n",
    "\n",
    "        # Pointwise convolution = standard convolution with kernel size =1\n",
    "        x = Conv2D(filters=filters, kernel_size=1, strides=1)(x)  # strides for pointwise convolution must be 1\n",
    "        x = BatchNormalization()(x)\n",
    "        x = ReLU()(x)  # TODO: option to change to ReLu6 or HardSwish\n",
    "\n",
    "        return x\n",
    "\n",
    "    # Stem of the model\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Conv2D(filters=32 * alpha, kernel_size=3, strides=2, padding=\"same\")(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)  # TODO: option to change to ReLu6 or HardSwish\n",
    "\n",
    "    # Main part of the model\n",
    "    x = mobilenet_v1_block(x, filters=64 * alpha, strides=1)\n",
    "    x = mobilenet_v1_block(x, filters=128 * alpha, strides=2)\n",
    "    x = mobilenet_v1_block(x, filters=128 * alpha, strides=1)\n",
    "    x = mobilenet_v1_block(x, filters=256 * alpha, strides=2)\n",
    "    x = mobilenet_v1_block(x, filters=256 * alpha, strides=1)\n",
    "    x = mobilenet_v1_block(x, filters=512 * alpha, strides=2)\n",
    "\n",
    "    for _ in range(loop_depth):  # TODO: reduce the depth of the net for faster inference\n",
    "        x = mobilenet_v1_block(x, filters=512 * alpha, strides=1)\n",
    "\n",
    "    x = mobilenet_v1_block(x, filters=1024 * alpha, strides=2)\n",
    "    x = mobilenet_v1_block(x, filters=1024 * alpha, strides=1)\n",
    "\n",
    "    if global_average_pooling:  \n",
    "        x = GlobalAveragePooling2D(keepdims=True)(x) #\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "        x = Conv2D(filters=classes, kernel_size=1, strides=1, padding=\"same\")(x)\n",
    "        x = Reshape(target_shape=(classes,))(x)\n",
    "        outputs = Activation(\"softmax\")(x)\n",
    "\n",
    "    else:\n",
    "        # use the original implementation from the paper with average pooling and fully-connected layers\n",
    "        x = AvgPool2D(pool_size=x.shape[-1], strides=1)(x)  # TODO: pool_size is dependent on the input resolution, lower resolutions than 224 might crash the architecture\n",
    "        outputs = Dense(units=classes, activation=\"softmax\")(x)  \n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs, name=\"mobilenetv1\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### MOBILENET V2 not yet correctly implemented!\n",
    "\n",
    "\n",
    "# Formular to avoid exccessive downsampling\n",
    "# copied from: https://github.com/keras-team/keras/blob/v2.11.0/keras/applications/mobilenet_v2.py#L563\n",
    "\n",
    "def _make_divisible(v, divisor, min_value=None):\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "\n",
    "\n",
    "def mobilenet_v2(input_shape, classes, alpha=1, loop_depth=5, global_average_pooling=True):\n",
    "    \"\"\"\n",
    "    This function builds a CNN model according to the MobileNet V2 specification, using the functional API.\n",
    "    The function returns the model.\n",
    "    \"\"\"\n",
    "\n",
    "    global block_id\n",
    "    block_id = 1\n",
    "    global dropout_rate\n",
    "\n",
    "    # Expansion block\n",
    "    def expansion_block(x, t, filters, block_id):\n",
    "        prefix = f\"block_{block_id}_\"\n",
    "        x = Conv2D(kernel_size=1, filters = filters * t, use_bias=False, name= prefix + \"expand\")(x)\n",
    "        x = BatchNormalization(name = prefix + \"expand_bn\")(x)\n",
    "        x = ReLU(6, name=prefix + \"expand_relu\")(x)\n",
    "        return x\n",
    "\n",
    "    def depthwise_block(x, strides, block_id):\n",
    "        prefix = f\"block_{block_id}_\"\n",
    "        x = DepthwiseConv2D(kernel_size=3, strides=strides, padding=\"same\", use_bias=False, name= prefix + \"dw_conv\")(x)\n",
    "        x = BatchNormalization(name = prefix + \"dw_bn\")(x)\n",
    "        x = ReLU(6, name=prefix + \"dw_relu\")(x)\n",
    "        return x\n",
    "\n",
    "    def projection_block(x, out_channels, block_id):\n",
    "        prefix = f\"block_{block_id}_\"\n",
    "        x = Conv2D(filters=out_channels, kernel_size=1, strides=1, padding=\"same\", use_bias=False, name= prefix + \"compress\")(x)\n",
    "        x = BatchNormalization(name = prefix + \"compress_bn\")(x)\n",
    "        return x\n",
    "\n",
    "    def bottleneck_residual_block(x, t, filters, out_channels, strides):\n",
    "        global block_id\n",
    "        block_id =  block_id +1\n",
    "        y = expansion_block(x, t, filters, block_id)\n",
    "        y = depthwise_block(y, strides, block_id)\n",
    "        y = projection_block(y, out_channels, block_id)\n",
    "\n",
    "        # TODO: Check if this implementation is ok\n",
    "        if y.shape[-1] == x.shape[-1]:\n",
    "            y = Add()([x, y])\n",
    "        return y\n",
    "\n",
    "    # Avoid massive downsampling\n",
    "    first_block_filters = _make_divisible(32 * alpha, 8)\n",
    "\n",
    "    # Stem of the model\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Conv2D(filters=first_block_filters, kernel_size=3, strides=2, padding=\"same\", use_bias=False)(inputs)\n",
    "    x = BatchNormalization(name='conv1_bn')(x)\n",
    "    x = ReLU(6, name = 'conv1_relu')(x)  # TODO: option to change to ReLu6 or HardSwish\n",
    "\n",
    "\n",
    "    # Main part of the model\n",
    "    block_id = 0\n",
    "    x = depthwise_block(x, strides=1, block_id=block_id)\n",
    "    x = projection_block(x, out_channels=16 * alpha, block_id=block_id)\n",
    "\n",
    "    # 2 identical layers\n",
    "    x  = bottleneck_residual_block(x, t=6, filters=x.shape[-1], out_channels=24 * alpha, strides=2) #, block_id=block_id +1)\n",
    "    x  = bottleneck_residual_block(x, t=6, filters=x.shape[-1], out_channels=24 * alpha, strides=1) #, block_id=block_id +1)\n",
    "\n",
    "    # 3 repeated bottle_necks\n",
    "    x = bottleneck_residual_block(x, t=6, filters=x.shape[-1], out_channels=32 * alpha, strides=2) #, block_id=block_id +1)   \n",
    "    x = bottleneck_residual_block(x, t=6, filters=x.shape[-1], out_channels=32 * alpha, strides=1) #, block_id=block_id +1)   \n",
    "    x = bottleneck_residual_block(x, t=6, filters=x.shape[-1], out_channels=32 * alpha, strides=1) #, block_id=block_id +1)   \n",
    "\n",
    "    # 4 repeated bottle_necks\n",
    "    x = bottleneck_residual_block(x, t=6, filters=x.shape[-1], out_channels=64 * alpha, strides=2) #, block_id=block_id +1)   \n",
    "    x = bottleneck_residual_block(x, t=6, filters=x.shape[-1], out_channels=64 * alpha, strides=1) #, block_id=block_id +1)   \n",
    "    x = bottleneck_residual_block(x, t=6, filters=x.shape[-1], out_channels=64 * alpha, strides=1) #, block_id=block_id +1)   \n",
    "    x = bottleneck_residual_block(x, t=6, filters=x.shape[-1], out_channels=64 * alpha, strides=1) #, block_id=block_id +1)   \n",
    "\n",
    "    # 3 repeated bottle_necks\n",
    "    x = bottleneck_residual_block(x, t=6, filters=x.shape[-1], out_channels=96 * alpha, strides=1) #, block_id=block_id +1)   \n",
    "    x = bottleneck_residual_block(x, t=6, filters=x.shape[-1], out_channels=96 * alpha, strides=1) #, block_id=block_id +1)   \n",
    "    x = bottleneck_residual_block(x, t=6, filters=x.shape[-1], out_channels=96 * alpha, strides=1) #, block_id=block_id +1)   \n",
    "\n",
    "    # 3 repeated bottle_necks\n",
    "    x = bottleneck_residual_block(x, t=6, filters=x.shape[-1], out_channels=160 * alpha, strides=2) #, block_id=block_id +1)   \n",
    "    x = bottleneck_residual_block(x, t=6, filters=x.shape[-1], out_channels=160 * alpha, strides=1) #, block_id=block_id +1)   \n",
    "    x = bottleneck_residual_block(x, t=6, filters=x.shape[-1], out_channels=160 * alpha, strides=1) #, block_id=block_id +1)   \n",
    "\n",
    "    x = bottleneck_residual_block(x, t=6, filters=x.shape[-1], out_channels=320 * alpha, strides=1) #, block_id=block_id +1) \n",
    "\n",
    "\n",
    "    # no alpha applied to last conv as stated in the paper:\n",
    "    # if the width multiplier is greater than 1 we increase the number of output\n",
    "    # channels.\n",
    "    if alpha > 1.0:\n",
    "        last_block_filters = _make_divisible(1280 * alpha, 8)\n",
    "    else:\n",
    "        last_block_filters = 1280\n",
    "\n",
    "    # 1*1 conv\n",
    "    x = Conv2D(filters=last_block_filters, kernel_size=1, padding=\"same\", use_bias=False, name=\"last_conv\")(x)\n",
    "    x = BatchNormalization(name='last_bn')(x)\n",
    "    x = ReLU(6,name='last_relu')(x)\n",
    "\n",
    "\n",
    "    if global_average_pooling:  \n",
    "        x = GlobalAveragePooling2D(keepdims=True)(x)\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "        x = Conv2D(filters=classes, kernel_size=1, strides=1, padding=\"same\")(x)\n",
    "        x = Reshape(target_shape=(classes,))(x)\n",
    "        #outputs = Activation(\"softmax\")(x)\n",
    "        outputs = Softmax()(x)\n",
    "\n",
    "\n",
    "    else:\n",
    "        # use the original implementation from the paper with average pooling and fully-connected layers\n",
    "        x = AvgPool2D(pool_size=7, strides=1)(x)  # TODO: pool_size is dependent on the input resolution, lower resolutions than 224 might crash the architecture\n",
    "        outputs = Dense(units=classes, activation=\"softmax\")(x)  # TODO: is there a stride=1 implementation in Dense?\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs, name=\"mobilenetv2\")\n",
    "\n",
    "    # https://github.com/tensorflow/tensorflow/issues/36065\n",
    "    # model in the original version does not train because momentum is set to 0.999 by default\n",
    "    model = set_batchnorm_momentum(model, momentum=0.9)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MY VERSION OF MOBILENET V2\n",
    "\n",
    "# Formular to avoid exccessive downsampling\n",
    "# copied from: https://github.com/keras-team/keras/blob/v2.11.0/keras/applications/mobilenet_v2.py#L563\n",
    "\n",
    "def _make_divisible(v, divisor, min_value=None):\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "\n",
    "\n",
    "def mobilenet_vme(input_shape, classes, alpha=1, loop_depth=5, global_average_pooling=True):\n",
    "    \"\"\"\n",
    "    This function builds a CNN model according to the MobileNet V2 specification, using the functional API.\n",
    "    The function returns the model.\n",
    "    \"\"\"\n",
    "\n",
    "    global block_id\n",
    "    block_id = 1\n",
    "    global dropout_rate\n",
    "\n",
    "    # Expansion block\n",
    "    def expansion_block(x, t, filters, block_id):\n",
    "        prefix = f\"block_{block_id}_\"\n",
    "        x = Conv2D(kernel_size=1, filters = filters * t, use_bias=False, name= prefix + \"expand\")(x)\n",
    "        x = BatchNormalization(name = prefix + \"expand_bn\")(x)\n",
    "        x = ReLU(6, name=prefix + \"expand_relu\")(x)\n",
    "        return x\n",
    "\n",
    "    def depthwise_block(x, strides, block_id):\n",
    "        prefix = f\"block_{block_id}_\"\n",
    "        x = DepthwiseConv2D(kernel_size=3, strides=strides, padding=\"same\", use_bias=False, name= prefix + \"dw_conv\")(x)\n",
    "        x = BatchNormalization(name = prefix + \"dw_bn\")(x)\n",
    "        x = ReLU(6, name=prefix + \"dw_relu\")(x)\n",
    "        return x\n",
    "\n",
    "    def projection_block(x, out_channels, block_id):\n",
    "        prefix = f\"block_{block_id}_\"\n",
    "        x = Conv2D(filters=out_channels, kernel_size=1, strides=1, padding=\"same\", use_bias=False, name= prefix + \"compress\")(x)\n",
    "        x = BatchNormalization(name = prefix + \"compress_bn\")(x)\n",
    "        return x\n",
    "\n",
    "    def bottleneck_residual_block(x, t, filters, out_channels, strides):\n",
    "        global block_id\n",
    "        block_id =  block_id +1\n",
    "        y = expansion_block(x, t, filters, block_id)\n",
    "        y = depthwise_block(y, strides, block_id)\n",
    "        y = projection_block(y, out_channels, block_id)\n",
    "\n",
    "        # TODO: Check if this implementation is ok\n",
    "        if y.shape[-1] == x.shape[-1]:\n",
    "            y = Add()([x, y])\n",
    "        return y\n",
    "\n",
    "    # Avoid massive downsampling\n",
    "    first_block_filters = _make_divisible(32 * alpha, 8)\n",
    "\n",
    "    # Stem of the model\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Conv2D(filters=first_block_filters, kernel_size=3, strides=2, padding=\"same\", use_bias=False)(inputs)\n",
    "    x = BatchNormalization(name='conv1_bn')(x)\n",
    "    x = ReLU(6, name = 'conv1_relu')(x)  # TODO: option to change to ReLu6 or HardSwish\n",
    "\n",
    "\n",
    "    # Main part of the model\n",
    "    block_id = 0\n",
    "    x = depthwise_block(x, strides=1, block_id=block_id)\n",
    "    x = projection_block(x, out_channels=16 * alpha, block_id=block_id)\n",
    "\n",
    "    # 2 identical layers\n",
    "    x  = bottleneck_residual_block(x, t=6, filters=x.shape[-1], out_channels=24 * alpha, strides=2) #, block_id=block_id +1)\n",
    "    x  = bottleneck_residual_block(x, t=6, filters=x.shape[-1], out_channels=24 * alpha, strides=1) #, block_id=block_id +1)\n",
    "\n",
    "    # 3 repeated bottle_necks\n",
    "    x = bottleneck_residual_block(x, t=6, filters=x.shape[-1], out_channels=32 * alpha, strides=2) #, block_id=block_id +1)   \n",
    "    x = bottleneck_residual_block(x, t=6, filters=x.shape[-1], out_channels=32 * alpha, strides=1) #, block_id=block_id +1)   \n",
    "    x = bottleneck_residual_block(x, t=6, filters=x.shape[-1], out_channels=32 * alpha, strides=1) #, block_id=block_id +1)   \n",
    "\n",
    "    # 4 repeated bottle_necks\n",
    "    x = bottleneck_residual_block(x, t=6, filters=x.shape[-1], out_channels=64 * alpha, strides=2) #, block_id=block_id +1)   \n",
    "    x = bottleneck_residual_block(x, t=6, filters=x.shape[-1], out_channels=64 * alpha, strides=1) #, block_id=block_id +1)   \n",
    "    x = bottleneck_residual_block(x, t=6, filters=x.shape[-1], out_channels=64 * alpha, strides=1) #, block_id=block_id +1)   \n",
    "    x = bottleneck_residual_block(x, t=6, filters=x.shape[-1], out_channels=64 * alpha, strides=1) #, block_id=block_id +1)   \n",
    "\n",
    "    # 3 repeated bottle_necks\n",
    "    x = bottleneck_residual_block(x, t=6, filters=x.shape[-1], out_channels=96 * alpha, strides=1) #, block_id=block_id +1)   \n",
    "    x = bottleneck_residual_block(x, t=6, filters=x.shape[-1], out_channels=96 * alpha, strides=1) #, block_id=block_id +1)   \n",
    "    x = bottleneck_residual_block(x, t=6, filters=x.shape[-1], out_channels=96 * alpha, strides=1) #, block_id=block_id +1)   \n",
    "\n",
    "    # 3 repeated bottle_necks\n",
    "    x = bottleneck_residual_block(x, t=6, filters=x.shape[-1], out_channels=160 * alpha, strides=2) #, block_id=block_id +1)   \n",
    "    x = bottleneck_residual_block(x, t=6, filters=x.shape[-1], out_channels=160 * alpha, strides=1) #, block_id=block_id +1)   \n",
    "    x = bottleneck_residual_block(x, t=6, filters=x.shape[-1], out_channels=160 * alpha, strides=1) #, block_id=block_id +1)   \n",
    "\n",
    "    x = bottleneck_residual_block(x, t=6, filters=x.shape[-1], out_channels=320 * alpha, strides=1) #, block_id=block_id +1) \n",
    "\n",
    "\n",
    "    # no alpha applied to last conv as stated in the paper:\n",
    "    # if the width multiplier is greater than 1 we increase the number of output\n",
    "    # channels.\n",
    "    if alpha > 1.0:\n",
    "        last_block_filters = _make_divisible(1280 * alpha, 8)\n",
    "    else:\n",
    "        last_block_filters = 1280\n",
    "\n",
    "    # 1*1 conv\n",
    "    x = Conv2D(filters=last_block_filters, kernel_size=1, padding=\"same\", use_bias=False, name=\"last_conv\")(x)\n",
    "    x = BatchNormalization(name='last_bn')(x)\n",
    "    x = ReLU(6,name='last_relu')(x)\n",
    "\n",
    "\n",
    "    if global_average_pooling:  \n",
    "        x = GlobalAveragePooling2D(keepdims=True)(x)\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "        x = Conv2D(filters=classes, kernel_size=1, strides=1, padding=\"same\")(x)\n",
    "        x = Reshape(target_shape=(classes,))(x)\n",
    "        outputs = Activation(\"softmax\")(x)\n",
    "        #outputs = Softmax()(x)\n",
    "\n",
    "\n",
    "    else:\n",
    "        # use the original implementation from the paper with average pooling and fully-connected layers\n",
    "        x = AvgPool2D(pool_size=7, strides=1)(x)  # TODO: pool_size is dependent on the input resolution, lower resolutions than 224 might crash the architecture\n",
    "        outputs = Dense(units=classes, activation=\"softmax\")(x)  # TODO: is there a stride=1 implementation in Dense?\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs, name=\"mobilenetvme\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # form https://github.com/keras-team/keras/blob/v2.11.0/keras/applications/mobilenet_v2.py#L96-L485\n",
    "# def _inverted_res_block(inputs, expansion, stride, alpha, filters, block_id):\n",
    "#     \"\"\"Inverted ResNet block.\"\"\"\n",
    "#     channel_axis = 1 if backend.image_data_format() == \"channels_first\" else -1\n",
    "\n",
    "#     in_channels = backend.int_shape(inputs)[channel_axis]\n",
    "#     pointwise_conv_filters = int(filters * alpha)\n",
    "#     # Ensure the number of filters on the last 1x1 convolution is divisible by\n",
    "#     # 8.\n",
    "#     pointwise_filters = _make_divisible(pointwise_conv_filters, 8)\n",
    "#     x = inputs\n",
    "#     prefix = f\"block_{block_id}_\"\n",
    "\n",
    "#     if block_id:\n",
    "#         # Expand with a pointwise 1x1 convolution.\n",
    "#         x = layers.Conv2D(\n",
    "#             expansion * in_channels,\n",
    "#             kernel_size=1,\n",
    "#             padding=\"same\",\n",
    "#             use_bias=False,\n",
    "#             activation=None,\n",
    "#             name=prefix + \"expand\",\n",
    "#         )(x)\n",
    "#         x = layers.BatchNormalization(\n",
    "#             axis=channel_axis,\n",
    "#             epsilon=1e-3,\n",
    "#             momentum=0.999,\n",
    "#             name=prefix + \"expand_BN\",\n",
    "#         )(x)\n",
    "#         x = layers.ReLU(6.0, name=prefix + \"expand_relu\")(x)\n",
    "#     else:\n",
    "#         prefix = \"expanded_conv_\"\n",
    "\n",
    "#     # Depthwise 3x3 convolution.\n",
    "#     if stride == 2:\n",
    "#         x = layers.ZeroPadding2D(\n",
    "#             padding=imagenet_utils.correct_pad(x, 3), name=prefix + \"pad\"\n",
    "#         )(x)\n",
    "#     x = layers.DepthwiseConv2D(\n",
    "#         kernel_size=3,\n",
    "#         strides=stride,\n",
    "#         activation=None,\n",
    "#         use_bias=False,\n",
    "#         padding=\"same\" if stride == 1 else \"valid\",\n",
    "#         name=prefix + \"depthwise\",\n",
    "#     )(x)\n",
    "#     x = layers.BatchNormalization(\n",
    "#         axis=channel_axis,\n",
    "#         epsilon=1e-3,\n",
    "#         momentum=0.999,\n",
    "#         name=prefix + \"depthwise_BN\",\n",
    "#     )(x)\n",
    "\n",
    "#     x = layers.ReLU(6.0, name=prefix + \"depthwise_relu\")(x)\n",
    "\n",
    "#     # Project with a pointwise 1x1 convolution.\n",
    "#     x = layers.Conv2D(\n",
    "#         pointwise_filters,\n",
    "#         kernel_size=1,\n",
    "#         padding=\"same\",\n",
    "#         use_bias=False,\n",
    "#         activation=None,\n",
    "#         name=prefix + \"project\",\n",
    "#     )(x)\n",
    "#     x = layers.BatchNormalization(\n",
    "#         axis=channel_axis,\n",
    "#         epsilon=1e-3,\n",
    "#         momentum=0.999,\n",
    "#         name=prefix + \"project_BN\",\n",
    "#     )(x)\n",
    "\n",
    "#     if in_channels == pointwise_filters and stride == 1:\n",
    "#         return layers.Add(name=prefix + \"add\")([inputs, x])\n",
    "#     return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mobilenet_v1_keras(input_shape, classes=classes, alpha=alpha):\n",
    "    global dropout_rate\n",
    "    model = tf.keras.applications.mobilenet.MobileNet(\n",
    "        input_shape=input_shape,\n",
    "        alpha=alpha,\n",
    "        depth_multiplier=1,\n",
    "        dropout=dropout_rate,\n",
    "        include_top=True,\n",
    "        weights=None, #'imagenet'\n",
    "        input_tensor=None,\n",
    "        pooling=None,\n",
    "        classes=classes,\n",
    "        classifier_activation='softmax',\n",
    "        #**kwargs\n",
    "    )\n",
    "\n",
    "    #model._name = model.name + \"_keras\" # model.name cannot be overritten\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mobilenet_v2_keras(input_shape, classes=classes, alpha=alpha):\n",
    "    model = tf.keras.applications.mobilenet_v2.MobileNetV2(\n",
    "        input_shape=input_shape,\n",
    "        alpha=alpha,\n",
    "        include_top=True, # False should be corect\n",
    "        weights=None, #'imagenet'\n",
    "        input_tensor=None,\n",
    "        pooling=\"avg\",\n",
    "        classes=classes,\n",
    "        classifier_activation='softmax',\n",
    "        #**kwargs\n",
    "    )\n",
    "\n",
    "    # https://github.com/tensorflow/tensorflow/issues/36065\n",
    "    # model in the original version does not train because momentum is set to 0.999 by default\n",
    "    model = set_batchnorm_momentum(model, momentum=0.9)\n",
    "    \n",
    "    #model._name = model.name + \"_keras\" # model.name cannot be overritten\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MobileNet V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mobilenet_v3_small_keras(input_shape, classes=classes, alpha=alpha):\n",
    "    global dropout_rate\n",
    "    model = tf.keras.applications.MobileNetV3Small(\n",
    "        input_shape=input_shape,\n",
    "        minimalistic=False, # TODO find out about this parameter\n",
    "        alpha=alpha,\n",
    "        include_top=True,\n",
    "        weights=None,\n",
    "        input_tensor=None,\n",
    "        classes=classes,\n",
    "        classifier_activation=\"softmax\",\n",
    "        dropout_rate=dropout_rate,\n",
    "        include_preprocessing=False\n",
    "        #**kwargs\n",
    "        )\n",
    "    # https://github.com/tensorflow/tensorflow/issues/36065\n",
    "    # model in the original version does not train because momentum is set to 0.999 by default\n",
    "    model = set_batchnorm_momentum(model, momentum=0.9)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mobilenet_v3_large_keras(input_shape, classes=classes, alpha=alpha):\n",
    "    global dropout_rate\n",
    "    model = tf.keras.applications.MobileNetV3Large(\n",
    "        input_shape=input_shape,\n",
    "        minimalistic=False, # TODO find out about this parameter\n",
    "        alpha=alpha,\n",
    "        include_top=True,\n",
    "        weights=None,\n",
    "        input_tensor=None,\n",
    "        classes=classes,\n",
    "        classifier_activation=\"softmax\",\n",
    "        dropout_rate=dropout_rate,\n",
    "        include_preprocessing=False\n",
    "        #**kwargs\n",
    "        )\n",
    "\n",
    "    # https://github.com/tensorflow/tensorflow/issues/36065\n",
    "    # model in the original version does not train because momentum is set to 0.999 by default\n",
    "    model = set_batchnorm_momentum(model, momentum=0.9)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def efficientNetB0_keras(input_shape, classes= classes, alpha= alpha):\n",
    "    model = tf.keras.applications.efficientnet.EfficientNetB0(\n",
    "        include_top=True,\n",
    "        weights= None, #'imagenet',\n",
    "        input_tensor=None,\n",
    "        input_shape=input_shape,\n",
    "        pooling=None,\n",
    "        classes=classes,\n",
    "        classifier_activation='softmax',\n",
    "        #**kwargs\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ShuffleNets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shufflenet_v1(input_shape, classes=classes, alpha= alpha, groups = 1):\n",
    "    # inspired by https://github.com/Haikoitoh/paper-implementation/blob/main/ShuffleNet.ipynb\n",
    "\n",
    "    #groups = 2 # [1, 2, 3, 4, 8]\n",
    "\n",
    "    start_channels_dict={\n",
    "        \"1\" : 144,\n",
    "        \"2\" : 200,\n",
    "        \"3\" : 240,\n",
    "        \"4\" : 272,\n",
    "        \"8\" : 384\n",
    "    }\n",
    "\n",
    "    start_channels = start_channels_dict[str(groups)]\n",
    "\n",
    "    def channel_shuffle(x, groups):  \n",
    "        batch, width, height, channels = x.get_shape().as_list()\n",
    "        group_ch = channels // groups\n",
    "\n",
    "        x = Reshape([width, height, group_ch, groups])(x)\n",
    "        x = Permute([1, 2, 4, 3])(x)\n",
    "        x = Reshape([width, height, channels])(x)\n",
    "        return x\n",
    "\n",
    "    def shuffle_unit(x, groups, channels, strides):\n",
    "\n",
    "        y = x\n",
    "        x = Conv2D(channels, kernel_size = 1, strides = (1,1),padding = 'same', groups=groups)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = ReLU()(x)\n",
    "\n",
    "        x = channel_shuffle(x, groups)\n",
    "        \n",
    "        x = DepthwiseConv2D(kernel_size = (3,3), strides = strides, padding = 'same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "\n",
    "        if strides == (2,2):\n",
    "            channels = channels - y.shape[-1]\n",
    "        x = Conv2D(channels, kernel_size = 1, strides = (1,1),padding = 'same', groups=groups)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "\n",
    "        if strides ==(1,1):\n",
    "            x =Add()([x,y])\n",
    "        if strides == (2,2):\n",
    "            y = AvgPool2D((3,3), strides = (2,2), padding = 'same')(y)\n",
    "            x = Concatenate(axis=-1)([x,y]) # TODO: check if the axis is correct!\n",
    "        \n",
    "        x = ReLU()(x)\n",
    "        return x\n",
    "\n",
    "    # Main architecture\n",
    "\n",
    "    input = Input (input_shape)\n",
    "\n",
    "    x =  Conv2D (24,kernel_size=3,strides = (2,2), padding = 'same', use_bias = True)(input)\n",
    "    x =  BatchNormalization()(x)\n",
    "    x =  ReLU()(x)\n",
    "    \n",
    "    x = MaxPooling2D (pool_size=(3,3), strides = 2, padding='same')(x)\n",
    "\n",
    "    repetitions = [3,7,3]\n",
    "\n",
    "    for i,repetition in enumerate(repetitions):\n",
    "        channels = start_channels * (2**i)\n",
    "\n",
    "        x  = shuffle_unit(x, groups, channels * alpha ,strides = (2,2))\n",
    "\n",
    "        for i in range(repetition):\n",
    "            x = shuffle_unit(x, groups, channels * alpha ,strides=(1,1))\n",
    "\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    output = Dense(classes,activation='softmax')(x)\n",
    "\n",
    "    model = Model(input, output, name=\"shufflenetv1\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shufflenet_v2(input_shape, classes=classes, alpha=alpha, use_bias=False):\n",
    "    \n",
    "\n",
    "    #groups = 2 # [1, 2, 3, 4, 8]\n",
    "\n",
    "    start_channels_dict={  # based on alpha factor\n",
    "        \"0.5\" : 48,\n",
    "        \"1\" : 116,\n",
    "        \"1.5\" : 176,\n",
    "        \"2\" : 244,\n",
    "    }\n",
    "\n",
    "    first_layer_channels = 24\n",
    "    last_layer_channels = 1024\n",
    "\n",
    "    start_channels = start_channels_dict[str(alpha)]\n",
    "\n",
    "    def channel_shuffle(x, groups):  \n",
    "        batch, width, height, channels = x.get_shape().as_list()\n",
    "        group_ch = channels // groups\n",
    "\n",
    "        x = Reshape([width, height, group_ch, groups])(x)\n",
    "        x = Permute([1, 2, 4, 3])(x)\n",
    "        x = Reshape([width, height, channels])(x)\n",
    "        return x\n",
    "\n",
    "    def basic_unit(x, channels):\n",
    "\n",
    "        y,z = tf.split(x, num_or_size_splits=2, axis=-1) # channel split \n",
    "        # batch, width, height, channels = x.get_shape().as_list()\n",
    "        # channel_split = channels//2\n",
    "        # y = x[:, :, :, 0:channel_split+1]\n",
    "        # print(y.shape)\n",
    "        # z = x[:, :, :, channel_split:-1]\n",
    "        # print(z.shape)\n",
    "\n",
    "        y = Conv2D(channels, kernel_size = 1, strides = (1,1) ,padding = 'same', use_bias=use_bias)(y)\n",
    "        y = BatchNormalization()(y)\n",
    "        y = ReLU()(y)\n",
    "        \n",
    "        y = DepthwiseConv2D(kernel_size = (3,3), strides = (1,1), padding = 'same', use_bias=use_bias)(y)\n",
    "        y = BatchNormalization()(y)\n",
    "\n",
    "        y = Conv2D(channels, kernel_size = 1, strides = (1,1),padding = 'same', use_bias=use_bias)(y)\n",
    "        y = BatchNormalization()(y)\n",
    "        y = ReLU()(y)\n",
    "\n",
    "        y = Concatenate(axis=-1)([y,z]) # TODO: check if the axis is correct!\n",
    "        y = channel_shuffle(y, 2)\n",
    "        return y\n",
    "\n",
    "    def down_sampling_unit(x, channels):\n",
    "\n",
    "        y = x\n",
    "        x = Conv2D(channels, kernel_size = 1, strides = (1,1),padding = 'same', use_bias=use_bias)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = ReLU()(x)\n",
    "        \n",
    "        x = DepthwiseConv2D(kernel_size = (3,3), strides = (2,2), padding = 'same', use_bias=use_bias)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "\n",
    "        x = Conv2D(channels, kernel_size = 1, strides = (1,1),padding = 'same', use_bias=use_bias)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = ReLU()(x)\n",
    "\n",
    "        y = DepthwiseConv2D(kernel_size = (3,3), strides = (2,2), padding = 'same', use_bias=use_bias)(y)\n",
    "        y = BatchNormalization()(y)\n",
    "\n",
    "        y = Conv2D(channels, kernel_size = 1, strides = (1,1),padding = 'same', use_bias=use_bias)(y)\n",
    "        y = BatchNormalization()(y)\n",
    "        y = ReLU()(y)\n",
    "\n",
    "        x = Concatenate(axis=-1)([x,y]) # TODO: check if the axis is correct!\n",
    "        x = channel_shuffle(x, 2)\n",
    "        return x\n",
    "\n",
    "    # Main architecture\n",
    "\n",
    "    input = Input (input_shape)\n",
    "\n",
    "    x =  Conv2D (first_layer_channels ,kernel_size=3, strides = (2,2), padding = 'valid', use_bias=use_bias)(input)\n",
    "    x =  BatchNormalization()(x)\n",
    "    x =  ReLU()(x)\n",
    "    \n",
    "    x = MaxPooling2D (pool_size=(3,3), strides = 2, padding='same')(x)\n",
    "\n",
    "    repetitions = [3,7,3]\n",
    "\n",
    "    channels = start_channels\n",
    "\n",
    "    for i,repetition in enumerate(repetitions):\n",
    "\n",
    "        x  = down_sampling_unit(x, channels)\n",
    "\n",
    "        for j in range(repetition):\n",
    "            x = basic_unit(x, channels)\n",
    "\n",
    "        channels = channels * 2 # ShuffleNet V1 *(2**1)\n",
    "\n",
    "    x =  Conv2D (last_layer_channels, kernel_size=1,strides = (1,1), padding = 'same', use_bias=use_bias)(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    output = Dense(classes,activation='softmax')(x)\n",
    "\n",
    "    model = Model(input, output, name=\"shufflenetv2\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shufflenet_v2tiny(input_shape, classes=classes, alpha=alpha, first_layer_channels=24, last_layer_channels=1024, use_bias=False):\n",
    "    \n",
    "\n",
    "    #groups = 2 # [1, 2, 3, 4, 8]\n",
    "\n",
    "    start_channels_dict={  # based on alpha factor\n",
    "        \"0.05\" : 6,\n",
    "        \"0.1\" : 12,\n",
    "        \"0.2\" : 24,\n",
    "        \"0.25\" : 28,\n",
    "        \"0.3\" : 34,        \n",
    "        \"0.5\" : 48, # this is the smallest original architecture alpha\n",
    "        \"1\" : 116,\n",
    "        \"1.5\" : 176,\n",
    "        \"2\" : 244,\n",
    "    }\n",
    "\n",
    "    # first_layer_channels = 24\n",
    "    # last_layer_channels = 1024\n",
    "\n",
    "    start_channels = start_channels_dict[str(alpha)]\n",
    "\n",
    "    def channel_shuffle(x, groups):  \n",
    "        batch, width, height, channels = x.get_shape().as_list()\n",
    "        group_ch = channels // groups\n",
    "\n",
    "        x = Reshape([width, height, group_ch, groups])(x)\n",
    "        x = Permute([1, 2, 4, 3])(x)\n",
    "        x = Reshape([width, height, channels])(x)\n",
    "        return x\n",
    "\n",
    "    #     x = Reshape([width, height, group_ch, groups])(x)\n",
    "    #     x = Permute([1, 2, 4, 3])(x)\n",
    "    #     x = Reshape([width, height, channels])(x)\n",
    "    #     return x\n",
    "\n",
    "    # https://stackoverflow.com/questions/62794840/apply-channel-shuffle-in-tensorflow-or-keras\n",
    "\n",
    "    # trouble version\n",
    "    # def channel_shuffle(x, groups):\n",
    "    #     # g = 2\n",
    "    #     b, h, w, c = x.shape.as_list()\n",
    "    #     x = tf.reshape(x,[-1, h, w, groups, c // groups])\n",
    "    #     x = tf.transpose(x, perm = [0, 1, 2, 4, 3])\n",
    "    #     #x = tf.reverse(x,[-1]) \n",
    "    #     x = tf.reshape(x, [-1, h, w, c])\n",
    "    #     return x\n",
    "\n",
    "\n",
    "    # https://github.com/MG2033/ShuffleNet/blob/master/layers.py#L238\n",
    "    # def channel_shuffle(name, x, num_groups):\n",
    "    #     with tf.variable_scope(name) as scope:\n",
    "    #         n, h, w, c = x.shape.as_list()\n",
    "    #         x_reshaped = tf.reshape(x, [-1, h, w, num_groups, c // num_groups])\n",
    "    #         x_transposed = tf.transpose(x_reshaped, [0, 1, 2, 4, 3])\n",
    "    #         output = tf.reshape(x_transposed, [-1, h, w, c])\n",
    "    #         return output\n",
    "\n",
    "\n",
    "    ## https://github.com/timctho/shufflenet-v2-tensorflow/blob/master/module.py\n",
    "    def channel_shuffle(x, groups):\n",
    "    #with tf.variable_scope('shuffle_unit'):\n",
    "    \n",
    "        n, h, w, c = x.get_shape().as_list()\n",
    "        x = tf.reshape(x, shape=tf.convert_to_tensor([tf.shape(x)[0], h, w, groups, c // groups]))\n",
    "        x = tf.transpose(x, tf.convert_to_tensor([0, 1, 2, 4, 3]))\n",
    "        x = tf.reshape(x, shape=tf.convert_to_tensor([tf.shape(x)[0], h, w, c]))\n",
    "        return x\n",
    "\n",
    "    def basic_unit(x, channels):\n",
    "\n",
    "        #x,y = tf.split(x, num_or_size_splits=2, axis=-1) # channel split \n",
    "\n",
    "        batch, width, height, channels = x.get_shape().as_list()\n",
    "        channel_split = channels//2 \n",
    "        y = x[:, :, :, 0:channel_split]\n",
    "        print(f\"Basic unit y: {y.shape} - channels: {channels}\")\n",
    "        z = x[:, :, :, channel_split-1:-1]\n",
    "        print(f\"Basic unit z: {z.shape} - channels: {channels}\")\n",
    "\n",
    "\n",
    "\n",
    "        y = Conv2D(channels, kernel_size = 1, strides = (1,1) ,padding = 'same', use_bias=use_bias)(y)\n",
    "        y = BatchNormalization()(y)\n",
    "        y = ReLU()(y)\n",
    "        \n",
    "        y = DepthwiseConv2D(kernel_size = (3,3), strides = (1,1), padding = 'same', use_bias=use_bias)(y)\n",
    "        y = BatchNormalization()(y)\n",
    "\n",
    "        y = Conv2D(channels, kernel_size = 1, strides = (1,1),padding = 'same', use_bias=use_bias)(y)\n",
    "        y = BatchNormalization()(y)\n",
    "        y = ReLU()(y)\n",
    "\n",
    "        y = Concatenate(axis=-1)([y,z]) # TODO: check if the axis is correct!\n",
    "        y = channel_shuffle(y, 2) # use 2 groups\n",
    "        return y\n",
    "\n",
    "    def down_sampling_unit(x, channels):\n",
    "\n",
    "        y = x\n",
    "        x = Conv2D(channels, kernel_size = 1, strides = (1,1),padding = 'same', use_bias=use_bias)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = ReLU()(x)\n",
    "        \n",
    "        x = DepthwiseConv2D(kernel_size = (3,3), strides = (2,2), padding = 'same', use_bias=use_bias)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "\n",
    "        x = Conv2D(channels, kernel_size = 1, strides = (1,1),padding = 'same', use_bias=use_bias)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = ReLU()(x)\n",
    "\n",
    "        y = DepthwiseConv2D(kernel_size = (3,3), strides = (2,2), padding = 'same', use_bias=use_bias)(y)\n",
    "        y = BatchNormalization()(y)\n",
    "\n",
    "        y = Conv2D(channels, kernel_size = 1, strides = (1,1),padding = 'same', use_bias=use_bias)(y)\n",
    "        y = BatchNormalization()(y)\n",
    "        y = ReLU()(y)\n",
    "\n",
    "        x = Concatenate(axis=-1)([x,y]) # TODO: check if the axis is correct!\n",
    "        x = channel_shuffle(x, 2)\n",
    "        return x\n",
    "\n",
    "    # Main architecture\n",
    "\n",
    "    input = Input (input_shape)\n",
    "\n",
    "    x =  Conv2D (first_layer_channels ,kernel_size=3, strides = (2,2), padding = 'valid', use_bias=use_bias)(input)\n",
    "    x =  BatchNormalization()(x)\n",
    "    x =  ReLU()(x)\n",
    "    \n",
    "    # exclude pooling layer to avoid model collapse\n",
    "    x = MaxPooling2D (pool_size=(3,3), strides = 2, padding='same')(x)\n",
    "\n",
    "    repetitions = [3,7,3]\n",
    "\n",
    "    channels = start_channels\n",
    "\n",
    "    # stage 2\n",
    "    print(\"stage 2\")\n",
    "    x = down_sampling_unit(x, channels)\n",
    "    for i in range(repetitions[0]):\n",
    "        print(f\"rep {i}, channels {channels}\")\n",
    "        x = basic_unit(x, channels)\n",
    "    \n",
    "    # stage 3\n",
    "    print(\"stage 3\")\n",
    "    channels = channels *2\n",
    "    print(\"channels\")\n",
    "\n",
    "    x = down_sampling_unit(x, channels)\n",
    "    for i in range(repetitions[1]):\n",
    "        print(f\"rep {i}, channels {channels}\")\n",
    "        x = basic_unit(x, channels)\n",
    "\n",
    "    # stage 3\n",
    "    channels = channels *2\n",
    "\n",
    "    x = down_sampling_unit(x, channels)\n",
    "    for i in range(repetitions[2]):\n",
    "        x = basic_unit(x, channels)\n",
    "       \n",
    "\n",
    "    # for i,repetition in enumerate(repetitions):\n",
    "\n",
    "    #     x  = down_sampling_unit(x, channels)\n",
    "\n",
    "    #     for j in range(repetition):\n",
    "    #         x = basic_unit(x, channels)\n",
    "\n",
    "    #     channels = channels * 2 # ShuffleNet V1 *(2**1)\n",
    "\n",
    "    x =  Conv2D (last_layer_channels, kernel_size=1,strides = (1,1), padding = 'same', use_bias=use_bias)(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    output = Dense(classes,activation='softmax')(x)\n",
    "\n",
    "    model = Model(input, output, name=\"shufflenetv2tiny\")\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = net = tf2cv_get_model(\"resnet18\", pretrained=True, data_format=\"channels_last\")\n",
    "#model.build(input_shape)\n",
    "#model.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model = shufflenet_v2(input_shape, classes=classes, alpha= alpha)\n",
    "# model.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = mobilenet_v1(input_shape, classes=classes, alpha=alpha)\n",
    "#model = mobilenet_v2(input_shape, classes=classes, alpha=alpha)\n",
    "#model = mobilenet_v1_keras(input_shape, classes=classes, alpha=alpha)\n",
    "#model = mobilenet_v2_keras(input_shape, classes=classes, alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(architecture, input_shape, classes, alpha, loop_depth= None,first_layer_channels=None, last_layer_channels=None, use_bias=False):\n",
    "    global base_model_name\n",
    "    if architecture==\"mobilenet_v1\":\n",
    "        if loop_depth==None:\n",
    "            model = mobilenet_v1(input_shape, classes=classes, alpha=alpha)\n",
    "            variation_code =\"000\"\n",
    "        else:\n",
    "            model = mobilenet_v1(input_shape, classes=classes, alpha=alpha, loop_depth=loop_depth)\n",
    "            variation_code =\"l\"+str(loop_depth)\n",
    "        base_model_name = model.name\n",
    "    elif architecture==\"mobilenet_v1_keras\":\n",
    "        model = mobilenet_v1_keras(input_shape, classes=classes, alpha=alpha)\n",
    "        variation_code =\"keras\"\n",
    "        base_model_name = model.name.split(\"_\")[0]+\"v1\"\n",
    "    elif architecture==\"mobilenet_v2\":\n",
    "        if loop_depth==None:\n",
    "            model = mobilenet_v2(input_shape, classes=classes, alpha=alpha)\n",
    "            variation_code =\"000\"\n",
    "        else:\n",
    "            model = mobilenet_v2(input_shape, classes=classes, alpha=alpha, loop_depth=loop_depth)\n",
    "            variation_code =\"l\"+str(loop_depth)\n",
    "        base_model_name = model.name\n",
    "\n",
    "    # my personal mobilenet version    \n",
    "    elif architecture==\"mobilenet_vme\":\n",
    "        if loop_depth==None:\n",
    "            model = mobilenet_vme(input_shape, classes=classes, alpha=alpha)\n",
    "            variation_code =\"000\"\n",
    "        else:\n",
    "            model = mobilenet_vme(input_shape, classes=classes, alpha=alpha, loop_depth=loop_depth)\n",
    "            variation_code =\"l\"+str(loop_depth)\n",
    "        base_model_name = model.name\n",
    "    elif architecture==\"mobilenet_v2_keras\":\n",
    "        model = mobilenet_v2_keras(input_shape, classes=classes, alpha=alpha)\n",
    "        variation_code =\"keras\"\n",
    "        base_model_name = model.name.split(\"_\")[0]#+\"v2\"\n",
    "    elif architecture== \"mobilenet_v3_small_keras\":\n",
    "        model = mobilenet_v3_small_keras(input_shape, classes=classes, alpha=alpha)\n",
    "        variation_code =\"keras\"\n",
    "        base_model_name = model.name.split(\"_\")[0]\n",
    "    elif architecture== \"mobilenet_v3_large_keras\":\n",
    "        model = mobilenet_v3_large_keras(input_shape, classes=classes, alpha=alpha)\n",
    "        variation_code =\"keras\"\n",
    "        base_model_name = model.name.split(\"_\")[0]\n",
    "    elif architecture==\"efficientNetB0_keras\":\n",
    "        model = efficientNetB0_keras(input_shape, classes=classes, alpha=alpha)\n",
    "        variation_code =\"keras\"\n",
    "        base_model_name = \"efficientNetB0\"\n",
    "    elif architecture==\"shufflenet_v1\":\n",
    "        model = shufflenet_v1(input_shape, classes=classes, alpha= alpha, groups=groups)\n",
    "        variation_code =\"g\"+str(groups)\n",
    "        base_model_name = model.name\n",
    "    elif architecture==\"shufflenet_v2\":\n",
    "        model = shufflenet_v2(input_shape, classes=classes, alpha= alpha)\n",
    "        variation_code =\"000\"\n",
    "        base_model_name = model.name\n",
    "\n",
    "    elif architecture==\"shufflenet_v2tiny\":\n",
    "        if (first_layer_channels==None) |(last_layer_channels==None) :\n",
    "            model = shufflenet_v2tiny(input_shape, classes, alpha, first_layer_channels=24, last_layer_channels=1024, use_bias=False) \n",
    "            variation_code =\"000\"\n",
    "        else:\n",
    "            model = shufflenet_v2tiny(input_shape, classes, alpha, first_layer_channels=first_layer_channels, last_layer_channels=last_layer_channels, use_bias=False) \n",
    "            variation_code = \"f\"+str(first_layer_channels)+\"l\"+str(last_layer_channels)\n",
    "        base_model_name = model.name\n",
    "    else:\n",
    "        #raise Exception e:\n",
    "        print(f\"Model architecture {architecture} is not supported.\")\n",
    "\n",
    "    return model, variation_code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mobilenetv2'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, variation_code = get_model(architecture, input_shape, classes, alpha, loop_depth, first_layer_channels=first_layer_channels, last_layer_channels=last_layer_channels)\n",
    "base_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.build(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = set_batchnorm_momentum(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n",
      "True\n",
      "0.9\n",
      "True\n",
      "0.9\n",
      "True\n",
      "0.9\n",
      "True\n",
      "0.9\n",
      "True\n",
      "0.9\n",
      "True\n",
      "0.9\n",
      "True\n",
      "0.9\n",
      "True\n",
      "0.9\n",
      "True\n",
      "0.9\n",
      "True\n",
      "0.9\n",
      "True\n",
      "0.9\n",
      "True\n",
      "0.9\n",
      "True\n",
      "0.9\n",
      "True\n",
      "0.9\n",
      "True\n",
      "0.9\n",
      "True\n",
      "0.9\n",
      "True\n",
      "0.9\n",
      "True\n",
      "0.9\n",
      "True\n",
      "0.9\n",
      "True\n",
      "0.9\n",
      "True\n",
      "0.9\n",
      "True\n",
      "0.9\n",
      "True\n",
      "0.9\n",
      "True\n",
      "0.9\n",
      "True\n",
      "0.9\n",
      "True\n",
      "0.9\n",
      "True\n",
      "0.9\n",
      "True\n",
      "0.9\n",
      "True\n",
      "0.9\n",
      "True\n",
      "0.9\n",
      "True\n",
      "0.9\n",
      "True\n",
      "0.9\n",
      "True\n",
      "0.9\n",
      "True\n",
      "0.9\n",
      "True\n",
      "0.9\n",
      "True\n",
      "0.9\n",
      "True\n",
      "0.9\n",
      "True\n",
      "0.9\n",
      "True\n",
      "0.9\n",
      "True\n",
      "0.9\n",
      "True\n",
      "0.9\n",
      "True\n",
      "0.9\n",
      "True\n",
      "0.9\n",
      "True\n",
      "0.9\n",
      "True\n",
      "0.9\n",
      "True\n",
      "0.9\n",
      "True\n",
      "0.9\n",
      "True\n",
      "0.9\n",
      "True\n",
      "0.9\n",
      "True\n",
      "0.9\n",
      "True\n",
      "0.9\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    if type(layer)==type(tf.keras.layers.BatchNormalization()):\n",
    "        print(layer.momentum)\n",
    "        print(layer.trainable)\n",
    "        #layer.momentum=0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'l5'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variation_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.applications.efficientnet.EfficientNetB0(\n",
    "#         include_top=True,\n",
    "#         weights= None, #'imagenet',\n",
    "#         input_tensor=None,\n",
    "#         input_shape=input_shape,\n",
    "#         pooling=None,\n",
    "#         classes=classes,\n",
    "#         classifier_activation='softmax',\n",
    "#         #**kwargs\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = efficientNetB0_keras(input_shape, classes=classes, alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.functional.Functional at 0x1de1f95cb80>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = mobilenet_v1(input_shape, classes=classes, alpha=alpha, global_average_pooling=False)\n",
    "# variation_code =\"noGAP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Adam',\n",
       " 'learning_rate': 0.001,\n",
       " 'decay': 0.0,\n",
       " 'beta_1': 0.9,\n",
       " 'beta_2': 0.999,\n",
       " 'epsilon': 1e-07,\n",
       " 'amsgrad': False}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mobilenetv2_1_224_c3_o3_l5\n"
     ]
    }
   ],
   "source": [
    "# Create the model name\n",
    "\n",
    "model_name = create_model_name(base_model_name, alpha, input_shape, classes, variation_code)\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_stats[\"model_name\"] = model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:\\tinyml\\tiny_cnn\\models\n"
     ]
    }
   ],
   "source": [
    "# Create the filepath structure\n",
    "(\n",
    "    models_path,\n",
    "    models_summary_path,\n",
    "    models_image_path,\n",
    "    models_layer_df_path,\n",
    "    models_tf_path,\n",
    "    models_tflite_path,\n",
    "    models_tflite_opt_path,\n",
    ") = create_filepaths(model_name)\n",
    "\n",
    "# mobilenet_v1 = keras.models.load_model(models_tf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving 'C:/Users/Susanne/AppData/Local/Temp/Susanne/mltk/tmp_models/model.h5' at http://localhost:8080\n",
      "Stopping http://localhost:8080\n"
     ]
    }
   ],
   "source": [
    "# Show model in local version of Netron.app\n",
    "view_model(model, tflite=True, build=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.utils.plot_model(\n",
    "#     model,\n",
    "#     to_file=models_image_path,\n",
    "#     show_shapes=True,\n",
    "#     show_dtype=False,\n",
    "#     show_layer_names=True,\n",
    "#     rankdir=\"TB\",  # TB for vertical plot, LR for horizontal plot\n",
    "#     expand_nested=True,\n",
    "#     layer_range=None,\n",
    "#     dpi=200,\n",
    "#     show_layer_activations=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mobilenetv2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 112, 112, 32  864         ['input_1[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv1_bn (BatchNormalization)  (None, 112, 112, 32  128         ['conv2d[0][0]']                 \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv1_relu (ReLU)              (None, 112, 112, 32  0           ['conv1_bn[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block_0_dw_conv (DepthwiseConv  (None, 112, 112, 32  288        ['conv1_relu[0][0]']             \n",
      " 2D)                            )                                                                 \n",
      "                                                                                                  \n",
      " block_0_dw_bn (BatchNormalizat  (None, 112, 112, 32  128        ['block_0_dw_conv[0][0]']        \n",
      " ion)                           )                                                                 \n",
      "                                                                                                  \n",
      " block_0_dw_relu (ReLU)         (None, 112, 112, 32  0           ['block_0_dw_bn[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block_0_compress (Conv2D)      (None, 112, 112, 16  512         ['block_0_dw_relu[0][0]']        \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block_0_compress_bn (BatchNorm  (None, 112, 112, 16  64         ['block_0_compress[0][0]']       \n",
      " alization)                     )                                                                 \n",
      "                                                                                                  \n",
      " block_1_expand (Conv2D)        (None, 112, 112, 96  1536        ['block_0_compress_bn[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block_1_expand_bn (BatchNormal  (None, 112, 112, 96  384        ['block_1_expand[0][0]']         \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " block_1_expand_relu (ReLU)     (None, 112, 112, 96  0           ['block_1_expand_bn[0][0]']      \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block_1_dw_conv (DepthwiseConv  (None, 56, 56, 96)  864         ['block_1_expand_relu[0][0]']    \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " block_1_dw_bn (BatchNormalizat  (None, 56, 56, 96)  384         ['block_1_dw_conv[0][0]']        \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " block_1_dw_relu (ReLU)         (None, 56, 56, 96)   0           ['block_1_dw_bn[0][0]']          \n",
      "                                                                                                  \n",
      " block_1_compress (Conv2D)      (None, 56, 56, 24)   2304        ['block_1_dw_relu[0][0]']        \n",
      "                                                                                                  \n",
      " block_1_compress_bn (BatchNorm  (None, 56, 56, 24)  96          ['block_1_compress[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_2_expand (Conv2D)        (None, 56, 56, 144)  3456        ['block_1_compress_bn[0][0]']    \n",
      "                                                                                                  \n",
      " block_2_expand_bn (BatchNormal  (None, 56, 56, 144)  576        ['block_2_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_2_expand_relu (ReLU)     (None, 56, 56, 144)  0           ['block_2_expand_bn[0][0]']      \n",
      "                                                                                                  \n",
      " block_2_dw_conv (DepthwiseConv  (None, 56, 56, 144)  1296       ['block_2_expand_relu[0][0]']    \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " block_2_dw_bn (BatchNormalizat  (None, 56, 56, 144)  576        ['block_2_dw_conv[0][0]']        \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " block_2_dw_relu (ReLU)         (None, 56, 56, 144)  0           ['block_2_dw_bn[0][0]']          \n",
      "                                                                                                  \n",
      " block_2_compress (Conv2D)      (None, 56, 56, 24)   3456        ['block_2_dw_relu[0][0]']        \n",
      "                                                                                                  \n",
      " block_2_compress_bn (BatchNorm  (None, 56, 56, 24)  96          ['block_2_compress[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 56, 56, 24)   0           ['block_1_compress_bn[0][0]',    \n",
      "                                                                  'block_2_compress_bn[0][0]']    \n",
      "                                                                                                  \n",
      " block_3_expand (Conv2D)        (None, 56, 56, 144)  3456        ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " block_3_expand_bn (BatchNormal  (None, 56, 56, 144)  576        ['block_3_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_3_expand_relu (ReLU)     (None, 56, 56, 144)  0           ['block_3_expand_bn[0][0]']      \n",
      "                                                                                                  \n",
      " block_3_dw_conv (DepthwiseConv  (None, 28, 28, 144)  1296       ['block_3_expand_relu[0][0]']    \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " block_3_dw_bn (BatchNormalizat  (None, 28, 28, 144)  576        ['block_3_dw_conv[0][0]']        \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " block_3_dw_relu (ReLU)         (None, 28, 28, 144)  0           ['block_3_dw_bn[0][0]']          \n",
      "                                                                                                  \n",
      " block_3_compress (Conv2D)      (None, 28, 28, 32)   4608        ['block_3_dw_relu[0][0]']        \n",
      "                                                                                                  \n",
      " block_3_compress_bn (BatchNorm  (None, 28, 28, 32)  128         ['block_3_compress[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_4_expand (Conv2D)        (None, 28, 28, 192)  6144        ['block_3_compress_bn[0][0]']    \n",
      "                                                                                                  \n",
      " block_4_expand_bn (BatchNormal  (None, 28, 28, 192)  768        ['block_4_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_4_expand_relu (ReLU)     (None, 28, 28, 192)  0           ['block_4_expand_bn[0][0]']      \n",
      "                                                                                                  \n",
      " block_4_dw_conv (DepthwiseConv  (None, 28, 28, 192)  1728       ['block_4_expand_relu[0][0]']    \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " block_4_dw_bn (BatchNormalizat  (None, 28, 28, 192)  768        ['block_4_dw_conv[0][0]']        \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " block_4_dw_relu (ReLU)         (None, 28, 28, 192)  0           ['block_4_dw_bn[0][0]']          \n",
      "                                                                                                  \n",
      " block_4_compress (Conv2D)      (None, 28, 28, 32)   6144        ['block_4_dw_relu[0][0]']        \n",
      "                                                                                                  \n",
      " block_4_compress_bn (BatchNorm  (None, 28, 28, 32)  128         ['block_4_compress[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 28, 28, 32)   0           ['block_3_compress_bn[0][0]',    \n",
      "                                                                  'block_4_compress_bn[0][0]']    \n",
      "                                                                                                  \n",
      " block_5_expand (Conv2D)        (None, 28, 28, 192)  6144        ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " block_5_expand_bn (BatchNormal  (None, 28, 28, 192)  768        ['block_5_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_5_expand_relu (ReLU)     (None, 28, 28, 192)  0           ['block_5_expand_bn[0][0]']      \n",
      "                                                                                                  \n",
      " block_5_dw_conv (DepthwiseConv  (None, 28, 28, 192)  1728       ['block_5_expand_relu[0][0]']    \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " block_5_dw_bn (BatchNormalizat  (None, 28, 28, 192)  768        ['block_5_dw_conv[0][0]']        \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " block_5_dw_relu (ReLU)         (None, 28, 28, 192)  0           ['block_5_dw_bn[0][0]']          \n",
      "                                                                                                  \n",
      " block_5_compress (Conv2D)      (None, 28, 28, 32)   6144        ['block_5_dw_relu[0][0]']        \n",
      "                                                                                                  \n",
      " block_5_compress_bn (BatchNorm  (None, 28, 28, 32)  128         ['block_5_compress[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 28, 28, 32)   0           ['add_1[0][0]',                  \n",
      "                                                                  'block_5_compress_bn[0][0]']    \n",
      "                                                                                                  \n",
      " block_6_expand (Conv2D)        (None, 28, 28, 192)  6144        ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " block_6_expand_bn (BatchNormal  (None, 28, 28, 192)  768        ['block_6_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_6_expand_relu (ReLU)     (None, 28, 28, 192)  0           ['block_6_expand_bn[0][0]']      \n",
      "                                                                                                  \n",
      " block_6_dw_conv (DepthwiseConv  (None, 14, 14, 192)  1728       ['block_6_expand_relu[0][0]']    \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " block_6_dw_bn (BatchNormalizat  (None, 14, 14, 192)  768        ['block_6_dw_conv[0][0]']        \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " block_6_dw_relu (ReLU)         (None, 14, 14, 192)  0           ['block_6_dw_bn[0][0]']          \n",
      "                                                                                                  \n",
      " block_6_compress (Conv2D)      (None, 14, 14, 64)   12288       ['block_6_dw_relu[0][0]']        \n",
      "                                                                                                  \n",
      " block_6_compress_bn (BatchNorm  (None, 14, 14, 64)  256         ['block_6_compress[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_7_expand (Conv2D)        (None, 14, 14, 384)  24576       ['block_6_compress_bn[0][0]']    \n",
      "                                                                                                  \n",
      " block_7_expand_bn (BatchNormal  (None, 14, 14, 384)  1536       ['block_7_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_7_expand_relu (ReLU)     (None, 14, 14, 384)  0           ['block_7_expand_bn[0][0]']      \n",
      "                                                                                                  \n",
      " block_7_dw_conv (DepthwiseConv  (None, 14, 14, 384)  3456       ['block_7_expand_relu[0][0]']    \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " block_7_dw_bn (BatchNormalizat  (None, 14, 14, 384)  1536       ['block_7_dw_conv[0][0]']        \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " block_7_dw_relu (ReLU)         (None, 14, 14, 384)  0           ['block_7_dw_bn[0][0]']          \n",
      "                                                                                                  \n",
      " block_7_compress (Conv2D)      (None, 14, 14, 64)   24576       ['block_7_dw_relu[0][0]']        \n",
      "                                                                                                  \n",
      " block_7_compress_bn (BatchNorm  (None, 14, 14, 64)  256         ['block_7_compress[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 14, 14, 64)   0           ['block_6_compress_bn[0][0]',    \n",
      "                                                                  'block_7_compress_bn[0][0]']    \n",
      "                                                                                                  \n",
      " block_8_expand (Conv2D)        (None, 14, 14, 384)  24576       ['add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " block_8_expand_bn (BatchNormal  (None, 14, 14, 384)  1536       ['block_8_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_8_expand_relu (ReLU)     (None, 14, 14, 384)  0           ['block_8_expand_bn[0][0]']      \n",
      "                                                                                                  \n",
      " block_8_dw_conv (DepthwiseConv  (None, 14, 14, 384)  3456       ['block_8_expand_relu[0][0]']    \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " block_8_dw_bn (BatchNormalizat  (None, 14, 14, 384)  1536       ['block_8_dw_conv[0][0]']        \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " block_8_dw_relu (ReLU)         (None, 14, 14, 384)  0           ['block_8_dw_bn[0][0]']          \n",
      "                                                                                                  \n",
      " block_8_compress (Conv2D)      (None, 14, 14, 64)   24576       ['block_8_dw_relu[0][0]']        \n",
      "                                                                                                  \n",
      " block_8_compress_bn (BatchNorm  (None, 14, 14, 64)  256         ['block_8_compress[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 14, 14, 64)   0           ['add_3[0][0]',                  \n",
      "                                                                  'block_8_compress_bn[0][0]']    \n",
      "                                                                                                  \n",
      " block_9_expand (Conv2D)        (None, 14, 14, 384)  24576       ['add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " block_9_expand_bn (BatchNormal  (None, 14, 14, 384)  1536       ['block_9_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_9_expand_relu (ReLU)     (None, 14, 14, 384)  0           ['block_9_expand_bn[0][0]']      \n",
      "                                                                                                  \n",
      " block_9_dw_conv (DepthwiseConv  (None, 14, 14, 384)  3456       ['block_9_expand_relu[0][0]']    \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " block_9_dw_bn (BatchNormalizat  (None, 14, 14, 384)  1536       ['block_9_dw_conv[0][0]']        \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " block_9_dw_relu (ReLU)         (None, 14, 14, 384)  0           ['block_9_dw_bn[0][0]']          \n",
      "                                                                                                  \n",
      " block_9_compress (Conv2D)      (None, 14, 14, 64)   24576       ['block_9_dw_relu[0][0]']        \n",
      "                                                                                                  \n",
      " block_9_compress_bn (BatchNorm  (None, 14, 14, 64)  256         ['block_9_compress[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 14, 14, 64)   0           ['add_4[0][0]',                  \n",
      "                                                                  'block_9_compress_bn[0][0]']    \n",
      "                                                                                                  \n",
      " block_10_expand (Conv2D)       (None, 14, 14, 384)  24576       ['add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " block_10_expand_bn (BatchNorma  (None, 14, 14, 384)  1536       ['block_10_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_10_expand_relu (ReLU)    (None, 14, 14, 384)  0           ['block_10_expand_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block_10_dw_conv (DepthwiseCon  (None, 14, 14, 384)  3456       ['block_10_expand_relu[0][0]']   \n",
      " v2D)                                                                                             \n",
      "                                                                                                  \n",
      " block_10_dw_bn (BatchNormaliza  (None, 14, 14, 384)  1536       ['block_10_dw_conv[0][0]']       \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block_10_dw_relu (ReLU)        (None, 14, 14, 384)  0           ['block_10_dw_bn[0][0]']         \n",
      "                                                                                                  \n",
      " block_10_compress (Conv2D)     (None, 14, 14, 96)   36864       ['block_10_dw_relu[0][0]']       \n",
      "                                                                                                  \n",
      " block_10_compress_bn (BatchNor  (None, 14, 14, 96)  384         ['block_10_compress[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_11_expand (Conv2D)       (None, 14, 14, 576)  55296       ['block_10_compress_bn[0][0]']   \n",
      "                                                                                                  \n",
      " block_11_expand_bn (BatchNorma  (None, 14, 14, 576)  2304       ['block_11_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_11_expand_relu (ReLU)    (None, 14, 14, 576)  0           ['block_11_expand_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block_11_dw_conv (DepthwiseCon  (None, 14, 14, 576)  5184       ['block_11_expand_relu[0][0]']   \n",
      " v2D)                                                                                             \n",
      "                                                                                                  \n",
      " block_11_dw_bn (BatchNormaliza  (None, 14, 14, 576)  2304       ['block_11_dw_conv[0][0]']       \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block_11_dw_relu (ReLU)        (None, 14, 14, 576)  0           ['block_11_dw_bn[0][0]']         \n",
      "                                                                                                  \n",
      " block_11_compress (Conv2D)     (None, 14, 14, 96)   55296       ['block_11_dw_relu[0][0]']       \n",
      "                                                                                                  \n",
      " block_11_compress_bn (BatchNor  (None, 14, 14, 96)  384         ['block_11_compress[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 14, 14, 96)   0           ['block_10_compress_bn[0][0]',   \n",
      "                                                                  'block_11_compress_bn[0][0]']   \n",
      "                                                                                                  \n",
      " block_12_expand (Conv2D)       (None, 14, 14, 576)  55296       ['add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " block_12_expand_bn (BatchNorma  (None, 14, 14, 576)  2304       ['block_12_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_12_expand_relu (ReLU)    (None, 14, 14, 576)  0           ['block_12_expand_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block_12_dw_conv (DepthwiseCon  (None, 14, 14, 576)  5184       ['block_12_expand_relu[0][0]']   \n",
      " v2D)                                                                                             \n",
      "                                                                                                  \n",
      " block_12_dw_bn (BatchNormaliza  (None, 14, 14, 576)  2304       ['block_12_dw_conv[0][0]']       \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block_12_dw_relu (ReLU)        (None, 14, 14, 576)  0           ['block_12_dw_bn[0][0]']         \n",
      "                                                                                                  \n",
      " block_12_compress (Conv2D)     (None, 14, 14, 96)   55296       ['block_12_dw_relu[0][0]']       \n",
      "                                                                                                  \n",
      " block_12_compress_bn (BatchNor  (None, 14, 14, 96)  384         ['block_12_compress[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 14, 14, 96)   0           ['add_6[0][0]',                  \n",
      "                                                                  'block_12_compress_bn[0][0]']   \n",
      "                                                                                                  \n",
      " block_13_expand (Conv2D)       (None, 14, 14, 576)  55296       ['add_7[0][0]']                  \n",
      "                                                                                                  \n",
      " block_13_expand_bn (BatchNorma  (None, 14, 14, 576)  2304       ['block_13_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_13_expand_relu (ReLU)    (None, 14, 14, 576)  0           ['block_13_expand_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block_13_dw_conv (DepthwiseCon  (None, 7, 7, 576)   5184        ['block_13_expand_relu[0][0]']   \n",
      " v2D)                                                                                             \n",
      "                                                                                                  \n",
      " block_13_dw_bn (BatchNormaliza  (None, 7, 7, 576)   2304        ['block_13_dw_conv[0][0]']       \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block_13_dw_relu (ReLU)        (None, 7, 7, 576)    0           ['block_13_dw_bn[0][0]']         \n",
      "                                                                                                  \n",
      " block_13_compress (Conv2D)     (None, 7, 7, 160)    92160       ['block_13_dw_relu[0][0]']       \n",
      "                                                                                                  \n",
      " block_13_compress_bn (BatchNor  (None, 7, 7, 160)   640         ['block_13_compress[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_14_expand (Conv2D)       (None, 7, 7, 960)    153600      ['block_13_compress_bn[0][0]']   \n",
      "                                                                                                  \n",
      " block_14_expand_bn (BatchNorma  (None, 7, 7, 960)   3840        ['block_14_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_14_expand_relu (ReLU)    (None, 7, 7, 960)    0           ['block_14_expand_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block_14_dw_conv (DepthwiseCon  (None, 7, 7, 960)   8640        ['block_14_expand_relu[0][0]']   \n",
      " v2D)                                                                                             \n",
      "                                                                                                  \n",
      " block_14_dw_bn (BatchNormaliza  (None, 7, 7, 960)   3840        ['block_14_dw_conv[0][0]']       \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block_14_dw_relu (ReLU)        (None, 7, 7, 960)    0           ['block_14_dw_bn[0][0]']         \n",
      "                                                                                                  \n",
      " block_14_compress (Conv2D)     (None, 7, 7, 160)    153600      ['block_14_dw_relu[0][0]']       \n",
      "                                                                                                  \n",
      " block_14_compress_bn (BatchNor  (None, 7, 7, 160)   640         ['block_14_compress[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 7, 7, 160)    0           ['block_13_compress_bn[0][0]',   \n",
      "                                                                  'block_14_compress_bn[0][0]']   \n",
      "                                                                                                  \n",
      " block_15_expand (Conv2D)       (None, 7, 7, 960)    153600      ['add_8[0][0]']                  \n",
      "                                                                                                  \n",
      " block_15_expand_bn (BatchNorma  (None, 7, 7, 960)   3840        ['block_15_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_15_expand_relu (ReLU)    (None, 7, 7, 960)    0           ['block_15_expand_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block_15_dw_conv (DepthwiseCon  (None, 7, 7, 960)   8640        ['block_15_expand_relu[0][0]']   \n",
      " v2D)                                                                                             \n",
      "                                                                                                  \n",
      " block_15_dw_bn (BatchNormaliza  (None, 7, 7, 960)   3840        ['block_15_dw_conv[0][0]']       \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block_15_dw_relu (ReLU)        (None, 7, 7, 960)    0           ['block_15_dw_bn[0][0]']         \n",
      "                                                                                                  \n",
      " block_15_compress (Conv2D)     (None, 7, 7, 160)    153600      ['block_15_dw_relu[0][0]']       \n",
      "                                                                                                  \n",
      " block_15_compress_bn (BatchNor  (None, 7, 7, 160)   640         ['block_15_compress[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 7, 7, 160)    0           ['add_8[0][0]',                  \n",
      "                                                                  'block_15_compress_bn[0][0]']   \n",
      "                                                                                                  \n",
      " block_16_expand (Conv2D)       (None, 7, 7, 960)    153600      ['add_9[0][0]']                  \n",
      "                                                                                                  \n",
      " block_16_expand_bn (BatchNorma  (None, 7, 7, 960)   3840        ['block_16_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_16_expand_relu (ReLU)    (None, 7, 7, 960)    0           ['block_16_expand_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block_16_dw_conv (DepthwiseCon  (None, 7, 7, 960)   8640        ['block_16_expand_relu[0][0]']   \n",
      " v2D)                                                                                             \n",
      "                                                                                                  \n",
      " block_16_dw_bn (BatchNormaliza  (None, 7, 7, 960)   3840        ['block_16_dw_conv[0][0]']       \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block_16_dw_relu (ReLU)        (None, 7, 7, 960)    0           ['block_16_dw_bn[0][0]']         \n",
      "                                                                                                  \n",
      " block_16_compress (Conv2D)     (None, 7, 7, 320)    307200      ['block_16_dw_relu[0][0]']       \n",
      "                                                                                                  \n",
      " block_16_compress_bn (BatchNor  (None, 7, 7, 320)   1280        ['block_16_compress[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " last_conv (Conv2D)             (None, 7, 7, 1280)   409600      ['block_16_compress_bn[0][0]']   \n",
      "                                                                                                  \n",
      " last_bn (BatchNormalization)   (None, 7, 7, 1280)   5120        ['last_conv[0][0]']              \n",
      "                                                                                                  \n",
      " last_relu (ReLU)               (None, 7, 7, 1280)   0           ['last_bn[0][0]']                \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 1, 1, 1280)  0           ['last_relu[0][0]']              \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 1, 1, 1280)   0           ['global_average_pooling2d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 1, 1, 3)      3843        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 3)            0           ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " softmax (Softmax)              (None, 3)            0           ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,261,827\n",
      "Trainable params: 2,227,715\n",
      "Non-trainable params: 34,112\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "Total MACs: 299.498 M\n",
      "Total OPs: 611.424 M\n"
     ]
    }
   ],
   "source": [
    "mltk_summary = summarize_model(model)\n",
    "print(mltk_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model summary to disk\n",
    "\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "with open(models_summary_path, \"w\", encoding='utf-8') as f:\n",
    "    with redirect_stdout(f):\n",
    "        print(mltk_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_mltk_model_summary(filepath): \n",
    "    # Parse the MLTK model summary to grab important metrics   \n",
    "    with open(filepath, \"r\") as f:\n",
    "        lines = f.readlines() # list containing lines of file\n",
    "        #columns = [] # To store column names\n",
    "\n",
    "        i = 1\n",
    "        for line in lines:\n",
    "            line = line.strip() # remove leading/trailing white spaces\n",
    "            if line.startswith(\"Total params:\"):\n",
    "                total_params = line.split()[-1]\n",
    "                total_params = int(total_params.replace(\",\", \"\"))\n",
    "            elif line.startswith(\"Trainable params:\"):\n",
    "                trainable_params = line.split()[-1]\n",
    "                trainable_params =  int(trainable_params.replace(\",\", \"\"))\n",
    "            elif line.startswith(\"Non-trainable params:\"):\n",
    "                non_trainable_params = line.split()[-1]\n",
    "                non_trainable_params = int(non_trainable_params.replace(\",\", \"\"))\n",
    "            elif line.startswith(\"Total MACs:\"):\n",
    "                MACs = line.split()[-2] + \" \" + line.split()[-1]\n",
    "                #MACs = (float(MACs))\n",
    "            elif line.startswith(\"Total OPs:\"):\n",
    "                FLOPs = line.split()[-2] + \" \" + line.split()[-1]\n",
    "                #FLOPs = (float(FLOPs))\n",
    "            else:\n",
    "                pass\n",
    "    \n",
    "    return (total_params, trainable_params, non_trainable_params, MACs, FLOPs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.build(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLTK profile model reads the mode from a path - only works for MLTK models! / Model must be trained first?\n",
    "# This only works with TFLite Models!\n",
    "\n",
    "#profiling_results = profile_model(model, accelerator='None', build=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_params, trainable_params, non_trainable_params, MACs, FLOPs = parse_mltk_model_summary(models_summary_path)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"MACs: {MACs} - FLOPs {FLOPs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_stats[\"MACs\"] = MACs\n",
    "# model_stats[\"FLOPs\"] = FLOPs\n",
    "# model_stats[\"total_params\"] = total_params\n",
    "# model_stats[\"trainable_params\"] = trainable_params\n",
    "# model_stats[\"non_trainable_params\"] = non_trainable_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('i:/tinyml/tiny_cnn/models/mobilenetv2_1_224_c3_o3_l5/mobilenetv2_1_224_c3_o3_l5.h5')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save(models_tf_path)\n",
    "models_tf_path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linux playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! wsl ls ./models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! wsl pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! wsl ls ./models -l >model.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! wsl ls ./models -laR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linux_path = models_tflite_path.as_posix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('i:/tinyml/tiny_cnn/models/mobilenetv2_1_224_c3_o3_l5/mobilenetv2_1_224_c3_o3_l5.h5')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_tf_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tiny_cnn_6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0faa6c31b20b8f809b81d6d7d22a84ccd9f354666f54133d1793fa4c65539801"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
