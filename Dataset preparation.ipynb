{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation of the Lemon Quality Dataset\n",
    "\n",
    "Download the dataset from [https://github.com/robotduinom/lemon_dataset](https://github.com/robotduinom/lemon_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "from urllib.request import urlopen\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "#import rarfile\n",
    "from unrar import rarfile\n",
    "import splitfolders\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_unzip(url, extract_to):\n",
    "    http_response = urlopen(url)\n",
    "    zipfile = ZipFile(BytesIO(http_response.read()))\n",
    "    zipfile.extractall(path=extract_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://github.com/robotduinom/lemon_dataset/archive/refs/heads/main.zip\"\n",
    "extract_folder = Path.cwd().joinpath(\"datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_and_unzip(url,extract_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_folder = extract_folder.joinpath(\"lemon_dataset-main\", \"docs\", \"data\")\n",
    "input_folder.exists()\n",
    "#input_folder.is_absolute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rar_path = extract_folder.joinpath(\"lemon_dataset-main\", \"docs\", \"data.rar\")\n",
    "rar_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "rar_extract_path = extract_folder.joinpath(\"lemon_dataset-main\", \"docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rar = rarfile.RarFile(str(rar_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "rar.extractall(str(rar_extract_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the dataset into training, validation and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('i:/tinyml/tiny_cnn/datasets/lemon_dataset')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_split_seed = 42\n",
    "output_folder = Path.cwd().joinpath(\"datasets\", \"lemon_dataset\")\n",
    "output_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 2528 files [00:12, 197.94 files/s]\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data into 80% training data, 10% validation data and 10% test data\n",
    "splitfolders.ratio(input_folder, output=output_folder,\n",
    "    seed=data_split_seed, ratio=(.8, .1, .1), group_prefix=None, move=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Tensorflow Datagenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_height = 92\n",
    "img_width = 92\n",
    "shuffle_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2021 files belonging to 3 classes.\n",
      "Found 252 files belonging to 3 classes.\n",
      "Found 255 files belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dir = Path.cwd().joinpath(\"datasets\", \"lemon_dataset\", \"train\")\n",
    "val_dir = Path.cwd().joinpath(\"datasets\", \"lemon_dataset\", \"val\")\n",
    "test_dir = Path.cwd().joinpath(\"datasets\", \"lemon_dataset\", \"test\")\n",
    "\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    subset=None,\n",
    "    seed=shuffle_seed,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size)\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    val_dir,\n",
    "    subset=None,\n",
    "    seed=shuffle_seed,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size)\n",
    "\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    test_dir,\n",
    "    subset=None,\n",
    "    seed=shuffle_seed,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemon_quality_dataset(dataset_path, normalize=True):\n",
    "    \"\"\" Fetches the lemon quality dataset and prints dataset info. It normalizes the image data to range [0,1] by default.\n",
    "\n",
    "    Args: \n",
    "        dataset_path (Path): the file location of the dataset. Subfolders \"train\", \"test\", and \"val\" are expected.\n",
    "        normalize (boolean): Normalizes the image data to range [0, 1]. Default: True\n",
    "\n",
    "    Returns:\n",
    "        (train_ds, val_ds, test_ds, class_names) (tuple(tf.datasets)): Tensorflow datasets for train, validation and test.\n",
    "    \n",
    "    \"\"\"\n",
    "    if dataset_path.exists():\n",
    "        try:\n",
    "            train_dir = dataset_path.joinpath(\"train\")\n",
    "            val_dir = dataset_path.joinpath( \"val\")\n",
    "            test_dir = dataset_path.joinpath( \"test\")\n",
    "        except:\n",
    "            print(f\"Please check the folder structure of {dataset_path}.\")\n",
    "            raise\n",
    "\n",
    "    print(\"Preparing training dataset...\")        \n",
    "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        train_dir,\n",
    "        subset=None,\n",
    "        seed=shuffle_seed,\n",
    "        image_size=(img_height, img_width),\n",
    "        batch_size=batch_size)\n",
    "\n",
    "    class_names = train_ds.class_names\n",
    "\n",
    "\n",
    "    print(\"Preparing validation dataset...\")    \n",
    "    val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        val_dir,\n",
    "        subset=None,\n",
    "        seed=shuffle_seed,\n",
    "        image_size=(img_height, img_width),\n",
    "        batch_size=batch_size)\n",
    "\n",
    "    print(\"Preparing test dataset...\")    \n",
    "    test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        test_dir,\n",
    "        subset=None,\n",
    "        seed=shuffle_seed,\n",
    "        image_size=(img_height, img_width),\n",
    "        batch_size=batch_size)\n",
    "    \n",
    "    # Normalize the data to the range [0, 1]\n",
    "    if normalize:\n",
    "        normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "\n",
    "        train_ds= train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "        val_ds= val_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "        test_ds= test_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    print (f\"Class names: {class_names}\")\n",
    "    print(train_ds.element_spec)\n",
    "    print(f\"Normalize: {normalize}\")\n",
    "    return (train_ds, val_ds, test_ds, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing training dataset...\n",
      "Found 2021 files belonging to 3 classes.\n",
      "Preparing validation dataset...\n",
      "Found 252 files belonging to 3 classes.\n",
      "Preparing test dataset...\n",
      "Found 255 files belonging to 3 classes.\n",
      "Class names: ['bad_quality', 'empty_background', 'good_quality']\n",
      "(TensorSpec(shape=(None, 92, 92, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))\n",
      "Normalize: True\n"
     ]
    }
   ],
   "source": [
    "train_ds, val_ds, test_ds, class_names = get_lemon_quality_dataset(output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.dataset_ops.BatchDataset"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'builtin_function_or_method' object has no attribute 'iter'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [67], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m np\u001b[39m.\u001b[39mmin(\u001b[39mnext\u001b[39;49m\u001b[39m.\u001b[39;49miter(train_ds))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'builtin_function_or_method' object has no attribute 'iter'"
     ]
    }
   ],
   "source": [
    "np.min(next.iter(train_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__DatasetToSingleElement_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Dataset had more than one element. [Op:DatasetToSingleElement]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [68], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_ds\u001b[39m.\u001b[39;49mget_single_element()\n",
      "File \u001b[1;32md:\\Miniconda\\envs\\tiny_cnn\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:2821\u001b[0m, in \u001b[0;36mDatasetV2.get_single_element\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   2817\u001b[0m \u001b[39mif\u001b[39;00m name:\n\u001b[0;32m   2818\u001b[0m   metadata\u001b[39m.\u001b[39mname \u001b[39m=\u001b[39m _validate_and_encode(name)\n\u001b[0;32m   2819\u001b[0m \u001b[39mreturn\u001b[39;00m structure\u001b[39m.\u001b[39mfrom_compatible_tensor_list(\n\u001b[0;32m   2820\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39melement_spec,\n\u001b[1;32m-> 2821\u001b[0m     gen_dataset_ops\u001b[39m.\u001b[39mdataset_to_single_element(\n\u001b[0;32m   2822\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variant_tensor,\n\u001b[0;32m   2823\u001b[0m         metadata\u001b[39m=\u001b[39mmetadata\u001b[39m.\u001b[39mSerializeToString(),\n\u001b[0;32m   2824\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_structure))\n",
      "File \u001b[1;32md:\\Miniconda\\envs\\tiny_cnn\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:1411\u001b[0m, in \u001b[0;36mdataset_to_single_element\u001b[1;34m(dataset, output_types, output_shapes, metadata, name)\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m   1410\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m-> 1411\u001b[0m   _ops\u001b[39m.\u001b[39;49mraise_from_not_ok_status(e, name)\n\u001b[0;32m   1412\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_FallbackException:\n\u001b[0;32m   1413\u001b[0m   \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[1;32md:\\Miniconda\\envs\\tiny_cnn\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:7209\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   7207\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[0;32m   7208\u001b[0m   e\u001b[39m.\u001b[39mmessage \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m name: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 7209\u001b[0m   \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__DatasetToSingleElement_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Dataset had more than one element. [Op:DatasetToSingleElement]"
     ]
    }
   ],
   "source": [
    "train_ds.get_single_element()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer = tf.keras.layers.Rescaling(1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_normalized = train_ds.map(lambda x, y: (normalization_layer(x), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('tiny_cnn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5618f2993617940a5c26d2b4a732b1ce578923eb12d8dcc67dd41bd2b07dc9c1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
