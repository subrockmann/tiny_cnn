{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation of the Lemon Quality Dataset\n",
    "\n",
    "Download the dataset from [https://github.com/robotduinom/lemon_dataset](https://github.com/robotduinom/lemon_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "from urllib.request import urlopen\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "#import rarfile\n",
    "from unrar import rarfile\n",
    "import splitfolders\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_unzip(url, extract_to):\n",
    "    http_response = urlopen(url)\n",
    "    zipfile = ZipFile(BytesIO(http_response.read()))\n",
    "    zipfile.extractall(path=extract_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://github.com/robotduinom/lemon_dataset/archive/refs/heads/main.zip\"\n",
    "extract_folder = Path.cwd().joinpath(\"datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_and_unzip(url,extract_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_folder = extract_folder.joinpath(\"lemon_dataset-main\", \"docs\", \"data\")\n",
    "input_folder.exists()\n",
    "#input_folder.is_absolute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rar_path = extract_folder.joinpath(\"lemon_dataset-main\", \"docs\", \"data.rar\")\n",
    "rar_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "rar_extract_path = extract_folder.joinpath(\"lemon_dataset-main\", \"docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rar = rarfile.RarFile(str(rar_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "rar.extractall(str(rar_extract_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the dataset into training, validation and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('i:/tinyml/tiny_cnn/datasets/lemon_dataset_binary')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_split_seed = 42\n",
    "#output_folder = Path.cwd().joinpath(\"datasets\", \"lemon_dataset\")\n",
    "output_folder = Path.cwd().joinpath(\"datasets\", \"lemon_dataset_binary\")\n",
    "output_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 2076 files [00:29, 71.53 files/s]\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data into 80% training data, 10% validation data and 10% test data\n",
    "splitfolders.ratio(input_folder, output=output_folder,\n",
    "    seed=data_split_seed, ratio=(.8, .1, .1), group_prefix=None, move=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Tensorflow Datagenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_height = 92\n",
    "img_width = 92\n",
    "shuffle_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2021 files belonging to 3 classes.\n",
      "Found 252 files belonging to 3 classes.\n",
      "Found 255 files belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dir = Path.cwd().joinpath(\"datasets\", \"lemon_dataset\", \"train\")\n",
    "val_dir = Path.cwd().joinpath(\"datasets\", \"lemon_dataset\", \"val\")\n",
    "test_dir = Path.cwd().joinpath(\"datasets\", \"lemon_dataset\", \"test\")\n",
    "\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    subset=None,\n",
    "    seed=shuffle_seed,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size)\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    val_dir,\n",
    "    subset=None,\n",
    "    seed=shuffle_seed,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size)\n",
    "\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    test_dir,\n",
    "    subset=None,\n",
    "    seed=shuffle_seed,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_height = 92\n",
    "img_width = 92\n",
    "shuffle_seed = 42\n",
    "\n",
    "def get_lemon_quality_dataset(dataset_path, normalize=True):\n",
    "    \"\"\" Fetches the lemon quality dataset and prints dataset info. It normalizes the image data to range [0,1] by default.\n",
    "\n",
    "    Args: \n",
    "        dataset_path (Path): the file location of the dataset. Subfolders \"train\", \"test\", and \"val\" are expected.\n",
    "        normalize (boolean): Normalizes the image data to range [0, 1]. Default: True\n",
    "\n",
    "    Returns:\n",
    "        (train_ds, val_ds, test_ds, class_names) (tuple(tf.datasets)): Tensorflow datasets for train, validation and test.\n",
    "    \n",
    "    \"\"\"\n",
    "    if dataset_path.exists():\n",
    "        try:\n",
    "            train_dir = dataset_path.joinpath(\"train\")\n",
    "            val_dir = dataset_path.joinpath( \"val\")\n",
    "            test_dir = dataset_path.joinpath( \"test\")\n",
    "        except:\n",
    "            print(f\"Please check the folder structure of {dataset_path}.\")\n",
    "            raise\n",
    "\n",
    "    print(\"Preparing training dataset...\")        \n",
    "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        train_dir,\n",
    "        subset=None,\n",
    "        seed=shuffle_seed,\n",
    "        image_size=(img_height, img_width),\n",
    "        batch_size=batch_size)\n",
    "\n",
    "    class_names = train_ds.class_names\n",
    "\n",
    "\n",
    "    print(\"Preparing validation dataset...\")    \n",
    "    val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        val_dir,\n",
    "        subset=None,\n",
    "        seed=shuffle_seed,\n",
    "        image_size=(img_height, img_width),\n",
    "        batch_size=batch_size)\n",
    "\n",
    "    print(\"Preparing test dataset...\")    \n",
    "    test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        test_dir,\n",
    "        subset=None,\n",
    "        seed=shuffle_seed,\n",
    "        image_size=(img_height, img_width),\n",
    "        batch_size=batch_size)\n",
    "    \n",
    "    # Normalize the data to the range [0, 1]\n",
    "    if normalize:\n",
    "        normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "\n",
    "        train_ds= train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "        val_ds= val_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "        test_ds= test_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    print (f\"Class names: {class_names}\")\n",
    "    print(train_ds.element_spec)\n",
    "    print(f\"Normalize: {normalize}\")\n",
    "    return (train_ds, val_ds, test_ds, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing training dataset...\n",
      "Found 2021 files belonging to 3 classes.\n",
      "Preparing validation dataset...\n",
      "Found 252 files belonging to 3 classes.\n",
      "Preparing test dataset...\n",
      "Found 255 files belonging to 3 classes.\n",
      "Class names: ['bad_quality', 'empty_background', 'good_quality']\n",
      "(TensorSpec(shape=(None, 92, 92, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))\n",
      "Normalize: True\n"
     ]
    }
   ],
   "source": [
    "train_ds, val_ds, test_ds, class_names = get_lemon_quality_dataset(output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer = tf.keras.layers.Rescaling(1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_normalized = train_ds.map(lambda x, y: (normalization_layer(x), y))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split visual wake words dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.silabs.com/public/files/github/machine_learning/benchmarks/datasets/vw_coco2014_96.tar.gz\"\n",
    "extract_folder = Path.cwd().joinpath(\"datasets\", \"vvw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download_and_unzip(url,extract_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_folder_vww= Path.cwd().joinpath(\"datasets\", \"vw_coco2014_96\")\n",
    "input_folder_vww.is_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_folder_vww= Path.cwd().joinpath(\"datasets\", \"visual_wake_words\")\n",
    "output_folder_vww.is_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_folder_lemon_binary = Path.cwd().joinpath(\"datasets\", \"lemon_dataset_binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 109619 files [02:07, 858.08 files/s] \n"
     ]
    }
   ],
   "source": [
    "# Splitting the data into 80% training data, 10% validation data and 10% test data\n",
    "splitfolders.ratio(input_folder_vww, output=output_folder_vww,\n",
    "    seed=data_split_seed, ratio=(.8, .1, .1), group_prefix=None, move=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tiny_cnn_6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0faa6c31b20b8f809b81d6d7d22a84ccd9f354666f54133d1793fa4c65539801"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
