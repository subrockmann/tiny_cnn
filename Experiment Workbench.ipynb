{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Workbench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Miniconda\\envs\\tiny_cnn_5\\lib\\site-packages\\deeplake\\util\\check_latest_version.py:32: UserWarning: A newer version of deeplake (3.2.2) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os, sys, math, datetime\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import PIL\n",
    "import PIL.Image\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "#from tensorflow import keras\n",
    "keras = tf.keras\n",
    "from keras.layers import ReLU\n",
    "from keras.layers import Input, Dense, Flatten, Conv2D,DepthwiseConv2D, MaxPooling2D, AvgPool2D, GlobalAveragePooling2D, BatchNormalization, Concatenate\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler\n",
    " # Import the necessary MLTK APIs\n",
    "from mltk.core import view_model,  profile_model # summarize_model\n",
    "\n",
    "# import workbench.config.config\n",
    "from workbench.config.config import initialize\n",
    "from workbench.utils.utils import create_filepaths\n",
    "from workbench.utils.utils import parse_model_name\n",
    "from workbench.data.data import get_vvw_dataset\n",
    "\n",
    "\n",
    "#from dotenv import load_dotenv\n",
    "import wandb\n",
    "from wandb.keras import WandbMetricsLogger, WandbModelCheckpoint\n",
    "import deeplake\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 2.10.0\n",
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "print(\"Tensorflow version \" + tf.__version__)\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# Confirm that TensorFlow can access GPU\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if not device_name:\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "global model_name\n",
    "#model_name = \"efficientNetB0_1_96_c3_o3_keras\"\n",
    "#model_name = \"shufflenetv2tinys_0.2_96_c3_o3_f4l1024\"2\n",
    "model_name = \"mobilenetv2_0.35_96_c3_o2_l5\"\n",
    "#model_name = \"mobilenetv2_0.35_96_c3_o2_keras\"\n",
    "#model_name = \"MobilenetV3small_1_96_c3_o3_keras\"\n",
    "#model_name = \"mobilenetvme_0.35_96_c3_o3_l5\"\n",
    "#model_name = \"shufflenetv2tiny_0.2_96_c3_o3_f4l1024\"\n",
    "#model_name = \"MobilenetV3small_1_96_c3_o3_keras\"#, \"MobilenetV3large_1_224_c3_o3_keras\"# ,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# DANGER ZONE: Disable warning messages\n",
    "\n",
    "import absl.logging\n",
    "absl.logging.set_verbosity(absl.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/api_docs/python/tf/random/set_seed  \n",
    "\n",
    "Operations that rely on a random seed actually derive it from two seeds: the global and operation-level seeds. This sets the global seed.\n",
    "\n",
    "Its interactions with operation-level seeds is as follows:\n",
    "\n",
    "1. If neither the global seed nor the operation seed is set: A randomly picked seed is used for this op.  \n",
    "2. If the global seed is set, but the operation seed is not: The system deterministically picks an operation seed in conjunction with the global seed so that it gets a unique random sequence. Within the same version of tensorflow and user code, this sequence is deterministic. However across different versions, this sequence might change. If the code depends on particular seeds to work, specify both global and operation-level seeds explicitly.  \n",
    "3. If the operation seed is set, but the global seed is not set: A default global seed and the specified operation seed are used to determine the random sequence.  \n",
    "4. If both the global and the operation seed are set: Both seeds are used in conjunction to determine the random sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_1 = 1\n",
    "seed_2 = 15\n",
    "seed_3 = 30\n",
    "seed_4 = 42\n",
    "seed_5 = 75\n",
    "\n",
    "seed = seed_2\n",
    "\n",
    "# set the random seeds\n",
    "#os.environ[\"TF_CUDNN_DETERMINISTIC\"]= \"1\"\n",
    "# random.seed(seed)\n",
    "# np.random.seed(seed)\n",
    "# tf.random.set_seed(seed) # setting tensorflow global seed\n",
    "tf.keras.utils.set_random_seed(seed)\n",
    "# tf.config.experimental.enable_op_determinism() \n",
    "\n",
    "# Node: 'gradient_tape/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/UnsortedSegmentSum'\n",
    "#Deterministic GPU implementation of unsorted segment reduction op not available.\n",
    "#\t [[{{node gradient_tape/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/UnsortedSegmentSum}}]] [Op:__inference_train_function_10280]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = initialize()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting datasets from deeplake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = deeplake.load('hub://activeloop/plantvillage-without-augmentation')#   61486 images, 39 different plant diseases\n",
    "# len_plants  = 61486\n",
    "# plant_classes = 39\n",
    "#ds = deeplake.load(\"hub://activeloop/places205\") # this dataset is huge! 2.5Mio Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_tensorflow = ds.tensorflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #deeplake.__version__\n",
    "# image_count = len_plants\n",
    "# train_size = int(0.8 * image_count)\n",
    "# val_size = int(0.1 * image_count)\n",
    "# test_size = int(0.1 * image_count)\n",
    "\n",
    "# ds_tensorflow = ds_tensorflow.shuffle(image_count)\n",
    "# test_ds = ds_tensorflow.take(test_size)\n",
    "# train_ds = ds_tensorflow.skip(test_size)\n",
    "# val_ds = ds_tensorflow.take(val_size)\n",
    "# train_ds = ds_tensorflow.skip(val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds.cardinality()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sample in train_ds.as_numpy_iterator():\n",
    "#     print(sample)\n",
    "#     break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_iter = train_ds.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# element = next(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for element in train_ds:\n",
    "#     print(element)\n",
    "#     break\n",
    "# element[\"images\"].numpy() # image numpy array\n",
    "# element[\"labels\"].numpy() # labels numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 0\n",
    "# for elem in ds_tensorflow: \n",
    "#   i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds.options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds_iter = iter(train_ds)\n",
    "# next(train_ds_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next(train_ds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Wake Words dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vvw_path = Path.cwd().parent.joinpath(\"person_detection\",\"datavisualwakewords\")\n",
    "vvw_path.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:\\tinyml\\tiny_cnn\\models\n"
     ]
    }
   ],
   "source": [
    "models_path, models_summary_path, models_image_path, models_layer_df_path, models_tf_path, models_tflite_path, models_tflite_opt_path = create_filepaths(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(models_tf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "global base_model_name\n",
    "global alpha\n",
    "global resolution\n",
    "global channels\n",
    "global classes\n",
    "global variation\n",
    "global early_stopping_patience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name, alpha, resolution, channels, classes, variation = model_name.split(\"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = float(alpha)\n",
    "resolution = int(resolution)\n",
    "classes = int(classes.strip(\"o\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a Tensorboard session\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#os.environ['WANDB_NOTEBOOK_NAME'] = 'Experiment Workbench'\n",
    "\n",
    "IMG_HEIGHT = resolution\n",
    "IMG_WIDTH = resolution\n",
    "BATCH_SIZE = 96\n",
    "EPOCHS = 30\n",
    "#LOGGING_STEPS = 64\n",
    "MOMENTUM = 0.9\n",
    "LR = 0.045\n",
    "DROPOUT = 0.2\n",
    "early_stopping_patience = 30\n",
    "\n",
    "PROJECT = base_model_name\n",
    "ENTITY = \"susbrock\"\n",
    "#PROJECT = \"tiny_cnn troubleshooting\"\n",
    "\n",
    "shuffle_seed = seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the Lemon Quality Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = Path.cwd().joinpath(\"datasets\", \"lemon_dataset\")\n",
    "dataset_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('i:/tinyml/tiny_cnn/datasets/lemon_dataset')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemon_quality_dataset(dataset_path, img_width, img_height, batch_size, channels, normalize=True):\n",
    "    \"\"\" Fetches the lemon quality dataset and prints dataset info. It normalizes the image data to range [0,1] by default.\n",
    "\n",
    "    Args: \n",
    "        dataset_path (Path): the file location of the dataset. Subfolders \"train\", \"test\", and \"val\" are expected.\n",
    "        normalize (boolean): Normalizes the image data to range [0, 1]. Default: True\n",
    "\n",
    "    Returns:\n",
    "        (train_ds, val_ds, test_ds, class_names) (tuple(tf.datasets)): Tensorflow datasets for train, validation and test.\n",
    "    \n",
    "    \"\"\"\n",
    "    if dataset_path.exists():\n",
    "        try:\n",
    "            train_dir = dataset_path.joinpath(\"train\")\n",
    "            val_dir = dataset_path.joinpath( \"val\")\n",
    "            test_dir = dataset_path.joinpath( \"test\")\n",
    "        except:\n",
    "            print(f\"Please check the folder structure of {dataset_path}.\")\n",
    "            raise\n",
    "\n",
    "    channels = int(channels.strip(\"c\"))\n",
    "    if channels==1:\n",
    "        color_mode = \"grayscale\"\n",
    "    else:\n",
    "        color_mode = \"rgb\" \n",
    "    print(f\"Color mode: {color_mode}\")\n",
    "\n",
    "    # create the labels list to avoid inclusion of .ipynb checkpoints\n",
    "    #labels = [\"bad_quality\", \"empty_background\", \"good_quality\"]\n",
    "\n",
    "    print(\"Preparing training dataset...\")        \n",
    "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        train_dir,\n",
    "        subset=None,\n",
    "        seed=shuffle_seed,\n",
    "        image_size=((img_height, img_width)),\n",
    "        #labels=labels,\n",
    "        batch_size=batch_size,\n",
    "        color_mode=color_mode,\n",
    "        shuffle=True\n",
    "        )\n",
    "    \n",
    "\n",
    "    class_names = train_ds.class_names\n",
    "\n",
    "\n",
    "    print(\"Preparing validation dataset...\")    \n",
    "    val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        val_dir,\n",
    "        subset=None,\n",
    "        seed=shuffle_seed,\n",
    "        image_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        color_mode=color_mode,\n",
    "        shuffle=True\n",
    "        )\n",
    "    \n",
    "\n",
    "    print(\"Preparing test dataset...\")    \n",
    "    test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        test_dir,\n",
    "        subset=None,\n",
    "        seed=shuffle_seed,\n",
    "        image_size=(img_height, img_width),\n",
    "        batch_size=1,\n",
    "        color_mode=color_mode,\n",
    "        shuffle=False\n",
    "        )\n",
    "    \n",
    "    # Create a data augmentation stage with horizontal flipping, rotations, zooms\n",
    "    data_augmentation = keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "            tf.keras.layers.RandomRotation(0.1),\n",
    "            tf.keras.layers.RandomZoom(0.1),\n",
    "        ]\n",
    "        )\n",
    "\n",
    "    #train_ds= train_ds.map(lambda x, y: (data_augmentation(x), y), num_parallel_calls=tf.data.AUTOTUNE )\n",
    "\n",
    "    \n",
    "    # Normalize the data to the range [0, 1]\n",
    "    if normalize:\n",
    "        normalization_layer = tf.keras.layers.Rescaling(1./255, offset=-1)\n",
    "\n",
    "        train_ds= train_ds.map(lambda x, y: (normalization_layer(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        val_ds= val_ds.map(lambda x, y: (normalization_layer(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        test_ds= test_ds.map(lambda x, y: (normalization_layer(x), y)) #, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    print (f\"Class names: {class_names}\")\n",
    "    print(f\"Train: {train_ds.element_spec}\")\n",
    "    print(f\"Normalize: {normalize}\")\n",
    "    return (train_ds, val_ds, test_ds, class_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Wake Words minval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path.cwd().parent.joinpath(\"tiny_mlperf\", \"vw_coco2014_96\")\n",
    "BASE_DIR_TEST = Path.cwd().parent.joinpath(\"tiny_mlperf\", \"vw_coco2014_96_test\")\n",
    "Path.exists(BASE_DIR)\n",
    "validation_split = 0.1\n",
    "color_mode = \"rgb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing training dataset...\n",
      "Found 99619 files belonging to 2 classes.\n",
      "Using 89658 files for training.\n",
      "Preparing validation dataset...\n",
      "Found 99619 files belonging to 2 classes.\n",
      "Using 9961 files for validation.\n",
      "Preparing test dataset...\n",
      "Found 10000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "print(\"Preparing training dataset...\")        \n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    BASE_DIR,\n",
    "    validation_split=validation_split,\n",
    "    subset=\"training\",\n",
    "    seed=shuffle_seed,\n",
    "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    #labels=labels,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    color_mode=color_mode,\n",
    "    shuffle=True\n",
    "    )\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "\n",
    "print(\"Preparing validation dataset...\")        \n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    BASE_DIR,\n",
    "    validation_split=validation_split,\n",
    "    subset=\"validation\",\n",
    "    seed=shuffle_seed,\n",
    "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    #labels=labels,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    color_mode=color_mode,\n",
    "    shuffle=True\n",
    "    )\n",
    "\n",
    "print(\"Preparing test dataset...\")        \n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    BASE_DIR_TEST,\n",
    "    validation_split=None,\n",
    "    #subset=\"validation\",\n",
    "    seed=shuffle_seed,\n",
    "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    #labels=labels,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    color_mode=color_mode,\n",
    "    shuffle=False\n",
    "    )\n",
    "\n",
    "\n",
    "    #   Preprocessing form Visual Wake Word Challenge:\n",
    "    #   rotation_range=10,\n",
    "    #   width_shift_range=0.05,\n",
    "    #   height_shift_range=0.05,\n",
    "    #   zoom_range=.1,\n",
    "    #   horizontal_flip=True,\n",
    "# Create a data augmentation stage with horizontal flipping, rotations, zooms\n",
    "data_augmentation = keras.Sequential([\n",
    "        tf.keras.layers.RandomRotation(0.1),\n",
    "        tf.keras.layers.RandomTranslation(\n",
    "            height_factor = 0.05,\n",
    "            width_factor = 0.05\n",
    "        ),\n",
    "        tf.keras.layers.RandomZoom(0.1),\n",
    "        tf.keras.layers.RandomFlip(\"horizontal\")\n",
    "    ]\n",
    "    )\n",
    "\n",
    "train_ds= train_ds.map(lambda x, y: (data_augmentation(x), y), num_parallel_calls=tf.data.AUTOTUNE )\n",
    "\n",
    "\n",
    "# Normalize the data to the range [0, 1]\n",
    "\n",
    "normalization_layer = tf.keras.layers.Rescaling(1./255, offset=-1)\n",
    "\n",
    "train_ds= train_ds.map(lambda x, y: (normalization_layer(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_ds= val_ds.map(lambda x, y: (normalization_layer(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_ds= test_ds.map(lambda x, y: (normalization_layer(x), y)) #, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "labels = class_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 98658 images belonging to 2 classes.\n",
      "Found 10961 images belonging to 2 classes.\n",
      "{'non_person': 0, 'person': 1}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "#       rotation_range=10,\n",
    "#       width_shift_range=0.05,\n",
    "#       height_shift_range=0.05,\n",
    "#       zoom_range=.1,\n",
    "#       horizontal_flip=True,\n",
    "#       validation_split=validation_split,\n",
    "#       rescale=1. / 255)\n",
    "# train_generator = datagen.flow_from_directory(\n",
    "#     BASE_DIR,\n",
    "#     target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     subset='training',\n",
    "#     color_mode='rgb',\n",
    "#     class_mode=\"sparse\")\n",
    "# val_generator = datagen.flow_from_directory(\n",
    "#     BASE_DIR,\n",
    "#     target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     subset='validation',\n",
    "#     color_mode='rgb',\n",
    "#     class_mode=\"sparse\")\n",
    "\n",
    "\n",
    "# print(train_generator.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epochs(model, train_generator, val_generator, epoch_count,\n",
    "                 learning_rate):\n",
    "  model.compile(\n",
    "      optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "      #loss='categorical_crossentropy',\n",
    "      loss='sparse_categorical_crossentropy',\n",
    "      metrics=['accuracy'])\n",
    "  history_fine = model.fit(\n",
    "      train_generator,\n",
    "      steps_per_epoch=len(train_generator),\n",
    "      epochs=epoch_count,\n",
    "      validation_data=val_generator,\n",
    "      validation_steps=len(val_generator),\n",
    "      batch_size=BATCH_SIZE)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      " 279/1028 [=======>......................] - ETA: 1:14:06 - loss: 0.7016 - accuracy: 0.5549"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "Graph execution error:\n\n2 root error(s) found.\n  (0) UNKNOWN:  FileNotFoundError: [Errno 2] No such file or directory: 'i:\\\\tinyml\\\\tiny_mlperf\\\\vw_coco2014_96\\\\non_person\\\\COCO_val2014_000000565543.jpg'\nTraceback (most recent call last):\n\n  File \"d:\\Miniconda\\envs\\tiny_cnn_5\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 271, in __call__\n    ret = func(*args)\n\n  File \"d:\\Miniconda\\envs\\tiny_cnn_5\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"d:\\Miniconda\\envs\\tiny_cnn_5\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 1035, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"d:\\Miniconda\\envs\\tiny_cnn_5\\lib\\site-packages\\keras\\engine\\data_adapter.py\", line 903, in wrapped_generator\n    for data in generator_fn():\n\n  File \"d:\\Miniconda\\envs\\tiny_cnn_5\\lib\\site-packages\\keras\\engine\\data_adapter.py\", line 1050, in generator_fn\n    yield x[i]\n\n  File \"d:\\Miniconda\\envs\\tiny_cnn_5\\lib\\site-packages\\keras\\preprocessing\\image.py\", line 116, in __getitem__\n    return self._get_batches_of_transformed_samples(index_array)\n\n  File \"d:\\Miniconda\\envs\\tiny_cnn_5\\lib\\site-packages\\keras\\preprocessing\\image.py\", line 370, in _get_batches_of_transformed_samples\n    img = image_utils.load_img(\n\n  File \"d:\\Miniconda\\envs\\tiny_cnn_5\\lib\\site-packages\\keras\\utils\\image_utils.py\", line 422, in load_img\n    with open(path, \"rb\") as f:\n\nFileNotFoundError: [Errno 2] No such file or directory: 'i:\\\\tinyml\\\\tiny_mlperf\\\\vw_coco2014_96\\\\non_person\\\\COCO_val2014_000000565543.jpg'\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n\t [[IteratorGetNext/_2]]\n  (1) UNKNOWN:  FileNotFoundError: [Errno 2] No such file or directory: 'i:\\\\tinyml\\\\tiny_mlperf\\\\vw_coco2014_96\\\\non_person\\\\COCO_val2014_000000565543.jpg'\nTraceback (most recent call last):\n\n  File \"d:\\Miniconda\\envs\\tiny_cnn_5\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 271, in __call__\n    ret = func(*args)\n\n  File \"d:\\Miniconda\\envs\\tiny_cnn_5\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"d:\\Miniconda\\envs\\tiny_cnn_5\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 1035, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"d:\\Miniconda\\envs\\tiny_cnn_5\\lib\\site-packages\\keras\\engine\\data_adapter.py\", line 903, in wrapped_generator\n    for data in generator_fn():\n\n  File \"d:\\Miniconda\\envs\\tiny_cnn_5\\lib\\site-packages\\keras\\engine\\data_adapter.py\", line 1050, in generator_fn\n    yield x[i]\n\n  File \"d:\\Miniconda\\envs\\tiny_cnn_5\\lib\\site-packages\\keras\\preprocessing\\image.py\", line 116, in __getitem__\n    return self._get_batches_of_transformed_samples(index_array)\n\n  File \"d:\\Miniconda\\envs\\tiny_cnn_5\\lib\\site-packages\\keras\\preprocessing\\image.py\", line 370, in _get_batches_of_transformed_samples\n    img = image_utils.load_img(\n\n  File \"d:\\Miniconda\\envs\\tiny_cnn_5\\lib\\site-packages\\keras\\utils\\image_utils.py\", line 422, in load_img\n    with open(path, \"rb\") as f:\n\nFileNotFoundError: [Errno 2] No such file or directory: 'i:\\\\tinyml\\\\tiny_mlperf\\\\vw_coco2014_96\\\\non_person\\\\COCO_val2014_000000565543.jpg'\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_11978]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [42], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m train_epochs(model, train_generator, val_generator, \u001b[39m20\u001b[39;49m, \u001b[39m0.001\u001b[39;49m)\n\u001b[0;32m      2\u001b[0m model \u001b[39m=\u001b[39m train_epochs(model, train_generator, val_generator, \u001b[39m10\u001b[39m, \u001b[39m0.0005\u001b[39m)\n\u001b[0;32m      3\u001b[0m model \u001b[39m=\u001b[39m train_epochs(model, train_generator, val_generator, \u001b[39m20\u001b[39m, \u001b[39m0.00025\u001b[39m)\n",
      "Cell \u001b[1;32mIn [41], line 8\u001b[0m, in \u001b[0;36mtrain_epochs\u001b[1;34m(model, train_generator, val_generator, epoch_count, learning_rate)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_epochs\u001b[39m(model, train_generator, val_generator, epoch_count,\n\u001b[0;32m      2\u001b[0m                  learning_rate):\n\u001b[0;32m      3\u001b[0m   model\u001b[39m.\u001b[39mcompile(\n\u001b[0;32m      4\u001b[0m       optimizer\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mAdam(learning_rate),\n\u001b[0;32m      5\u001b[0m       \u001b[39m#loss='categorical_crossentropy',\u001b[39;00m\n\u001b[0;32m      6\u001b[0m       loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msparse_categorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      7\u001b[0m       metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m----> 8\u001b[0m   history_fine \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m      9\u001b[0m       train_generator,\n\u001b[0;32m     10\u001b[0m       steps_per_epoch\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(train_generator),\n\u001b[0;32m     11\u001b[0m       epochs\u001b[39m=\u001b[39;49mepoch_count,\n\u001b[0;32m     12\u001b[0m       validation_data\u001b[39m=\u001b[39;49mval_generator,\n\u001b[0;32m     13\u001b[0m       validation_steps\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(val_generator),\n\u001b[0;32m     14\u001b[0m       batch_size\u001b[39m=\u001b[39;49mBATCH_SIZE)\n\u001b[0;32m     15\u001b[0m   \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[1;32md:\\Miniconda\\envs\\tiny_cnn_5\\lib\\site-packages\\wandb\\integration\\keras\\keras.py:174\u001b[0m, in \u001b[0;36mpatch_tf_keras.<locals>.new_v2\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[39mfor\u001b[39;00m cbk \u001b[39min\u001b[39;00m cbks:\n\u001b[0;32m    173\u001b[0m         set_wandb_attrs(cbk, val_data)\n\u001b[1;32m--> 174\u001b[0m \u001b[39mreturn\u001b[39;00m old_v2(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Miniconda\\envs\\tiny_cnn_5\\lib\\site-packages\\wandb\\integration\\keras\\keras.py:174\u001b[0m, in \u001b[0;36mpatch_tf_keras.<locals>.new_v2\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[39mfor\u001b[39;00m cbk \u001b[39min\u001b[39;00m cbks:\n\u001b[0;32m    173\u001b[0m         set_wandb_attrs(cbk, val_data)\n\u001b[1;32m--> 174\u001b[0m \u001b[39mreturn\u001b[39;00m old_v2(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Miniconda\\envs\\tiny_cnn_5\\lib\\site-packages\\wandb\\integration\\keras\\keras.py:174\u001b[0m, in \u001b[0;36mpatch_tf_keras.<locals>.new_v2\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[39mfor\u001b[39;00m cbk \u001b[39min\u001b[39;00m cbks:\n\u001b[0;32m    173\u001b[0m         set_wandb_attrs(cbk, val_data)\n\u001b[1;32m--> 174\u001b[0m \u001b[39mreturn\u001b[39;00m old_v2(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Miniconda\\envs\\tiny_cnn_5\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32md:\\Miniconda\\envs\\tiny_cnn_5\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mUnknownError\u001b[0m: Graph execution error:\n\n2 root error(s) found.\n  (0) UNKNOWN:  FileNotFoundError: [Errno 2] No such file or directory: 'i:\\\\tinyml\\\\tiny_mlperf\\\\vw_coco2014_96\\\\non_person\\\\COCO_val2014_000000565543.jpg'\nTraceback (most recent call last):\n\n  File \"d:\\Miniconda\\envs\\tiny_cnn_5\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 271, in __call__\n    ret = func(*args)\n\n  File \"d:\\Miniconda\\envs\\tiny_cnn_5\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"d:\\Miniconda\\envs\\tiny_cnn_5\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 1035, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"d:\\Miniconda\\envs\\tiny_cnn_5\\lib\\site-packages\\keras\\engine\\data_adapter.py\", line 903, in wrapped_generator\n    for data in generator_fn():\n\n  File \"d:\\Miniconda\\envs\\tiny_cnn_5\\lib\\site-packages\\keras\\engine\\data_adapter.py\", line 1050, in generator_fn\n    yield x[i]\n\n  File \"d:\\Miniconda\\envs\\tiny_cnn_5\\lib\\site-packages\\keras\\preprocessing\\image.py\", line 116, in __getitem__\n    return self._get_batches_of_transformed_samples(index_array)\n\n  File \"d:\\Miniconda\\envs\\tiny_cnn_5\\lib\\site-packages\\keras\\preprocessing\\image.py\", line 370, in _get_batches_of_transformed_samples\n    img = image_utils.load_img(\n\n  File \"d:\\Miniconda\\envs\\tiny_cnn_5\\lib\\site-packages\\keras\\utils\\image_utils.py\", line 422, in load_img\n    with open(path, \"rb\") as f:\n\nFileNotFoundError: [Errno 2] No such file or directory: 'i:\\\\tinyml\\\\tiny_mlperf\\\\vw_coco2014_96\\\\non_person\\\\COCO_val2014_000000565543.jpg'\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n\t [[IteratorGetNext/_2]]\n  (1) UNKNOWN:  FileNotFoundError: [Errno 2] No such file or directory: 'i:\\\\tinyml\\\\tiny_mlperf\\\\vw_coco2014_96\\\\non_person\\\\COCO_val2014_000000565543.jpg'\nTraceback (most recent call last):\n\n  File \"d:\\Miniconda\\envs\\tiny_cnn_5\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 271, in __call__\n    ret = func(*args)\n\n  File \"d:\\Miniconda\\envs\\tiny_cnn_5\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"d:\\Miniconda\\envs\\tiny_cnn_5\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 1035, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"d:\\Miniconda\\envs\\tiny_cnn_5\\lib\\site-packages\\keras\\engine\\data_adapter.py\", line 903, in wrapped_generator\n    for data in generator_fn():\n\n  File \"d:\\Miniconda\\envs\\tiny_cnn_5\\lib\\site-packages\\keras\\engine\\data_adapter.py\", line 1050, in generator_fn\n    yield x[i]\n\n  File \"d:\\Miniconda\\envs\\tiny_cnn_5\\lib\\site-packages\\keras\\preprocessing\\image.py\", line 116, in __getitem__\n    return self._get_batches_of_transformed_samples(index_array)\n\n  File \"d:\\Miniconda\\envs\\tiny_cnn_5\\lib\\site-packages\\keras\\preprocessing\\image.py\", line 370, in _get_batches_of_transformed_samples\n    img = image_utils.load_img(\n\n  File \"d:\\Miniconda\\envs\\tiny_cnn_5\\lib\\site-packages\\keras\\utils\\image_utils.py\", line 422, in load_img\n    with open(path, \"rb\") as f:\n\nFileNotFoundError: [Errno 2] No such file or directory: 'i:\\\\tinyml\\\\tiny_mlperf\\\\vw_coco2014_96\\\\non_person\\\\COCO_val2014_000000565543.jpg'\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_11978]"
     ]
    }
   ],
   "source": [
    "model = train_epochs(model, train_generator, val_generator, 20, 0.001)\n",
    "model = train_epochs(model, train_generator, val_generator, 10, 0.0005)\n",
    "model = train_epochs(model, train_generator, val_generator, 20, 0.00025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_ds, val_ds, test_ds, labels = get_lemon_quality_dataset(dataset_path, IMG_WIDTH, IMG_HEIGHT, BATCH_SIZE, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (IMG_WIDTH, IMG_HEIGHT, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds, test_ds, labels = get_vvw_dataset(input_shape, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for images, labels in train_ds.take(1):\n",
    "#   for i in range(9):\n",
    "#     img = tf.keras.preprocessing.image.array_to_img(\n",
    "#                 images[0], scale=True\n",
    "#             )\n",
    "#     display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_names = labels\n",
    "# print(class_names)\n",
    "\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# for images, labels in train_ds.take(1):\n",
    "#   for i in range(9):\n",
    "#     ax = plt.subplot(3, 3, i + 1)\n",
    "#     plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "#     plt.title(class_names[labels[i]])\n",
    "#     plt.axis(\"off\")'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# img = np.random.random(size=(100, 100, 3))\n",
    "# pil_img = tf.keras.utils.array_to_img(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for image_batch, labels_batch in train_ds:\n",
    "#     print(image_batch.shape)\n",
    "#     print(labels_batch.shape)\n",
    "#     print(labels_batch)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes = len(labels)\n",
    "# print(f\"The dataset contains {classes } classes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = os.path.join(\"logs\", model_name, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "root_logdir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wandb.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# optimize the data flow\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "#train_ds = train_ds.cache().prefetch(AUTOTUNE)\n",
    "train_ds = train_ds.prefetch(AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#api = wandb.Api()\n",
    "api = wandb.Api(timeout=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code taken from https://www.tensorflow.org/guide/keras/custom_callback#examples_of_keras_callback_applications\n",
    "\n",
    "class EarlyStoppingAtMinLoss(keras.callbacks.Callback):\n",
    "    \"\"\"Stop training when the loss is at its min, i.e. the loss stops decreasing.\n",
    "\n",
    "  Arguments:\n",
    "      patience: Number of epochs to wait after min has been hit. After this\n",
    "      number of no improvement, training stops.\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self, patience=0):\n",
    "        super(EarlyStoppingAtMinLoss, self).__init__()\n",
    "        self.patience = patience\n",
    "        # best_weights to store the weights at which the minimum loss occurs.\n",
    "        self.best_weights = None\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        # The number of epoch it has waited when loss is no longer minimum.\n",
    "        self.wait = 0\n",
    "        # The epoch the training stops at.\n",
    "        self.stopped_epoch = 0\n",
    "        # Initialize the best as infinity.\n",
    "        self.best = np.Inf\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = logs.get(\"loss\")\n",
    "        if np.less(current, self.best):\n",
    "            self.best = current\n",
    "            self.wait = 0\n",
    "            # Record the best weights if current results is better (less).\n",
    "            self.best_weights = self.model.get_weights()\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                self.stopped_epoch = epoch\n",
    "                self.model.stop_training = True\n",
    "                print(\"Restoring model weights from the end of the best epoch.\")\n",
    "                self.model.set_weights(self.best_weights)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.stopped_epoch > 0:\n",
    "            print(\"Epoch %05d: early stopping\" % (self.stopped_epoch + 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStoppingAtMaxValAccuracy(keras.callbacks.Callback):\n",
    "    \"\"\"Stop training when the loss is at its min, i.e. the loss stops decreasing.\n",
    "\n",
    "  Arguments:\n",
    "      patience: Number of epochs to wait after max has been hit. After this\n",
    "      number of no improvement, training stops.\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self, patience=30):\n",
    "        super(EarlyStoppingAtMaxValAccuracy, self).__init__()\n",
    "        self.patience = patience\n",
    "        # best_weights to store the weights at which the minimum loss occurs.\n",
    "        self.best_weights = None\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        # The number of epoch it has waited when loss is no longer minimum.\n",
    "        self.wait = 0\n",
    "        # The epoch the training stops at.\n",
    "        self.stopped_epoch = 0\n",
    "        # Initialize the best as infinity.\n",
    "        self.best = 0\n",
    "        self.best_epoch = 0\n",
    "        self.best_epoch_loss = np.Infinity\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = logs.get(\"val_accuracy\")\n",
    "        if np.greater(current, self.best):\n",
    "            self.best = current\n",
    "            self.best_epoch = epoch\n",
    "            self.best_epoch_loss = logs.get(\"val_loss\")\n",
    "            self.wait = 0\n",
    "            # Record the best weights if current results is better (less).\n",
    "            self.best_weights = self.model.get_weights()\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                self.stopped_epoch = epoch\n",
    "                self.model.stop_training = True\n",
    "                print(\"Restoring model weights from the end of the best epoch.\")\n",
    "                self.model.set_weights(self.best_weights)\n",
    "\n",
    "        metrics = dict()\n",
    "        metrics[\"best_epoch\"] = self.best_epoch\n",
    "        metrics[\"best_val_accuracy\"] = self.best\n",
    "        metrics[\"best_epoch_loss\"] = self.best_epoch_loss\n",
    "\n",
    "        wandb.log(metrics)\n",
    "\n",
    "\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.stopped_epoch > 0:\n",
    "            print(\"Epoch %05d: early stopping\" % (self.stopped_epoch + 1))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_MODE\"] = \"online\"\n",
    "#os.environ[\"WANDB_MODE\"] = \"offline\"\n",
    "def train_model_wandb(model):\n",
    "\n",
    "        # solve issue from: https://github.com/wandb/wandb/issues/3536\n",
    "        if len(wandb.patched[\"tensorboard\"]) > 0:\n",
    "                wandb.tensorboard.unpatch()\n",
    "                \n",
    "        # Configure Tensorboard root log directory to read the debugging information\n",
    "        wandb.tensorboard.patch(root_logdir=root_logdir)\n",
    "        # wandb.tensorboard.patch(root_logdir=\"wandb.run.dir\")\n",
    "        \n",
    "        # Generate run ids\n",
    "        id = wandb.util.generate_id()\n",
    "\n",
    "        run = wandb.init(\n",
    "                # Set the project where this run will be logged\n",
    "                project=PROJECT, \n",
    "                id = id, \n",
    "                resume=True,\n",
    "                sync_tensorboard=True\n",
    "                )\n",
    "\n",
    "        # Specify the configuration variables\n",
    "        config = wandb.config\n",
    "        \n",
    "        config.batch_size = BATCH_SIZE\n",
    "        #config.dropout =DROPOUT\n",
    "        config.learn_rate = LR\n",
    "        config.momentum = MOMENTUM\n",
    "        #config.decay = 1e-6\n",
    "        config.epochs = EPOCHS\n",
    "        config.classes = classes\n",
    "        config.id = id\n",
    "        config.seed = seed\n",
    "        config.architecture = model_name\n",
    "        \n",
    "\n",
    "        # enable Tensorflow Debugging\n",
    "        #tf.debugging.experimental.enable_dump_debug_info(\"./logs/debug\", \n",
    "        #        tensor_debug_mode=\"FULL_HEALTH\", circular_buffer_size=-1)\n",
    "\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=LR)\n",
    "\n",
    "       # optimizer = tf.keras.optimizers.SGD(learning_rate=LR, momentum=MOMENTUM)\n",
    "        config.optimizer = optimizer._name\n",
    "\n",
    "        model.compile(optimizer=optimizer,\n",
    "                        loss='sparse_categorical_crossentropy',\n",
    "                        metrics=['accuracy'])\n",
    "\n",
    "        logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir= wandb.run.dir, histogram_freq=0, update_freq=\"epoch\") #, profile_batch=\"10, 20\")\n",
    "        #tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir= logdir, histogram_freq=1)\n",
    "        #wandb_callback = WandbCallback()# input_type=\"image\", labels=labels) #, validation_data = val_ds.as_numpy_iterator())\n",
    "\n",
    "        def lr_schedule(epoch):\n",
    "                \"\"\"\n",
    "                Returns a custom learning rate that decreases as epochs progress.\n",
    "                \"\"\"\n",
    "                learning_rate = LR\n",
    "                if epoch > 20:\n",
    "                        learning_rate = 0.0001\n",
    "                tf.summary.scalar('learning rate', data=learning_rate, step=epoch)\n",
    "                return learning_rate\n",
    "\n",
    "        def lr_vvw(epoch, lr):\n",
    "                \"\"\"\n",
    "                Returns the learing rate schedule used in training for Visual Wake Word dataset.\n",
    "                \"\"\"\n",
    "                lr = LR\n",
    "                if epoch == 1:\n",
    "                        lr = 0.045\n",
    "                else:\n",
    "                        lr = lr*0.98\n",
    "                return lr\n",
    "\n",
    "\n",
    "        lr_callback = LearningRateScheduler(lr_vvw)\n",
    "        #lr_callback = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "        #best_model_path = Path(wandb.run.dir).joinpath(f\"best_model\")\n",
    "\n",
    "        checkpoint = WandbModelCheckpoint(\"best_model\",\n",
    "                monitor=\"val_accuracy\",\n",
    "                save_best_only=True,\n",
    "                save_freq=\"epoch\")\n",
    "\n",
    "        global early_stopping_patience\n",
    "        early_stopping = EarlyStopping(monitor=\"val_accuracy\", patience=early_stopping_patience)\n",
    "\n",
    "        callbacks =[\n",
    "                tensorboard_callback,\n",
    "                lr_callback,\n",
    "                #wandb_callback,\n",
    "                WandbMetricsLogger(),\n",
    "                checkpoint,\n",
    "                #early_stopping,\n",
    "                EarlyStoppingAtMaxValAccuracy()\n",
    "        ]\n",
    "\n",
    "        history = model.fit(train_ds,\n",
    "                epochs=EPOCHS, \n",
    "                validation_data=val_ds, \n",
    "                callbacks=callbacks\n",
    "        )\n",
    "\n",
    "        #wandb.save(\"last_model.h5\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #best_model = keras.models.load_model(best_model_path) # not needed due to \"restore_best_weights=True\"\n",
    "\n",
    "        y_val_true = np.concatenate([y for x, y in val_ds], axis=0)\n",
    "        y_val_pred = model.predict(val_ds).argmax(axis=1)\n",
    "\n",
    "        y_test_true = np.concatenate([y for x, y in test_ds], axis=0)\n",
    "        y_test_pred = model.predict(test_ds).argmax(axis=1)\n",
    "\n",
    "        results = model.evaluate(test_ds, batch_size=BATCH_SIZE)\n",
    "        print(\"test loss, test acc:\", results)\n",
    "        wandb.log({\n",
    "                \"test_loss\" : results[0],\n",
    "                \"test_accuracy\" : results[1]\n",
    "        })\n",
    "\n",
    "        # log data for the confusion matrix\n",
    "        # wandb.log({\"conf_mat\" : wandb.plot.confusion_matrix(probs=None,\n",
    "        #                 y_true=y_test_true[0], preds=y_test_pred,\n",
    "        #                 class_names=labels)})\n",
    "\n",
    "\n",
    "        run.finish()\n",
    "        return history, model, run.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msusbrock\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.9 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>i:\\tinyml\\tiny_cnn\\wandb\\run-20230130_203627-1apprpn0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/susbrock/mobilenetv2/runs/1apprpn0\" target=\"_blank\">fortuitous-dog-72</a></strong> to <a href=\"https://wandb.ai/susbrock/mobilenetv2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m When using `save_best_only`, ensure that the `filepath` argument contains formatting placeholders like `{epoch:02d}` or `{batch:02d}`. This ensures correct interpretation of the logged artifacts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "934/934 [==============================] - ETA: 0s - loss: 0.6719 - accuracy: 0.6329INFO:tensorflow:Assets written to: best_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 578s 608ms/step - loss: 0.6719 - accuracy: 0.6329 - val_loss: 0.7006 - val_accuracy: 0.5964 - lr: 0.0441\n",
      "Epoch 2/30\n",
      "934/934 [==============================] - ETA: 0s - loss: 0.5950 - accuracy: 0.6882INFO:tensorflow:Assets written to: best_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 97s 104ms/step - loss: 0.5950 - accuracy: 0.6882 - val_loss: 0.6185 - val_accuracy: 0.6702 - lr: 0.0450\n",
      "Epoch 3/30\n",
      "934/934 [==============================] - ETA: 0s - loss: 0.5631 - accuracy: 0.7139"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 80s 86ms/step - loss: 0.5631 - accuracy: 0.7139 - val_loss: 0.6498 - val_accuracy: 0.6658 - lr: 0.0441\n",
      "Epoch 4/30\n",
      "934/934 [==============================] - ETA: 0s - loss: 0.5298 - accuracy: 0.7397INFO:tensorflow:Assets written to: best_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 97s 104ms/step - loss: 0.5298 - accuracy: 0.7397 - val_loss: 0.5162 - val_accuracy: 0.7502 - lr: 0.0441\n",
      "Epoch 5/30\n",
      "934/934 [==============================] - ETA: 0s - loss: 0.5007 - accuracy: 0.7583"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 80s 85ms/step - loss: 0.5007 - accuracy: 0.7583 - val_loss: 0.5491 - val_accuracy: 0.7469 - lr: 0.0441\n",
      "Epoch 6/30\n",
      "934/934 [==============================] - ETA: 0s - loss: 0.4740 - accuracy: 0.7733INFO:tensorflow:Assets written to: best_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 96s 103ms/step - loss: 0.4740 - accuracy: 0.7733 - val_loss: 0.4845 - val_accuracy: 0.7655 - lr: 0.0441\n",
      "Epoch 7/30\n",
      "934/934 [==============================] - ETA: 0s - loss: 0.4550 - accuracy: 0.7846INFO:tensorflow:Assets written to: best_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 97s 104ms/step - loss: 0.4550 - accuracy: 0.7846 - val_loss: 0.4650 - val_accuracy: 0.7787 - lr: 0.0441\n",
      "Epoch 8/30\n",
      "934/934 [==============================] - ETA: 0s - loss: 0.4416 - accuracy: 0.7941INFO:tensorflow:Assets written to: best_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 97s 103ms/step - loss: 0.4416 - accuracy: 0.7941 - val_loss: 0.4558 - val_accuracy: 0.7904 - lr: 0.0441\n",
      "Epoch 9/30\n",
      "934/934 [==============================] - ETA: 0s - loss: 0.4289 - accuracy: 0.8004"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 82s 87ms/step - loss: 0.4289 - accuracy: 0.8004 - val_loss: 0.4563 - val_accuracy: 0.7885 - lr: 0.0441\n",
      "Epoch 10/30\n",
      "934/934 [==============================] - ETA: 0s - loss: 0.4192 - accuracy: 0.8059INFO:tensorflow:Assets written to: best_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 96s 103ms/step - loss: 0.4192 - accuracy: 0.8059 - val_loss: 0.4405 - val_accuracy: 0.8007 - lr: 0.0441\n",
      "Epoch 11/30\n",
      "934/934 [==============================] - ETA: 0s - loss: 0.4110 - accuracy: 0.8099"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 78s 84ms/step - loss: 0.4110 - accuracy: 0.8099 - val_loss: 0.4561 - val_accuracy: 0.7806 - lr: 0.0441\n",
      "Epoch 12/30\n",
      "934/934 [==============================] - ETA: 0s - loss: 0.4034 - accuracy: 0.8142"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 78s 84ms/step - loss: 0.4034 - accuracy: 0.8142 - val_loss: 0.4356 - val_accuracy: 0.7982 - lr: 0.0441\n",
      "Epoch 13/30\n",
      "934/934 [==============================] - ETA: 0s - loss: 0.3978 - accuracy: 0.8180"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 78s 83ms/step - loss: 0.3978 - accuracy: 0.8180 - val_loss: 0.4432 - val_accuracy: 0.7920 - lr: 0.0441\n",
      "Epoch 14/30\n",
      "934/934 [==============================] - ETA: 0s - loss: 0.3914 - accuracy: 0.8212"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 79s 84ms/step - loss: 0.3914 - accuracy: 0.8212 - val_loss: 0.4493 - val_accuracy: 0.7929 - lr: 0.0441\n",
      "Epoch 15/30\n",
      "933/934 [============================>.] - ETA: 0s - loss: 0.3850 - accuracy: 0.8249"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 69s 73ms/step - loss: 0.3850 - accuracy: 0.8249 - val_loss: 0.4611 - val_accuracy: 0.7895 - lr: 0.0441\n",
      "Epoch 16/30\n",
      "934/934 [==============================] - ETA: 0s - loss: 0.3789 - accuracy: 0.8275"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 69s 74ms/step - loss: 0.3789 - accuracy: 0.8275 - val_loss: 0.4414 - val_accuracy: 0.7993 - lr: 0.0441\n",
      "Epoch 17/30\n",
      "934/934 [==============================] - ETA: 0s - loss: 0.3715 - accuracy: 0.8310"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 69s 73ms/step - loss: 0.3715 - accuracy: 0.8310 - val_loss: 0.4576 - val_accuracy: 0.7927 - lr: 0.0441\n",
      "Epoch 18/30\n",
      "934/934 [==============================] - ETA: 0s - loss: 0.3657 - accuracy: 0.8347"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 70s 74ms/step - loss: 0.3657 - accuracy: 0.8347 - val_loss: 0.4527 - val_accuracy: 0.7930 - lr: 0.0441\n",
      "Epoch 19/30\n",
      "934/934 [==============================] - ETA: 0s - loss: 0.3624 - accuracy: 0.8379"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 71s 76ms/step - loss: 0.3624 - accuracy: 0.8379 - val_loss: 0.4331 - val_accuracy: 0.8004 - lr: 0.0441\n",
      "Epoch 20/30\n",
      "934/934 [==============================] - ETA: 0s - loss: 0.3543 - accuracy: 0.8406"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 69s 73ms/step - loss: 0.3543 - accuracy: 0.8406 - val_loss: 0.4983 - val_accuracy: 0.7792 - lr: 0.0441\n",
      "Epoch 21/30\n",
      "934/934 [==============================] - ETA: 0s - loss: 0.3501 - accuracy: 0.8433"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 69s 74ms/step - loss: 0.3501 - accuracy: 0.8433 - val_loss: 0.4445 - val_accuracy: 0.7954 - lr: 0.0441\n",
      "Epoch 22/30\n",
      "934/934 [==============================] - ETA: 0s - loss: 0.3474 - accuracy: 0.8441"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 73s 77ms/step - loss: 0.3474 - accuracy: 0.8441 - val_loss: 0.4807 - val_accuracy: 0.7779 - lr: 0.0441\n",
      "Epoch 23/30\n",
      "934/934 [==============================] - ETA: 0s - loss: 0.3423 - accuracy: 0.8468"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 74s 79ms/step - loss: 0.3423 - accuracy: 0.8468 - val_loss: 0.4746 - val_accuracy: 0.7829 - lr: 0.0441\n",
      "Epoch 24/30\n",
      "934/934 [==============================] - ETA: 0s - loss: 0.3376 - accuracy: 0.8496"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 70s 75ms/step - loss: 0.3376 - accuracy: 0.8496 - val_loss: 0.4747 - val_accuracy: 0.7920 - lr: 0.0441\n",
      "Epoch 25/30\n",
      "934/934 [==============================] - ETA: 0s - loss: 0.3325 - accuracy: 0.8508"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 72s 77ms/step - loss: 0.3325 - accuracy: 0.8508 - val_loss: 0.4714 - val_accuracy: 0.7887 - lr: 0.0441\n",
      "Epoch 26/30\n",
      "934/934 [==============================] - ETA: 0s - loss: 0.3301 - accuracy: 0.8526"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 72s 76ms/step - loss: 0.3301 - accuracy: 0.8526 - val_loss: 0.4605 - val_accuracy: 0.7949 - lr: 0.0441\n",
      "Epoch 27/30\n",
      "934/934 [==============================] - ETA: 0s - loss: 0.3234 - accuracy: 0.8559"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 70s 75ms/step - loss: 0.3234 - accuracy: 0.8559 - val_loss: 0.4944 - val_accuracy: 0.7710 - lr: 0.0441\n",
      "Epoch 28/30\n",
      "934/934 [==============================] - ETA: 0s - loss: 0.3158 - accuracy: 0.8607"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 72s 77ms/step - loss: 0.3158 - accuracy: 0.8607 - val_loss: 0.4808 - val_accuracy: 0.7945 - lr: 0.0441\n",
      "Epoch 29/30\n",
      "934/934 [==============================] - ETA: 0s - loss: 0.3123 - accuracy: 0.8623"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 69s 74ms/step - loss: 0.3123 - accuracy: 0.8623 - val_loss: 0.5215 - val_accuracy: 0.7695 - lr: 0.0441\n",
      "Epoch 30/30\n",
      "934/934 [==============================] - ETA: 0s - loss: 0.3082 - accuracy: 0.8646"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 69s 74ms/step - loss: 0.3082 - accuracy: 0.8646 - val_loss: 0.5276 - val_accuracy: 0.7829 - lr: 0.0441\n",
      "104/104 [==============================] - 3s 16ms/step\n",
      "105/105 [==============================] - 5s 50ms/step\n",
      "105/105 [==============================] - 4s 39ms/step - loss: 0.5320 - accuracy: 0.7777\n",
      "test loss, test acc: [0.5319555401802063, 0.7777000069618225]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>▁▂▂▃▃▅▆▆▆█████████████████████</td></tr><tr><td>best_epoch_loss</td><td>█▆▆▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>best_val_accuracy</td><td>▁▄▄▆▆▇▇███████████████████████</td></tr><tr><td>epoch/accuracy</td><td>▁▃▃▄▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇███████</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>epoch/learning_rate</td><td>▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▇▆▅▅▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>epoch/lr</td><td>▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/val_accuracy</td><td>▁▄▃▆▆▇▇███▇████████▇█▇▇███▇█▇▇</td></tr><tr><td>epoch/val_loss</td><td>█▆▇▃▄▂▂▂▂▁▂▁▁▁▂▁▂▂▁▃▁▂▂▂▂▂▃▂▃▃</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>9</td></tr><tr><td>best_epoch_loss</td><td>0.4405</td></tr><tr><td>best_val_accuracy</td><td>0.80072</td></tr><tr><td>epoch/accuracy</td><td>0.86457</td></tr><tr><td>epoch/epoch</td><td>29</td></tr><tr><td>epoch/learning_rate</td><td>0.0441</td></tr><tr><td>epoch/loss</td><td>0.30823</td></tr><tr><td>epoch/lr</td><td>0.0441</td></tr><tr><td>epoch/val_accuracy</td><td>0.78285</td></tr><tr><td>epoch/val_loss</td><td>0.52762</td></tr><tr><td>test_accuracy</td><td>0.7777</td></tr><tr><td>test_loss</td><td>0.53196</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">fortuitous-dog-72</strong>: <a href=\"https://wandb.ai/susbrock/mobilenetv2/runs/1apprpn0\" target=\"_blank\">https://wandb.ai/susbrock/mobilenetv2/runs/1apprpn0</a><br/>Synced 7 W&B file(s), 0 media file(s), 23 artifact file(s) and 4 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230130_203627-1apprpn0\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()\n",
    "history, model, run_id = train_model_wandb(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wandb sync i:\\tinyml\\tiny_cnn\\wandb\\offline-run-20221227_091238-1vzrst0a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_model = wandb.restore('/best_model/saved_model', run_path=f\"{ENTITY}/{PROJECT}/{run_id}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversion to TFLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_tflite_trained_path = models_dir.joinpath(model_name, f\"{model_name}_trained.tflite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the model to the TensorFlow Lite format without quantization\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "# converter = tf.lite.TFLiteConverter.from_saved_model(models_path)\n",
    "tflite_model = converter.convert()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the model.\n",
    "with open(models_tflite_trained_path, \"wb\") as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversion to TFLite with INT8 quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_iter = test_ds.as_numpy_iterator()\n",
    "\n",
    "# for i in range(1):\n",
    "#     sample = next(sample_iter)[0]\n",
    "# print(\"Number of samples: {}\".format(sample.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def representative_data_gen():\n",
    "#     for i in range(100):\n",
    "#       yield([test_ds[i].reshape(1, 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# representative_data_gen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repr_ds = test_ds.unbatch()\n",
    "\n",
    "def representative_data_gen():\n",
    "  for i_value, o_value in repr_ds.batch(1).take(48):\n",
    "    yield [i_value]\n",
    "    \n",
    "converter_opt = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "# set the optimization flag\n",
    "converter_opt.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "# enforce integer only quantization\n",
    "converter_opt.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter_opt.inference_input_type = tf.uint8\n",
    "converter_opt.inference_output_type = tf.uint8\n",
    "\n",
    "# provide a representative dataset for quantization\n",
    "converter_opt.representative_dataset = representative_data_gen\n",
    "\n",
    "tflite_model_opt = converter_opt.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open(models_tflite_opt_path, 'wb') as f:\n",
    "  f.write(tflite_model_opt)\n",
    "models_tflite_opt_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tflite_quant_INT8(model, data_generator):\n",
    "    converter_opt = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "    # set the optimization flag\n",
    "    converter_opt.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    # enforce integer only quantization\n",
    "    converter_opt.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "    converter_opt.inference_input_type = tf.uint8\n",
    "    converter_opt.inference_output_type = tf.uint8\n",
    "\n",
    "    # provide a representative dataset for quantization\n",
    "    converter_opt.representative_dataset = data_generator\n",
    "\n",
    "    tflite_model_opt = converter_opt.convert()\n",
    "\n",
    "    return tflite_model_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf_opt = convert_tflite_quant_INT8(model, data_generator)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the TensorFlow Lite models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_image = test_ds.take(1)\n",
    "# test_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_gen = test_ds.unbatch().batch(1)\n",
    "test_gen = test_ds.as_numpy_iterator()\n",
    "#test_gen = test_gen.next() \n",
    "#test_image = test_gen.take(1)\n",
    "test_image, test_label = next(test_gen)\n",
    "test_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_test_images = len(list(test_gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tflite_predict(model_path, test_image):\n",
    "    # Initialize the interpreter\n",
    "    interpreter = tf.lite.Interpreter(model_path=str(model_path))\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    input_details = interpreter.get_input_details()[0]\n",
    "    output_details = interpreter.get_output_details()[0]\n",
    "\n",
    "    # Check if the input type is quantized, then rescale input data to uint8\n",
    "    if input_details['dtype'] == np.uint8:  # was np.uint8\n",
    "        input_scale, input_zero_point = input_details[\"quantization\"]\n",
    "        test_image = test_image / input_scale + input_zero_point\n",
    "        \n",
    "    test_image = test_image.astype(input_details[\"dtype\"])\n",
    "    interpreter.set_tensor(input_details[\"index\"], test_image)\n",
    "    #interpreter.set_tensor(input_details[\"index\"], np.expand_dims(test_image[0], axis=0)) # only needed when input shape (96, 96, 3)\n",
    "    interpreter.invoke()\n",
    "    output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
    "    prediction = output.argmax()\n",
    "    print(f\"Prediction: Class {prediction} derived from {output}\")\n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_result = tflite_predict(models_tflite_opt_path, test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tflite_predict_on_dataset(model_path, dataset):\n",
    "    # find length of dataset\n",
    "    test_gen = dataset.as_numpy_iterator()\n",
    "    num_images = len(list(test_gen))\n",
    "\n",
    "    predictions = []\n",
    "    y_trues = []\n",
    "\n",
    "    test_gen = dataset.as_numpy_iterator()\n",
    "    accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "    \n",
    "    # iterate over the complete test_set\n",
    "    for i in range(num_images):\n",
    "        test_image, y_true = next(test_gen)\n",
    "        prediction = tflite_predict(model_path, test_image)\n",
    "        predictions.append(prediction)\n",
    "        y_trues.append(y_true[0])\n",
    "        #accuracy.update_state(y_true, prediction) # TODO: correct accuracy\n",
    "        print(f\"{i}, {test_image.shape} - true label: {y_true[0]} vs {tflite_result}\")\n",
    "\n",
    "    #accuracy = (np.sum(predictions == y_trues) * 100) / num_images\n",
    "    print(f\"Accuracy: {accuracy.result()} - (Number of test samples: {num_images})\")\n",
    "    return predictions, y_trues    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preds, trues = tflite_predict_on_dataset(models_tflite_opt_path, test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy = (np.sum(preds == trues) * 100) / num_test_images\n",
    "# accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data using `evaluate`\n",
    "# print(\"Evaluate on test data\")\n",
    "# results = model.evaluate(test_ds, batch_size=BATCH_SIZE)\n",
    "# print(\"test loss, test acc:\", results)\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "# print(\"Generate predictions for 3 samples\")\n",
    "# predictions = model.predict(x_test[:3])\n",
    "# print(\"predictions shape:\", predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entity = \"susbrock\"\n",
    "\n",
    "\n",
    "# run = api.run(f\"{entity}/{PROJECT}/{run_id}\")\n",
    "# run.summary[\"test_accuracy\"] = results[1]\n",
    "# run.summary[\"test_loss\"] = results[0]\n",
    "# run.summary.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_predictions = model.predict(test_ds)\n",
    "# test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_pred_ids = test_predictions.argmax(axis=1)\n",
    "# len(top_pred_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_pred_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_true = [y for x, y in test_ds]\n",
    "# y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_true = np.concatenate([y for x, y in test_ds], axis=0)\n",
    "# len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion_mtx = tf.math.confusion_matrix(y_true, top_pred_ids, num_classes=classes)\n",
    "#     # list(ds_test.map(lambda x, y: y)),\n",
    "#     # predict_class_label_number(test_data),\n",
    "#     # num_classes=len(label_names))\n",
    "    \n",
    "# confusion_mtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_confusion_matrix(cm, labels):\n",
    "  plt.figure(figsize=(6, 6))\n",
    "  sns.heatmap(cm, xticklabels=labels, yticklabels=labels, \n",
    "              annot=True, fmt='g')\n",
    "  plt.xlabel('Prediction')\n",
    "  plt.ylabel('Label')\n",
    "  plt.show()\n",
    "  return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_plot = show_confusion_matrix(confusion_mtx, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_plot.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code reserved for troubleshooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def mobilenet_v1_keras(input_shape, classes=classes, alpha=alpha):\n",
    "#     model = tf.keras.applications.mobilenet.MobileNet(\n",
    "#         input_shape=input_shape,\n",
    "#         alpha=alpha,\n",
    "#         depth_multiplier=1,\n",
    "#         dropout=0.001,\n",
    "#         include_top=True,\n",
    "#         weights=None, #'imagenet'\n",
    "#         input_tensor=None,\n",
    "#         pooling=None,\n",
    "#         classes=classes,\n",
    "#         classifier_activation='softmax',\n",
    "#         #**kwargs\n",
    "#     )\n",
    "\n",
    "#     #model._name = model.name + \"_keras\" # model.name cannot be overritten\n",
    "\n",
    "#     return model\n",
    "#     #model = mobilenet_v1_keras((IMG_WIDTH, IMG_HEIGHT, 3), classes=classes, alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #os.environ[\"WANDB_MODE\"] = \"online\"\n",
    "# def train_model(model):\n",
    "\n",
    "#         # solve issue from: https://github.com/wandb/wandb/issues/3536\n",
    "#         # if len(wandb.patched[\"tensorboard\"]) > 0:\n",
    "#         #         wandb.tensorboard.unpatch()\n",
    "                \n",
    "#         # Configure Tensorboard root log directory to read the debugging information\n",
    "#         #wandb.tensorboard.patch(root_logdir=root_logdir)\n",
    "#         # wandb.tensorboard.patch(root_logdir=\"wandb.run.dir\")\n",
    "        \n",
    "#         # wandb.init(\n",
    "#         #         # Set the project where this run will be logged\n",
    "#         #         project=PROJECT, \n",
    "#         #         # Track hyperparameters and run metadata\n",
    "#         #         #config={\n",
    "#         #         #\"learning_rate\": LR,\n",
    "#         #         #\"epochs\": EPOCHS,\n",
    "#         #         #},\n",
    "#         #         sync_tensorboard=True\n",
    "#         #         )\n",
    "\n",
    "\n",
    "\n",
    "#         # config = wandb.config\n",
    "#         # # Specify the configuration variables\n",
    "#         # config.batch_size = BATCH_SIZE\n",
    "#         # config.dropout =DROPOUT\n",
    "#         # config.learn_rate = LR\n",
    "#         # #config.decay = 1e-6\n",
    "#         # #config.momentum = 0.9\n",
    "#         # config.epochs = EPOCHS\n",
    "#         # config.classes = classes\n",
    "        \n",
    "\n",
    "#         # enable Tensorflow Debugging\n",
    "#         #tf.debugging.experimental.enable_dump_debug_info(\"./logs/debug\", \n",
    "#         #        tensor_debug_mode=\"FULL_HEALTH\", circular_buffer_size=-1)\n",
    "\n",
    "#         #model = mobilenet\n",
    "#         model.compile(optimizer='adam',\n",
    "#                         loss='sparse_categorical_crossentropy',\n",
    "#                         metrics=['accuracy'])\n",
    "\n",
    "#         logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "#         #tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir= wandb.run.dir, histogram_freq=10, update_freq=\"epoch\") #, profile_batch=\"10, 20\")\n",
    "#         tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir= logdir) #, histogram_freq=1)\n",
    "#         #wandb_callback = WandbCallback()# input_type=\"image\", labels=labels) #, validation_data = val_ds.as_numpy_iterator())\n",
    "\n",
    "#         early_stopping = EarlyStopping(monitor=\"val_accuracy\", patience= early_stopping_patience)\n",
    "\n",
    "#         #checkpoint = ModelCheckpoint(\"my_tiny_model\", save_weights_only=True)\n",
    "\n",
    "#         callbacks =[\n",
    "#                 #tensorboard_callback,\n",
    "#                 #wandb_callback,\n",
    "#                 #WandbMetricsLogger(),\n",
    "#                 #checkpoint,\n",
    "#                 #early_stopping\n",
    "#         ]\n",
    "\n",
    "#         history = model.fit(train_ds,\n",
    "#                 epochs=EPOCHS, \n",
    "#                 validation_data=val_ds, \n",
    "#                 callbacks=callbacks\n",
    "#         )\n",
    "\n",
    "#         # wandb.log({\n",
    "#         #         \"loss\": history.history[\"loss\"],\n",
    "#         #         \"accuracy\": history.history[\"accuracy\"],\n",
    "#         #         \"val_loss\": history.history[\"val_loss\"],\n",
    "#         #         \"val_accuracy\": history.history[\"val_accuracy\"],                                \n",
    "#         # })\n",
    "        \n",
    "#         #wandb.finish()\n",
    "#         return history, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tiny_cnn_5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4b362e3f7439b35b0bc0fa2e0c3a5b3cc0154c779cd4216b658aa3553d61d067"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
