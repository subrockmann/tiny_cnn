{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code required for Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Colab\n"
     ]
    }
   ],
   "source": [
    "if not IN_COLAB:\n",
    "    print(\"No Colab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create the data directory\n",
    "# !mkdir visual_wake_words\n",
    "\n",
    "# # download and unzip dataset\n",
    "# !wget https://tiny-cnn.s3.eu-central-1.amazonaws.com/test_person.zip \n",
    "# !unzip test_person.zip -d ./visual_wake_words/test\n",
    "# !wget https://tiny-cnn.s3.eu-central-1.amazonaws.com/test_non_person.zip \n",
    "# !unzip test_non_person.zip -d ./visual_wake_words/test\n",
    "\n",
    "# !wget https://tiny-cnn.s3.eu-central-1.amazonaws.com/train_non_person.zip \n",
    "# !unzip train_non_person.zip -d ./visual_wake_words/train\n",
    "# !wget https://tiny-cnn.s3.eu-central-1.amazonaws.com/train_person.zip \n",
    "# !unzip train_person.zip -d ./visual_wake_words/train\n",
    "\n",
    "# !wget https://tiny-cnn.s3.eu-central-1.amazonaws.com/val_non_person.zip \n",
    "# !unzip val_non_person.zip -d ./visual_wake_words/val\n",
    "# !wget https://tiny-cnn.s3.eu-central-1.amazonaws.com/val_person.zip \n",
    "# !unzip val_person.zip -d ./visual_wake_words/val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount(\"/content/drive\")\n",
    "\n",
    "# %cd \"./drive/MyDrive/tiny_cnn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install mltk\n",
    "# !pip install wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Workbench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, math, datetime, configparser\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import PIL\n",
    "import PIL.Image\n",
    "import seaborn as sns\n",
    "import socket\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "#import tensorflow_datasets as tfds\n",
    "#from tensorflow import keras\n",
    "keras = tf.keras\n",
    "from keras.layers import ReLU\n",
    "from keras.layers import Input, Dense, Flatten, Conv2D,DepthwiseConv2D, MaxPooling2D, AvgPool2D, GlobalAveragePooling2D, BatchNormalization, Concatenate\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler\n",
    " # Import the necessary MLTK APIs\n",
    "#from mltk.core import view_model,  profile_model # summarize_model\n",
    "\n",
    "# import workbench.config.config\n",
    "from workbench.config.config import initialize\n",
    "from workbench.utils.utils import create_filepaths\n",
    "#from workbench.utils.utils import parse_model_name\n",
    "#from workbench.data.data import get_vvw_dataset, get_lemon_quality_dataset\n",
    "from workbench.tensorflow import set_batchnorm_momentum, set_dropout\n",
    "\n",
    "\n",
    "#from dotenv import load_dotenv\n",
    "import wandb\n",
    "#from wandb import AlertLevel\n",
    "from wandb.keras import WandbMetricsLogger, WandbModelCheckpoint\n",
    "\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mobilenetv1_0.1_96_c3_o2_l2.MV1']\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model_names = []\n",
    "    with open(\"deployment_list.txt\", \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            if len(line) < 2:\n",
    "                pass\n",
    "            else:\n",
    "                model_names.append(line.strip())\n",
    "\n",
    "    print(model_names)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOST_NAME = socket.gethostname()\n",
    "HOST_NAME\n",
    "\n",
    "if HOST_NAME in [\"default\"]:\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(\"config.ini\")\n",
    "\n",
    "    HOST_NAME = config['MACHINE']['HOST_NAME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 2.10.0\n",
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "print(\"Tensorflow version \" + tf.__version__)\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# Confirm that TensorFlow can access GPU\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if not device_name:\n",
    "  #raise SystemError('GPU device not found')\n",
    "  print('GPU device not found')\n",
    "\n",
    "else:\n",
    "  print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "global model_name\n",
    "model_name = \"mobilenetv1_0.1_96_c3_o2_l2.MV1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# DANGER ZONE: Disable warning messages\n",
    "\n",
    "import absl.logging\n",
    "absl.logging.set_verbosity(absl.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/api_docs/python/tf/random/set_seed  \n",
    "\n",
    "Operations that rely on a random seed actually derive it from two seeds: the global and operation-level seeds. This sets the global seed.\n",
    "\n",
    "Its interactions with operation-level seeds is as follows:\n",
    "\n",
    "1. If neither the global seed nor the operation seed is set: A randomly picked seed is used for this op.  \n",
    "2. If the global seed is set, but the operation seed is not: The system deterministically picks an operation seed in conjunction with the global seed so that it gets a unique random sequence. Within the same version of tensorflow and user code, this sequence is deterministic. However across different versions, this sequence might change. If the code depends on particular seeds to work, specify both global and operation-level seeds explicitly.  \n",
    "3. If the operation seed is set, but the global seed is not set: A default global seed and the specified operation seed are used to determine the random sequence.  \n",
    "4. If both the global and the operation seed are set: Both seeds are used in conjunction to determine the random sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_1 = 1\n",
    "seed_2 = 15\n",
    "seed_3 = 30\n",
    "seed_4 = 42\n",
    "seed_5 = 75\n",
    "\n",
    "seed = seed_2\n",
    "\n",
    "# set the random seeds\n",
    "#os.environ[\"TF_CUDNN_DETERMINISTIC\"]= \"1\"\n",
    "# random.seed(seed)\n",
    "# np.random.seed(seed)\n",
    "# tf.random.set_seed(seed) # setting tensorflow global seed\n",
    "tf.keras.utils.set_random_seed(seed)\n",
    "#tf.config.experimental.enable_op_determinism() \n",
    "\n",
    "# Node: 'gradient_tape/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/UnsortedSegmentSum'\n",
    "#Deterministic GPU implementation of unsorted segment reduction op not available.\n",
    "#\t [[{{node gradient_tape/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/UnsortedSegmentSum}}]] [Op:__inference_train_function_10280]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = initialize()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Wake Words dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if IN_COLAB:\n",
    "    #path to the base directory of the visual_wake_words dataset\n",
    "    vww_path = Path(\"/content/visual_wake_words\")\n",
    "\n",
    "else:\n",
    "    vww_path = Path.cwd().joinpath(\"datasets\",\"visual_wake_words\")\n",
    "vww_path.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:\\tinyml\\tiny_cnn\\models\n"
     ]
    }
   ],
   "source": [
    "models_path, models_summary_path, models_image_path, models_layer_df_path, models_tf_path, models_tflite_path, models_tflite_opt_path = create_filepaths(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(models_tf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "global base_model_name\n",
    "global alpha\n",
    "global resolution\n",
    "global channels\n",
    "global classes\n",
    "global variation\n",
    "global early_stopping_patience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name, alpha, resolution, channels, classes, variation = model_name.split(\"_\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#os.environ['WANDB_NOTEBOOK_NAME'] = 'Experiment Workbench'\n",
    "#dataset = \"lemon_quality\"\n",
    "#dataset = \"lemon_binary_datagen\"\n",
    "#dataset = \"vvw_minval\"\n",
    "#dataset = \"vvw_minval_datagen\n",
    "#dataset = \"vvw_minval_fix\"\n",
    "dataset = \"vvw_minval_datagen_fix\"\n",
    "IMG_HEIGHT = resolution\n",
    "IMG_WIDTH = resolution\n",
    "BATCH_SIZE = 50\n",
    "EPOCHS = 50\n",
    "#LOGGING_STEPS = 64\n",
    "MOMENTUM = 0.9\n",
    "LR = 0.001\n",
    "DROPOUT = 0.2\n",
    "early_stopping_patience = 30\n",
    "\n",
    "# BatchNormalization parameters\n",
    "BATCH_NORM_MOMENTUM = 0.9\n",
    "BATCH_NORM_EPSILON = 0.001\n",
    "\n",
    "PROJECT = base_model_name\n",
    "ENTITY = \"susbrock\"\n",
    "WANDB_ONLINE = True\n",
    "\n",
    "shuffle_seed = seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = float(alpha)\n",
    "resolution = int(resolution)\n",
    "classes = int(classes.strip(\"o\"))\n",
    "channels = int(channels.strip(\"c\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = set_batchnorm_momentum(model, BATCH_NORM_MOMENTUM)\n",
    "variation = variation + \".BN\" + str(BATCH_NORM_MOMENTUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = set_dropout(model, DROPOUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for layer in model.layers:\n",
    "#     if layer is isinstance(tf.keras.layers.Dropout(rate =DROPOUT)):\n",
    "#         print(layer.rate)'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "# Start a Tensorboard session\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the Lemon Quality Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemon_dataset_path = Path.cwd().joinpath(\"datasets\", \"lemon_dataset\")\n",
    "lemon_dataset_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#lemon_binary_datagen \n",
    "\n",
    "def get_lemon_binary_datagen(dataset_path, img_width, img_height, batch_size, channels, normalize=True):\n",
    "    lemon_binary_dataset_path = Path.cwd().joinpath(\"datasets\", \"lemon_dataset_binary\")\n",
    "    TRAIN_DIR = lemon_binary_dataset_path.joinpath(\"train\")\n",
    "    VAL_DIR = lemon_binary_dataset_path.joinpath(\"val\")\n",
    "    TEST_DIR = lemon_binary_dataset_path.joinpath(\"test\")\n",
    "    #BASE_DIR_TEST = Path.cwd().parent.joinpath(\"tiny_mlperf\", \"vw_coco2014_96_test\")\n",
    "    #Path.exists(BASE_DIR)\n",
    "    validation_split = 0\n",
    "    color_mode = \"rgb\"\n",
    "\n",
    "    datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        rotation_range=10,\n",
    "        width_shift_range=0.05,\n",
    "        height_shift_range=0.05,\n",
    "        zoom_range=.1,\n",
    "        horizontal_flip=True,\n",
    "        #validation_split=validation_split,\n",
    "        rescale=1. / 255)\n",
    "    \n",
    "    train_generator = datagen.flow_from_directory(\n",
    "        TRAIN_DIR,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        subset='training',\n",
    "        color_mode=color_mode,\n",
    "        class_mode=\"sparse\")\n",
    "    \n",
    "    val_generator = datagen.flow_from_directory(\n",
    "        VAL_DIR,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        subset='validation',\n",
    "        color_mode=color_mode,\n",
    "        class_mode=\"sparse\")\n",
    "    \n",
    "    test_gen =  tf.keras.preprocessing.image.ImageDataGenerator(rescale=1. / 255)\n",
    "    \n",
    "    test_generator = test_gen.flow_from_directory(\n",
    "        TEST_DIR,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=BATCH_SIZE, # was 1\n",
    "        #subset = \"validation\",\n",
    "        color_mode=color_mode,\n",
    "        class_mode=\"sparse\")\n",
    "    \n",
    "    #print (f\"Class names: {class_names}\")\n",
    "    #print(f\"Train: {train_generator.element_spec}\")\n",
    "    #print(f\"Normalize: {normalize}\")\n",
    "\n",
    "    class_names  = [\"bad_quality\", \"good_quality\"]\n",
    "    return (train_generator, val_generator, test_generator, class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_lemon_quality_dataset(dataset_path, img_width, img_height, batch_size, channels, normalize=True):\n",
    "#     \"\"\" Fetches the lemon quality dataset and prints dataset info. It normalizes the image data to range [0,1] by default.\n",
    "\n",
    "#     Args: \n",
    "#         dataset_path (Path): the file location of the dataset. Subfolders \"train\", \"test\", and \"val\" are expected.\n",
    "#         normalize (boolean): Normalizes the image data to range [0, 1]. Default: True\n",
    "\n",
    "#     Returns:\n",
    "#         (train_ds, val_ds, test_ds, class_names) (tuple(tf.datasets)): Tensorflow datasets for train, validation and test.\n",
    "    \n",
    "#     \"\"\"\n",
    "#     if dataset_path.exists():\n",
    "#         try:\n",
    "#             train_dir = dataset_path.joinpath(\"train\")\n",
    "#             val_dir = dataset_path.joinpath( \"val\")\n",
    "#             test_dir = dataset_path.joinpath( \"test\")\n",
    "#         except:\n",
    "#             print(f\"Please check the folder structure of {dataset_path}.\")\n",
    "#             raise\n",
    "\n",
    "#     channels = int(channels.strip(\"c\"))\n",
    "#     if channels==1:\n",
    "#         color_mode = \"grayscale\"\n",
    "#     else:\n",
    "#         color_mode = \"rgb\" \n",
    "#     print(f\"Color mode: {color_mode}\")\n",
    "\n",
    "#     # create the labels list to avoid inclusion of .ipynb checkpoints\n",
    "#     #labels = [\"bad_quality\", \"empty_background\", \"good_quality\"]\n",
    "\n",
    "#     print(\"Preparing training dataset...\")        \n",
    "#     train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "#         train_dir,\n",
    "#         subset=None,\n",
    "#         seed=shuffle_seed,\n",
    "#         image_size=((img_height, img_width)),\n",
    "#         #labels=labels,\n",
    "#         batch_size=batch_size,\n",
    "#         color_mode=color_mode,\n",
    "#         shuffle=True\n",
    "#         )\n",
    "    \n",
    "\n",
    "#     class_names = train_ds.class_names\n",
    "\n",
    "\n",
    "#     print(\"Preparing validation dataset...\")    \n",
    "#     val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "#         val_dir,\n",
    "#         subset=None,\n",
    "#         seed=shuffle_seed,\n",
    "#         image_size=(img_height, img_width),\n",
    "#         batch_size=batch_size,\n",
    "#         color_mode=color_mode,\n",
    "#         shuffle=True\n",
    "#         )\n",
    "    \n",
    "\n",
    "#     print(\"Preparing test dataset...\")    \n",
    "#     test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "#         test_dir,\n",
    "#         subset=None,\n",
    "#         seed=shuffle_seed,\n",
    "#         image_size=(img_height, img_width),\n",
    "#         batch_size=1,\n",
    "#         color_mode=color_mode,\n",
    "#         shuffle=False\n",
    "#         )\n",
    "    \n",
    "#     # Create a data augmentation stage with horizontal flipping, rotations, zooms\n",
    "#     data_augmentation = keras.Sequential(\n",
    "#         [\n",
    "#             tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "#             tf.keras.layers.RandomRotation(0.1),\n",
    "#             tf.keras.layers.RandomZoom(0.1),\n",
    "#         ]\n",
    "#         )\n",
    "\n",
    "#     #train_ds= train_ds.map(lambda x, y: (data_augmentation(x), y), num_parallel_calls=tf.data.AUTOTUNE )\n",
    "\n",
    "    \n",
    "#     # Normalize the data to the range [0, 1]\n",
    "#     if normalize:\n",
    "#         normalization_layer = tf.keras.layers.Rescaling(1./255, offset=-1)\n",
    "\n",
    "#         train_ds= train_ds.map(lambda x, y: (normalization_layer(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "#         val_ds= val_ds.map(lambda x, y: (normalization_layer(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "#         test_ds= test_ds.map(lambda x, y: (normalization_layer(x), y)) #, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "#     else:\n",
    "#         pass\n",
    "\n",
    "#     print (f\"Class names: {class_names}\")\n",
    "#     print(f\"Train: {train_ds.element_spec}\")\n",
    "#     print(f\"Normalize: {normalize}\")\n",
    "#     return (train_ds, val_ds, test_ds, class_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Wake Words minval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vvw_minval_dataset(dataset_path, img_width, img_height, batch_size, channels, normalize=True):\n",
    "    \n",
    "    BASE_DIR = Path.cwd().parent.joinpath(\"tiny_mlperf\", \"vw_coco2014_96\")\n",
    "    BASE_DIR_TEST = Path.cwd().parent.joinpath(\"tiny_mlperf\", \"vw_coco2014_96_test\")\n",
    "    Path.exists(BASE_DIR)\n",
    "    validation_split = 0.1\n",
    "    color_mode = \"rgb\"\n",
    "\n",
    "\n",
    "    print(\"Preparing vvw_minval_training dataset...\")        \n",
    "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        BASE_DIR,\n",
    "        validation_split=validation_split,\n",
    "        subset=\"training\",\n",
    "        seed=shuffle_seed,\n",
    "        image_size=(img_height, img_width),\n",
    "        #labels=labels,\n",
    "        batch_size=batch_size,\n",
    "        color_mode=color_mode,\n",
    "        shuffle=True\n",
    "        )\n",
    "\n",
    "    class_names = train_ds.class_names\n",
    "\n",
    "    print(\"Preparing vvw_minval_validation dataset...\")        \n",
    "    val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        BASE_DIR,\n",
    "        validation_split=validation_split,\n",
    "        subset=\"validation\",\n",
    "        seed=shuffle_seed,\n",
    "        image_size=(img_height, img_width),\n",
    "        #labels=labels,\n",
    "        batch_size=batch_size,\n",
    "        color_mode=color_mode,\n",
    "        shuffle=True\n",
    "        )\n",
    "\n",
    "    print(\"Preparing test dataset...\")        \n",
    "    test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        BASE_DIR_TEST,\n",
    "        validation_split=None,\n",
    "        #subset=\"validation\",\n",
    "        seed=shuffle_seed,\n",
    "        image_size=(img_height, img_width),\n",
    "        #labels=labels,\n",
    "        batch_size=1,\n",
    "        color_mode=color_mode,\n",
    "        shuffle=False\n",
    "        )\n",
    "\n",
    "\n",
    "    # Create a data augmentation stage with horizontal flipping, rotations, zooms\n",
    "    data_augmentation = keras.Sequential([\n",
    "            tf.keras.layers.RandomRotation(10), #0.1\n",
    "            tf.keras.layers.RandomTranslation(\n",
    "                height_factor = 0.05,\n",
    "                width_factor = 0.05),\n",
    "            tf.keras.layers.RandomZoom(0.1),\n",
    "            tf.keras.layers.RandomFlip(\"horizontal\")\n",
    "        ]\n",
    "        )\n",
    "\n",
    "    train_ds= train_ds.map(lambda x, y: (data_augmentation(x), y), num_parallel_calls=tf.data.AUTOTUNE )\n",
    "    val_ds= val_ds.map(lambda x, y: (data_augmentation(x), y), num_parallel_calls=tf.data.AUTOTUNE )\n",
    "\n",
    "    # Normalize the data to the range [0, 1]\n",
    "    normalization_layer = tf.keras.layers.Rescaling(1./255, offset=-1)\n",
    "\n",
    "    train_ds= train_ds.map(lambda x, y: (normalization_layer(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    val_ds= val_ds.map(lambda x, y: (normalization_layer(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    test_ds= test_ds.map(lambda x, y: (normalization_layer(x), y)) #, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    labels = class_names\n",
    "    \n",
    "    print (f\"Class names: {class_names}\")\n",
    "    print(f\"Train: {train_ds.element_spec}\")\n",
    "    print(f\"Normalize: {normalize}\")\n",
    "    return (train_ds, val_ds, test_ds, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vvw_minval_dataset_fix(dataset_path, img_width, img_height, batch_size, channels, normalize=True):\n",
    "    \n",
    "    TRAIN_DIR = Path.cwd().joinpath(\"datasets\", \"visual_wake_words\", \"train\")\n",
    "    VAL_DIR = Path.cwd().joinpath(\"datasets\", \"visual_wake_words\", \"val\")\n",
    "    TEST_DIR = Path.cwd().joinpath(\"datasets\", \"visual_wake_words\", \"test\")\n",
    "    #Path.exists(BASE_DIR)\n",
    "    #validation_split = 0.1\n",
    "    color_mode = \"rgb\"\n",
    "\n",
    "\n",
    "    print(\"Preparing vvw_minval_training dataset...\")        \n",
    "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        TRAIN_DIR,\n",
    "        #validation_split=validation_split,\n",
    "        #subset=\"training\",\n",
    "        seed=shuffle_seed,\n",
    "        image_size=(img_height, img_width),\n",
    "        #labels=labels,\n",
    "        batch_size=batch_size,\n",
    "        color_mode=color_mode,\n",
    "        shuffle=True\n",
    "        )\n",
    "\n",
    "    class_names = train_ds.class_names\n",
    "\n",
    "    print(\"Preparing vvw_minval_validation dataset...\")        \n",
    "    val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        VAL_DIR,\n",
    "        #validation_split=validation_split,\n",
    "        #subset=\"validation\",\n",
    "        seed=shuffle_seed,\n",
    "        image_size=(img_height, img_width),\n",
    "        #labels=labels,\n",
    "        batch_size=batch_size,\n",
    "        color_mode=color_mode,\n",
    "        shuffle=True\n",
    "        )\n",
    "\n",
    "    print(\"Preparing test dataset...\")        \n",
    "    test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        TEST_DIR,\n",
    "        #validation_split=None,\n",
    "        #subset=\"validation\",\n",
    "        seed=shuffle_seed,\n",
    "        image_size=(img_height, img_width),\n",
    "        #labels=labels,\n",
    "        batch_size=1,\n",
    "        color_mode=color_mode,\n",
    "        shuffle=False\n",
    "        )\n",
    "\n",
    "\n",
    "    # Create a data augmentation stage with horizontal flipping, rotations, zooms\n",
    "    data_augmentation = keras.Sequential([\n",
    "            tf.keras.layers.RandomRotation(10), #0.1\n",
    "            tf.keras.layers.RandomTranslation(\n",
    "                height_factor = 0.05,\n",
    "                width_factor = 0.05),\n",
    "            tf.keras.layers.RandomZoom(0.1),\n",
    "            tf.keras.layers.RandomFlip(\"horizontal\")\n",
    "        ]\n",
    "        )\n",
    "\n",
    "    train_ds= train_ds.map(lambda x, y: (data_augmentation(x), y), num_parallel_calls=tf.data.AUTOTUNE )\n",
    "    val_ds= val_ds.map(lambda x, y: (data_augmentation(x), y), num_parallel_calls=tf.data.AUTOTUNE )\n",
    "\n",
    "    # Normalize the data to the range [0, 1]\n",
    "    normalization_layer = tf.keras.layers.Rescaling(1./255, offset=-1)\n",
    "\n",
    "    train_ds= train_ds.map(lambda x, y: (normalization_layer(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    val_ds= val_ds.map(lambda x, y: (normalization_layer(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    test_ds= test_ds.map(lambda x, y: (normalization_layer(x), y)) #, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    labels = class_names\n",
    "    \n",
    "    print (f\"Class names: {class_names}\")\n",
    "    print(f\"Train: {train_ds.element_spec}\")\n",
    "    print(f\"Normalize: {normalize}\")\n",
    "    return (train_ds, val_ds, test_ds, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASE_DIR = Path.cwd().parent.joinpath(\"tiny_mlperf\", \"vw_coco2014_96\")\n",
    "# BASE_DIR_TEST = Path.cwd().parent.joinpath(\"tiny_mlperf\", \"vw_coco2014_96_test\")\n",
    "# Path.exists(BASE_DIR)\n",
    "# validation_split = 0.1\n",
    "# color_mode = \"rgb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Preparing training dataset...\")        \n",
    "# train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "#     BASE_DIR,\n",
    "#     validation_split=validation_split,\n",
    "#     subset=\"training\",\n",
    "#     seed=shuffle_seed,\n",
    "#     image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "#     #labels=labels,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     color_mode=color_mode,\n",
    "#     shuffle=True\n",
    "#     )\n",
    "\n",
    "# class_names = train_ds.class_names\n",
    "\n",
    "# print(\"Preparing validation dataset...\")        \n",
    "# val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "#     BASE_DIR,\n",
    "#     validation_split=validation_split,\n",
    "#     subset=\"validation\",\n",
    "#     seed=shuffle_seed,\n",
    "#     image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "#     #labels=labels,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     color_mode=color_mode,\n",
    "#     shuffle=True\n",
    "#     )\n",
    "\n",
    "# print(\"Preparing test dataset...\")        \n",
    "# test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "#     BASE_DIR_TEST,\n",
    "#     validation_split=None,\n",
    "#     #subset=\"validation\",\n",
    "#     seed=shuffle_seed,\n",
    "#     image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "#     #labels=labels,\n",
    "#     batch_size=1,\n",
    "#     color_mode=color_mode,\n",
    "#     shuffle=False\n",
    "#     )\n",
    "\n",
    "\n",
    "#     #   Preprocessing form Visual Wake Word Challenge:\n",
    "#     #   rotation_range=10,\n",
    "#     #   width_shift_range=0.05,\n",
    "#     #   height_shift_range=0.05,\n",
    "#     #   zoom_range=.1,\n",
    "#     #   horizontal_flip=True,\n",
    "# # Create a data augmentation stage with horizontal flipping, rotations, zooms\n",
    "# data_augmentation = keras.Sequential([\n",
    "#         tf.keras.layers.RandomRotation(0.1), #0,1\n",
    "#         tf.keras.layers.RandomTranslation(\n",
    "#             height_factor = 0.05,\n",
    "#             width_factor = 0.05\n",
    "#         ),\n",
    "#         tf.keras.layers.RandomZoom(0.1),\n",
    "#         tf.keras.layers.RandomFlip(\"horizontal\")\n",
    "#     ]\n",
    "#     )\n",
    "\n",
    "# train_ds= train_ds.map(lambda x, y: (data_augmentation(x), y), num_parallel_calls=tf.data.AUTOTUNE )\n",
    "\n",
    "\n",
    "# # Normalize the data to the range [0, 1]\n",
    "\n",
    "# normalization_layer = tf.keras.layers.Rescaling(1./255, offset=-1)\n",
    "\n",
    "# train_ds= train_ds.map(lambda x, y: (normalization_layer(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "# val_ds= val_ds.map(lambda x, y: (normalization_layer(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "# test_ds= test_ds.map(lambda x, y: (normalization_layer(x), y)) #, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "# labels = class_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vvw_minval_datagen(dataset_path, img_width, img_height, batch_size, channels, normalize=True):\n",
    "    BASE_DIR = Path.cwd().parent.joinpath(\"tiny_mlperf\", \"vw_coco2014_96\")\n",
    "    BASE_DIR_TEST = Path.cwd().parent.joinpath(\"tiny_mlperf\", \"vw_coco2014_96_test\")\n",
    "    Path.exists(BASE_DIR)\n",
    "    validation_split = 0.1\n",
    "    color_mode = \"rgb\"\n",
    "\n",
    "    datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        rotation_range=10,\n",
    "        width_shift_range=0.05,\n",
    "        height_shift_range=0.05,\n",
    "        zoom_range=.1,\n",
    "        horizontal_flip=True,\n",
    "        validation_split=validation_split,\n",
    "        rescale=1. / 255)\n",
    "    \n",
    "    train_generator = datagen.flow_from_directory(\n",
    "        BASE_DIR,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        subset='training',\n",
    "        color_mode=color_mode,\n",
    "        class_mode=\"sparse\")\n",
    "    \n",
    "    # val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    #     rotation_range=10,\n",
    "    #     width_shift_range=0.05,\n",
    "    #     height_shift_range=0.05,\n",
    "    #     zoom_range=.1,\n",
    "    #     horizontal_flip=True,\n",
    "    #     validation_split=validation_split,\n",
    "    #     rescale=1. / 255)\n",
    "\n",
    "    val_generator = datagen.flow_from_directory(\n",
    "        BASE_DIR,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        subset='validation',\n",
    "        color_mode=color_mode,\n",
    "        class_mode=\"sparse\")\n",
    "    \n",
    "    test_gen =  tf.keras.preprocessing.image.ImageDataGenerator(rescale=1. / 255)\n",
    "    \n",
    "    test_generator = test_gen.flow_from_directory(\n",
    "        BASE_DIR_TEST,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=BATCH_SIZE, # was 1\n",
    "        color_mode=color_mode,\n",
    "        subset='validation',\n",
    "        class_mode=\"sparse\")\n",
    "    \n",
    "    #print (f\"Class names: {class_names}\")\n",
    "    #print(f\"Train: {train_generator.element_spec}\")\n",
    "    #print(f\"Normalize: {normalize}\")\n",
    "\n",
    "    class_names  = [\"non_person\", \"person\"]\n",
    "    return (train_generator, val_generator, test_generator, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vvw_minval_datagen_fix(dataset_path, img_width, img_height, batch_size, channels, normalize=True):\n",
    "    TRAIN_DIR = dataset_path.joinpath(\"train\")\n",
    "    VAL_DIR = dataset_path.joinpath(\"val\")\n",
    "    TEST_DIR = dataset_path.joinpath(\"test\")\n",
    "    #Path.exists(BASE_DIR)\n",
    "    #validation_split = 0\n",
    "    color_mode = \"rgb\"\n",
    "\n",
    "    train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        rotation_range=10,\n",
    "        width_shift_range=0.05,\n",
    "        height_shift_range=0.05,\n",
    "        zoom_range=.1,\n",
    "        horizontal_flip=True,\n",
    "        #validation_split=validation_split,\n",
    "        rescale=1. / 255)\n",
    "    \n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        TRAIN_DIR,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        #subset='training',\n",
    "        color_mode=color_mode,\n",
    "        class_mode=\"sparse\",\n",
    "        shuffle=True\n",
    "        )\n",
    "    \n",
    "\n",
    "    val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "    val_generator = val_datagen.flow_from_directory(\n",
    "        VAL_DIR,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        #subset='validation',\n",
    "        color_mode=color_mode,\n",
    "        class_mode=\"sparse\",\n",
    "        shuffle=True)\n",
    "    \n",
    "    test_gen =  tf.keras.preprocessing.image.ImageDataGenerator(rescale=1. / 255)\n",
    "    \n",
    "    test_generator = test_gen.flow_from_directory(\n",
    "        TEST_DIR,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=BATCH_SIZE, # was 1\n",
    "        color_mode=color_mode,\n",
    "        #subset='validation',\n",
    "        class_mode=\"sparse\",\n",
    "        shuffle=False)\n",
    "    \n",
    "    #print (f\"Class names: {class_names}\")\n",
    "    #print(f\"Train: {train_generator.element_spec}\")\n",
    "    #print(f\"Normalize: {normalize}\")\n",
    "\n",
    "    class_names  = [\"non_person\", \"person\"]\n",
    "    return (train_generator, val_generator, test_generator, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASE_DIR = Path.cwd().parent.joinpath(\"tiny_mlperf\", \"vw_coco2014_96\")\n",
    "# BASE_DIR_TEST = Path.cwd().parent.joinpath(\"tiny_mlperf\", \"vw_coco2014_96_test\")\n",
    "# Path.exists(BASE_DIR)\n",
    "# validation_split = 0.1\n",
    "# color_mode = \"rgb\"\n",
    "\n",
    "# datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "#       rotation_range=10,\n",
    "#       width_shift_range=0.05,\n",
    "#       height_shift_range=0.05,\n",
    "#       zoom_range=.1,\n",
    "#       horizontal_flip=True,\n",
    "#       validation_split=validation_split,\n",
    "#       rescale=1. / 255)\n",
    "# train_generator = datagen.flow_from_directory(\n",
    "#     BASE_DIR,\n",
    "#     target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     subset='training',\n",
    "#     color_mode='rgb',\n",
    "#     class_mode=\"sparse\")\n",
    "# val_generator = datagen.flow_from_directory(\n",
    "#     BASE_DIR,\n",
    "#     target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     subset='validation',\n",
    "#     color_mode='rgb',\n",
    "#     class_mode=\"sparse\")\n",
    "\n",
    "\n",
    "# print(train_generator.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_epochs(model, train_generator, val_generator, epoch_count,\n",
    "#                  learning_rate):\n",
    "#   model.compile(\n",
    "#       optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "#       #loss='categorical_crossentropy',\n",
    "#       loss='sparse_categorical_crossentropy',\n",
    "#       metrics=['accuracy'])\n",
    "#   history_fine = model.fit(\n",
    "#       train_generator,\n",
    "#       steps_per_epoch=len(train_generator),\n",
    "#       epochs=epoch_count,\n",
    "#       validation_data=val_generator,\n",
    "#       validation_steps=len(val_generator),\n",
    "#       batch_size=BATCH_SIZE)\n",
    "#   return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = train_epochs(model, train_generator, val_generator, 20, 0.001)\n",
    "# model = train_epochs(model, train_generator, val_generator, 10, 0.0005)\n",
    "# model = train_epochs(model, train_generator, val_generator, 20, 0.00025)\n",
    "# model.save(filepath=\"vvw_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_shape = (IMG_WIDTH, IMG_HEIGHT, channels)\n",
    "#train_ds, val_ds, test_ds, labels = get_vvw_dataset(input_shape, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for images, labels in train_ds.take(1):\n",
    "#   for i in range(9):\n",
    "#     img = tf.keras.preprocessing.image.array_to_img(\n",
    "#                 images[0], scale=True\n",
    "#             )\n",
    "#     display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_names = labels\n",
    "# print(class_names)\n",
    "\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# for images, labels in train_ds.take(1):\n",
    "#   for i in range(9):\n",
    "#     ax = plt.subplot(3, 3, i + 1)\n",
    "#     plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "#     plt.title(class_names[labels[i]])\n",
    "#     plt.axis(\"off\")'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# img = np.random.random(size=(100, 100, 3))\n",
    "# pil_img = tf.keras.utils.array_to_img(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for image_batch, labels_batch in train_ds:\n",
    "#     print(image_batch.shape)\n",
    "#     print(labels_batch.shape)\n",
    "#     print(labels_batch)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes = len(labels)\n",
    "# print(f\"The dataset contains {classes } classes.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(name, classes):\n",
    "    if name == \"lemon_quality\":\n",
    "        train_ds, val_ds, test_ds, class_names = get_lemon_quality_dataset(lemon_dataset_path, resolution, resolution, BATCH_SIZE, channels)\n",
    "        dataset_name = \"lemon_quality\"\n",
    "\n",
    "    elif name == \"lemon_binary_datagen\":\n",
    "        train_ds, val_ds, test_ds, class_names = get_lemon_binary_datagen(None, resolution, resolution, BATCH_SIZE, channels, normalize=True)\n",
    "        dataset_name = \"lemon_binary_datagen\"\n",
    "    elif name == \"vvw_minval\":\n",
    "        train_ds, val_ds, test_ds, class_names = get_vvw_minval_dataset(None, resolution, resolution, BATCH_SIZE, channels, normalize=True)\n",
    "        dataset_name = \"vvw_minval\"\n",
    "    elif name == \"vvw_minval_fix\":\n",
    "        train_ds, val_ds, test_ds, class_names = get_vvw_minval_dataset_fix(None, resolution, resolution, BATCH_SIZE, channels, normalize=True)\n",
    "        dataset_name = \"vvw_minval_fix\"\n",
    "\n",
    "\n",
    "    elif name == \"vvw_minval_datagen\":\n",
    "        train_ds, val_ds, test_ds, class_names = get_vvw_minval_datagen(None, resolution, resolution, BATCH_SIZE, channels, normalize=True)\n",
    "        dataset_name = \"vvw_minval_datagen\"  \n",
    "    elif name == \"vvw_minval_datagen_fix\":\n",
    "        train_ds, val_ds, test_ds, class_names = get_vvw_minval_datagen_fix(vww_path, resolution, resolution, BATCH_SIZE, channels, normalize=True)\n",
    "        dataset_name = \"vvw_minval_datagen_fix\"  \n",
    "    else:\n",
    "        print(f\"Dataset {name} is not a valid dataset\")\n",
    "        train_ds, val_ds, test_ds, class_names, dataset_name = 0\n",
    "\n",
    "\n",
    "    if len(class_names) != classes:\n",
    "        print(f\"Incompatible dataset and model. \\n, \\\n",
    "            Model uses {classes} classes - dataset has {len(class_names)} classes!\")\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    return train_ds, val_ds, test_ds, class_names, dataset_name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_vvw_minval_dataset_fix(None, resolution, resolution, BATCH_SIZE, channels, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = \"vvw_minval\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 87695 images belonging to 2 classes.\n",
      "Found 10961 images belonging to 2 classes.\n",
      "Found 10963 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_ds, val_ds, test_ds, class_names, dataset_name = get_dataset(dataset, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = os.path.join(\"logs\", model_name, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "root_logdir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wandb.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_name not in  [\"vvw_minval_datagen\", \"vvw_minval_datagen_fix\", \"lemon_binary_datagen\"]:\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    # optimize the data flow\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "    #train_ds = train_ds.cache().prefetch(AUTOTUNE)\n",
    "    train_ds = train_ds.prefetch(AUTOTUNE)\n",
    "    val_ds = val_ds.cache().prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "#api = wandb.Api()\n",
    "api = wandb.Api(timeout=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code taken from https://www.tensorflow.org/guide/keras/custom_callback#examples_of_keras_callback_applications\n",
    "\n",
    "class EarlyStoppingAtMinLoss(keras.callbacks.Callback):\n",
    "    \"\"\"Stop training when the loss is at its min, i.e. the loss stops decreasing.\n",
    "\n",
    "  Arguments:\n",
    "      patience: Number of epochs to wait after min has been hit. After this\n",
    "      number of no improvement, training stops.\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self, patience=0):\n",
    "        super(EarlyStoppingAtMinLoss, self).__init__()\n",
    "        self.patience = patience\n",
    "        # best_weights to store the weights at which the minimum loss occurs.\n",
    "        self.best_weights = None\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        # The number of epoch it has waited when loss is no longer minimum.\n",
    "        self.wait = 0\n",
    "        # The epoch the training stops at.\n",
    "        self.stopped_epoch = 0\n",
    "        # Initialize the best as infinity.\n",
    "        self.best = np.Inf\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = logs.get(\"loss\")\n",
    "        if np.less(current, self.best):\n",
    "            self.best = current\n",
    "            self.wait = 0\n",
    "            # Record the best weights if current results is better (less).\n",
    "            self.best_weights = self.model.get_weights()\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                self.stopped_epoch = epoch\n",
    "                self.model.stop_training = True\n",
    "                print(\"Restoring model weights from the end of the best epoch.\")\n",
    "                self.model.set_weights(self.best_weights)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.stopped_epoch > 0:\n",
    "            print(\"Epoch %05d: early stopping\" % (self.stopped_epoch + 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStoppingAtMaxValAccuracy(keras.callbacks.Callback):\n",
    "    \"\"\"Stop training when the loss is at its min, i.e. the loss stops decreasing.\n",
    "\n",
    "  Arguments:\n",
    "      patience: Number of epochs to wait after max has been hit. After this\n",
    "      number of no improvement, training stops.\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self, patience=30):\n",
    "        super(EarlyStoppingAtMaxValAccuracy, self).__init__()\n",
    "        self.patience = patience\n",
    "        # best_weights to store the weights at which the minimum loss occurs.\n",
    "        self.best_weights = None\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        # The number of epoch it has waited when loss is no longer minimum.\n",
    "        self.wait = 0\n",
    "        # The epoch the training stops at.\n",
    "        self.stopped_epoch = 0\n",
    "        # Initialize the best as infinity.\n",
    "        self.best = 0\n",
    "        self.best_epoch = 0\n",
    "        self.best_epoch_loss = np.Infinity\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = logs.get(\"val_accuracy\")\n",
    "        if np.greater(current, self.best):\n",
    "            self.best = current\n",
    "            self.best_epoch = epoch\n",
    "            self.best_epoch_loss = logs.get(\"val_loss\")\n",
    "            self.wait = 0\n",
    "            # Record the best weights if current results is better (less).\n",
    "            self.best_weights = self.model.get_weights()\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                self.stopped_epoch = epoch\n",
    "                self.model.stop_training = True\n",
    "                print(\"Restoring model weights from the end of the best epoch.\")\n",
    "                self.model.set_weights(self.best_weights)\n",
    "\n",
    "        metrics = dict()\n",
    "        metrics[\"best_epoch\"] = self.best_epoch\n",
    "        metrics[\"best_val_accuracy\"] = self.best\n",
    "        metrics[\"best_epoch_loss\"] = self.best_epoch_loss\n",
    "\n",
    "        wandb.log(metrics)\n",
    "\n",
    "\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.stopped_epoch > 0:\n",
    "            print(\"Epoch %05d: early stopping\" % (self.stopped_epoch + 1))\n",
    "        \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "if WANDB_ONLINE == True:\n",
    "    os.environ[\"WANDB_MODE\"] = \"online\"\n",
    "else:\n",
    "    os.environ[\"WANDB_MODE\"] = \"offline\"\n",
    "    \n",
    "\n",
    "def train_model_wandb(model, train_ds, val_ds, test_ds):\n",
    "\n",
    "        # solve issue from: https://github.com/wandb/wandb/issues/3536\n",
    "        if len(wandb.patched[\"tensorboard\"]) > 0:\n",
    "                wandb.tensorboard.unpatch()\n",
    "                \n",
    "        # Configure Tensorboard root log directory to read the debugging information\n",
    "        wandb.tensorboard.patch(root_logdir=root_logdir)\n",
    "        # wandb.tensorboard.patch(root_logdir=\"wandb.run.dir\")\n",
    "        \n",
    "        # Generate run ids\n",
    "        id = wandb.sdk.lib.runid.generate_id()\n",
    "\n",
    "        run = wandb.init(\n",
    "                # Set the project where this run will be logged\n",
    "                project=PROJECT,\n",
    "                name = model_name,\n",
    "                id = id, \n",
    "                #resume=True,\n",
    "                #resume=\"must\",\n",
    "                sync_tensorboard=True\n",
    "                )\n",
    "\n",
    "        # if wandb.run.resumed:\n",
    "        #        model = keras.models.load_model(wandb.restore(\"model-best.h5\").name)\n",
    "\n",
    "        global BATCH_SIZE\n",
    "        # Specify the configuration variables\n",
    "        config = wandb.config\n",
    "        \n",
    "        config.batch_size = BATCH_SIZE\n",
    "        #config.dropout =DROPOUT\n",
    "        config.learn_rate = LR\n",
    "        config.momentum = MOMENTUM\n",
    "        #config.decay = 1e-6\n",
    "        config.epochs = EPOCHS\n",
    "        config.classes = classes\n",
    "        config.id = id\n",
    "        config.seed = seed\n",
    "        config.architecture = model_name\n",
    "        config.dataset = dataset_name\n",
    "        config.batch_norm_momentum = BATCH_NORM_MOMENTUM\n",
    "        \n",
    "\n",
    "        # enable Tensorflow Debugging\n",
    "        #tf.debugging.experimental.enable_dump_debug_info(\"./logs/debug\", \n",
    "        #        tensor_debug_mode=\"FULL_HEALTH\", circular_buffer_size=-1)\n",
    "\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=LR)\n",
    "\n",
    "       # optimizer = tf.keras.optimizers.SGD(learning_rate=LR, momentum=MOMENTUM)\n",
    "        config.optimizer = optimizer._name\n",
    "\n",
    "        model.compile(optimizer=optimizer,\n",
    "                        loss='sparse_categorical_crossentropy', # sparse_categorical_crossentropy\n",
    "                        metrics=['accuracy'])\n",
    "\n",
    "        logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir= wandb.run.dir, histogram_freq=0, update_freq=\"epoch\") #, profile_batch=\"10, 20\")\n",
    "        #tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir= logdir, histogram_freq=1)\n",
    "        #wandb_callback = WandbCallback()# input_type=\"image\", labels=labels) #, validation_data = val_ds.as_numpy_iterator())\n",
    "\n",
    "        def lr_schedule(epoch):\n",
    "                \"\"\"\n",
    "                Returns a custom learning rate that decreases as epochs progress.\n",
    "                \"\"\"\n",
    "                learning_rate = LR\n",
    "                if epoch > 20:\n",
    "                        learning_rate = 0.0001\n",
    "                tf.summary.scalar('learning rate', data=learning_rate, step=epoch)\n",
    "                return learning_rate\n",
    "\n",
    "        def lr_vvw(epoch, lr):\n",
    "                \"\"\"\n",
    "                Returns the learing rate schedule used in training for Visual Wake Word dataset.\n",
    "                \"\"\"\n",
    "        \n",
    "                if epoch <= 20: # was 20\n",
    "                        lr = 0.001\n",
    "                elif epoch <= 30:\n",
    "                        lr = 0.0005\n",
    "                else:\n",
    "                        lr = 0.00025\n",
    "                return lr\n",
    "\n",
    "\n",
    "        lr_callback = LearningRateScheduler(lr_vvw)\n",
    "        #lr_callback = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "        #best_model_path = Path(wandb.run.dir).joinpath(f\"best_model\")\n",
    "\n",
    "        checkpoint = WandbModelCheckpoint(\"best_model\",\n",
    "                monitor=\"val_accuracy\",\n",
    "                save_best_only=True,\n",
    "                save_freq=\"epoch\")\n",
    "\n",
    "        global early_stopping_patience\n",
    "        early_stopping = EarlyStopping(monitor=\"val_accuracy\", patience=early_stopping_patience) # TODO: should this be the loss?\n",
    "\n",
    "        callbacks =[\n",
    "                #tensorboard_callback,\n",
    "                lr_callback,\n",
    "                #wandb_callback,\n",
    "                WandbMetricsLogger(),\n",
    "                checkpoint,\n",
    "                #early_stopping,\n",
    "                #EarlyStoppingAtMaxValAccuracy()\n",
    "        ]\n",
    "\n",
    "        print(f\"Training on {dataset_name}\")\n",
    "\n",
    "        history = model.fit(train_ds,\n",
    "                epochs=EPOCHS, \n",
    "                validation_data=val_ds, \n",
    "                callbacks=callbacks\n",
    "        )\n",
    "\n",
    "        #wandb.save(\"last_model.h5\")\n",
    "\n",
    "        print(\"finished training\")\n",
    "\n",
    "\n",
    "        #best_model = keras.models.load_model(best_model_path) # not needed due to \"restore_best_weights=True\"\n",
    "\n",
    "        # y_val_true = np.concatenate([y for x, y in val_ds], axis=0)\n",
    "        # y_val_pred = model.predict(val_ds).argmax(axis=1)\n",
    "\n",
    "        print(\"Predict on test dataset\")\n",
    "\n",
    "\n",
    "\n",
    "        # for x_test, y_test in test_ds:\n",
    "        # #test_ds = list(test_ds)\n",
    "\n",
    "        # print(\"Get predictions on test set\")\n",
    "        # y_test_pred = model.predict(test_ds).argmax(axis=1)\n",
    "        # print(\"Get y_test_true:\")\n",
    "        # y_test_true = np.concatenate([y for x, y in test_ds], axis=0)\n",
    "\n",
    "        # print(\"finish predictions\")\n",
    "\n",
    "        print(\"evaluate on test dataset\")\n",
    "        results = model.evaluate(test_ds, batch_size=BATCH_SIZE)\n",
    "        print(\"test loss, test acc:\", results)\n",
    "        wandb.log({\n",
    "                \"test_loss\" : results[0],\n",
    "                \"test_accuracy\" : results[1]\n",
    "        })\n",
    "\n",
    "        # # log data for the confusion matrix\n",
    "        # wandb.log({\"conf_mat\" : wandb.plot.confusion_matrix(probs=None,\n",
    "        #                 y_true=y_test_true, preds=y_test_pred, #y_test_true[0]\n",
    "        #                 class_names=class_names)})\n",
    "\n",
    "\n",
    "        wandb.config.update({\"hostname\": HOST_NAME})\n",
    "\n",
    "        wandb.alert(\n",
    "        title=\"Training finished\", \n",
    "        text=f\"Training of model: {model_name} on host {HOST_NAME} has finished.\",\n",
    "        level=wandb.AlertLevel.INFO,\n",
    "        #wait_duration=300\n",
    "        )\n",
    "        \n",
    "        run.finish()\n",
    "        return history, model, run.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.13.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>i:\\tinyml\\tiny_cnn\\wandb\\run-20230225_205903-od1ga73i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/susbrock/mobilenetv2/runs/od1ga73i\" target=\"_blank\">mobilenetv2_0.25_96_c3_o2_t4l1024.MV1</a></strong> to <a href=\"https://wandb.ai/susbrock/mobilenetv2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/susbrock/mobilenetv2\" target=\"_blank\">https://wandb.ai/susbrock/mobilenetv2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/susbrock/mobilenetv2/runs/od1ga73i\" target=\"_blank\">https://wandb.ai/susbrock/mobilenetv2/runs/od1ga73i</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on vvw_minval_datagen_fix\n",
      "Epoch 1/50\n",
      "1754/1754 [==============================] - ETA: 0s - loss: 0.6746 - accuracy: 0.5910INFO:tensorflow:Assets written to: best_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1754/1754 [==============================] - 4971s 3s/step - loss: 0.6746 - accuracy: 0.5910 - val_loss: 0.6374 - val_accuracy: 0.6403 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "1754/1754 [==============================] - ETA: 0s - loss: 0.6265 - accuracy: 0.6518INFO:tensorflow:Assets written to: best_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1754/1754 [==============================] - 1285s 731ms/step - loss: 0.6265 - accuracy: 0.6518 - val_loss: 0.5940 - val_accuracy: 0.6843 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "1754/1754 [==============================] - ETA: 0s - loss: 0.5918 - accuracy: 0.6859INFO:tensorflow:Assets written to: best_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1754/1754 [==============================] - 239s 136ms/step - loss: 0.5918 - accuracy: 0.6859 - val_loss: 0.5618 - val_accuracy: 0.7119 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "1754/1754 [==============================] - ETA: 0s - loss: 0.5623 - accuracy: 0.7112INFO:tensorflow:Assets written to: best_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1754/1754 [==============================] - 248s 141ms/step - loss: 0.5623 - accuracy: 0.7112 - val_loss: 0.5424 - val_accuracy: 0.7273 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "1754/1754 [==============================] - ETA: 0s - loss: 0.5357 - accuracy: 0.7324INFO:tensorflow:Assets written to: best_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1754/1754 [==============================] - 254s 145ms/step - loss: 0.5357 - accuracy: 0.7324 - val_loss: 0.5445 - val_accuracy: 0.7300 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "1754/1754 [==============================] - ETA: 0s - loss: 0.5141 - accuracy: 0.7493INFO:tensorflow:Assets written to: best_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1754/1754 [==============================] - 244s 139ms/step - loss: 0.5141 - accuracy: 0.7493 - val_loss: 0.5337 - val_accuracy: 0.7441 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "1754/1754 [==============================] - ETA: 0s - loss: 0.4990 - accuracy: 0.7576INFO:tensorflow:Assets written to: best_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1754/1754 [==============================] - 253s 144ms/step - loss: 0.4990 - accuracy: 0.7576 - val_loss: 0.4911 - val_accuracy: 0.7597 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "1754/1754 [==============================] - ETA: 0s - loss: 0.4888 - accuracy: 0.7634INFO:tensorflow:Assets written to: best_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1754/1754 [==============================] - 249s 142ms/step - loss: 0.4888 - accuracy: 0.7634 - val_loss: 0.4804 - val_accuracy: 0.7657 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "1754/1754 [==============================] - ETA: 0s - loss: 0.4807 - accuracy: 0.7688INFO:tensorflow:Assets written to: best_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1754/1754 [==============================] - 240s 137ms/step - loss: 0.4807 - accuracy: 0.7688 - val_loss: 0.4662 - val_accuracy: 0.7770 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "1754/1754 [==============================] - ETA: 0s - loss: 0.4734 - accuracy: 0.7731"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1754/1754 [==============================] - 234s 133ms/step - loss: 0.4734 - accuracy: 0.7731 - val_loss: 0.4678 - val_accuracy: 0.7735 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "1754/1754 [==============================] - ETA: 0s - loss: 0.4679 - accuracy: 0.7767INFO:tensorflow:Assets written to: best_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1754/1754 [==============================] - 279s 159ms/step - loss: 0.4679 - accuracy: 0.7767 - val_loss: 0.4636 - val_accuracy: 0.7841 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "1754/1754 [==============================] - ETA: 0s - loss: 0.4617 - accuracy: 0.7798"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1754/1754 [==============================] - 272s 155ms/step - loss: 0.4617 - accuracy: 0.7798 - val_loss: 0.4606 - val_accuracy: 0.7758 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "1754/1754 [==============================] - ETA: 0s - loss: 0.4572 - accuracy: 0.7846INFO:tensorflow:Assets written to: best_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1754/1754 [==============================] - 272s 155ms/step - loss: 0.4572 - accuracy: 0.7846 - val_loss: 0.4396 - val_accuracy: 0.7970 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "1754/1754 [==============================] - ETA: 0s - loss: 0.4531 - accuracy: 0.7859"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1754/1754 [==============================] - 246s 140ms/step - loss: 0.4531 - accuracy: 0.7859 - val_loss: 0.4406 - val_accuracy: 0.7942 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "1754/1754 [==============================] - ETA: 0s - loss: 0.4469 - accuracy: 0.7912"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1754/1754 [==============================] - 225s 128ms/step - loss: 0.4469 - accuracy: 0.7912 - val_loss: 0.4457 - val_accuracy: 0.7917 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "1754/1754 [==============================] - ETA: 0s - loss: 0.4472 - accuracy: 0.7903"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1754/1754 [==============================] - 241s 137ms/step - loss: 0.4472 - accuracy: 0.7903 - val_loss: 0.4473 - val_accuracy: 0.7910 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "1754/1754 [==============================] - ETA: 0s - loss: 0.4429 - accuracy: 0.7924INFO:tensorflow:Assets written to: best_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1754/1754 [==============================] - 241s 137ms/step - loss: 0.4429 - accuracy: 0.7924 - val_loss: 0.4333 - val_accuracy: 0.8028 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "1754/1754 [==============================] - ETA: 0s - loss: 0.4389 - accuracy: 0.7953"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1754/1754 [==============================] - 228s 130ms/step - loss: 0.4389 - accuracy: 0.7953 - val_loss: 0.4324 - val_accuracy: 0.8003 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "1754/1754 [==============================] - ETA: 0s - loss: 0.4372 - accuracy: 0.7971INFO:tensorflow:Assets written to: best_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1754/1754 [==============================] - 245s 139ms/step - loss: 0.4372 - accuracy: 0.7971 - val_loss: 0.4284 - val_accuracy: 0.8043 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "1754/1754 [==============================] - ETA: 0s - loss: 0.4315 - accuracy: 0.7978"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1754/1754 [==============================] - 227s 129ms/step - loss: 0.4315 - accuracy: 0.7978 - val_loss: 0.4252 - val_accuracy: 0.8029 - lr: 0.0010\n",
      "Epoch 21/50\n",
      "1754/1754 [==============================] - ETA: 0s - loss: 0.4305 - accuracy: 0.7987INFO:tensorflow:Assets written to: best_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1754/1754 [==============================] - 244s 139ms/step - loss: 0.4305 - accuracy: 0.7987 - val_loss: 0.4204 - val_accuracy: 0.8067 - lr: 0.0010\n",
      "Epoch 22/50\n",
      "1754/1754 [==============================] - ETA: 0s - loss: 0.4195 - accuracy: 0.8057INFO:tensorflow:Assets written to: best_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1754/1754 [==============================] - 241s 137ms/step - loss: 0.4195 - accuracy: 0.8057 - val_loss: 0.4085 - val_accuracy: 0.8165 - lr: 5.0000e-04\n",
      "Epoch 23/50\n",
      "1754/1754 [==============================] - ETA: 0s - loss: 0.4154 - accuracy: 0.8081INFO:tensorflow:Assets written to: best_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1754/1754 [==============================] - 247s 141ms/step - loss: 0.4154 - accuracy: 0.8081 - val_loss: 0.4073 - val_accuracy: 0.8177 - lr: 5.0000e-04\n",
      "Epoch 24/50\n",
      "1754/1754 [==============================] - ETA: 0s - loss: 0.4125 - accuracy: 0.8100"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1754/1754 [==============================] - 227s 129ms/step - loss: 0.4125 - accuracy: 0.8100 - val_loss: 0.4089 - val_accuracy: 0.8150 - lr: 5.0000e-04\n",
      "Epoch 25/50\n",
      "1754/1754 [==============================] - ETA: 0s - loss: 0.4125 - accuracy: 0.8100"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1754/1754 [==============================] - 223s 127ms/step - loss: 0.4125 - accuracy: 0.8100 - val_loss: 0.4086 - val_accuracy: 0.8126 - lr: 5.0000e-04\n",
      "Epoch 26/50\n",
      "1754/1754 [==============================] - ETA: 0s - loss: 0.4102 - accuracy: 0.8117"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1754/1754 [==============================] - 225s 128ms/step - loss: 0.4102 - accuracy: 0.8117 - val_loss: 0.4078 - val_accuracy: 0.8154 - lr: 5.0000e-04\n",
      "Epoch 27/50\n",
      "1754/1754 [==============================] - ETA: 0s - loss: 0.4097 - accuracy: 0.8110"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1754/1754 [==============================] - 227s 129ms/step - loss: 0.4097 - accuracy: 0.8110 - val_loss: 0.4103 - val_accuracy: 0.8132 - lr: 5.0000e-04\n",
      "Epoch 28/50\n",
      "1754/1754 [==============================] - ETA: 0s - loss: 0.4062 - accuracy: 0.8136"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1754/1754 [==============================] - 223s 127ms/step - loss: 0.4062 - accuracy: 0.8136 - val_loss: 0.4069 - val_accuracy: 0.8143 - lr: 5.0000e-04\n",
      "Epoch 29/50\n",
      "1754/1754 [==============================] - ETA: 0s - loss: 0.4066 - accuracy: 0.8126"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1754/1754 [==============================] - 231s 132ms/step - loss: 0.4066 - accuracy: 0.8126 - val_loss: 0.4084 - val_accuracy: 0.8125 - lr: 5.0000e-04\n",
      "Epoch 30/50\n",
      "1754/1754 [==============================] - ETA: 0s - loss: 0.4051 - accuracy: 0.8121"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1754/1754 [==============================] - 222s 127ms/step - loss: 0.4051 - accuracy: 0.8121 - val_loss: 0.4043 - val_accuracy: 0.8152 - lr: 5.0000e-04\n",
      "Epoch 31/50\n",
      "1754/1754 [==============================] - ETA: 0s - loss: 0.4038 - accuracy: 0.8146"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1754/1754 [==============================] - 222s 127ms/step - loss: 0.4038 - accuracy: 0.8146 - val_loss: 0.4027 - val_accuracy: 0.8170 - lr: 5.0000e-04\n",
      "Epoch 32/50\n",
      "1754/1754 [==============================] - ETA: 0s - loss: 0.3972 - accuracy: 0.8182INFO:tensorflow:Assets written to: best_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1754/1754 [==============================] - 246s 140ms/step - loss: 0.3972 - accuracy: 0.8182 - val_loss: 0.3955 - val_accuracy: 0.8205 - lr: 2.5000e-04\n",
      "Epoch 33/50\n",
      "1754/1754 [==============================] - ETA: 0s - loss: 0.3951 - accuracy: 0.8199"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1754/1754 [==============================] - 230s 131ms/step - loss: 0.3951 - accuracy: 0.8199 - val_loss: 0.3965 - val_accuracy: 0.8196 - lr: 2.5000e-04\n",
      "Epoch 34/50\n",
      "1754/1754 [==============================] - ETA: 0s - loss: 0.3955 - accuracy: 0.8188INFO:tensorflow:Assets written to: best_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1754/1754 [==============================] - 243s 139ms/step - loss: 0.3955 - accuracy: 0.8188 - val_loss: 0.3963 - val_accuracy: 0.8210 - lr: 2.5000e-04\n",
      "Epoch 35/50\n",
      "1754/1754 [==============================] - ETA: 0s - loss: 0.3953 - accuracy: 0.8198"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1754/1754 [==============================] - 228s 130ms/step - loss: 0.3953 - accuracy: 0.8198 - val_loss: 0.4025 - val_accuracy: 0.8154 - lr: 2.5000e-04\n",
      "Epoch 36/50\n",
      "1754/1754 [==============================] - ETA: 0s - loss: 0.3931 - accuracy: 0.8197INFO:tensorflow:Assets written to: best_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1754/1754 [==============================] - 241s 138ms/step - loss: 0.3931 - accuracy: 0.8197 - val_loss: 0.3963 - val_accuracy: 0.8217 - lr: 2.5000e-04\n",
      "Epoch 37/50\n",
      "1754/1754 [==============================] - ETA: 0s - loss: 0.3934 - accuracy: 0.8202"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1754/1754 [==============================] - 223s 127ms/step - loss: 0.3934 - accuracy: 0.8202 - val_loss: 0.3959 - val_accuracy: 0.8198 - lr: 2.5000e-04\n",
      "Epoch 38/50\n",
      "1754/1754 [==============================] - ETA: 0s - loss: 0.3926 - accuracy: 0.8207"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1754/1754 [==============================] - 228s 130ms/step - loss: 0.3926 - accuracy: 0.8207 - val_loss: 0.3963 - val_accuracy: 0.8204 - lr: 2.5000e-04\n",
      "Epoch 39/50\n",
      "1754/1754 [==============================] - ETA: 0s - loss: 0.3892 - accuracy: 0.8233"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1754/1754 [==============================] - 231s 131ms/step - loss: 0.3892 - accuracy: 0.8233 - val_loss: 0.3961 - val_accuracy: 0.8216 - lr: 2.5000e-04\n",
      "Epoch 40/50\n",
      "1754/1754 [==============================] - ETA: 0s - loss: 0.3903 - accuracy: 0.8230"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1754/1754 [==============================] - 232s 132ms/step - loss: 0.3903 - accuracy: 0.8230 - val_loss: 0.3956 - val_accuracy: 0.8202 - lr: 2.5000e-04\n",
      "Epoch 41/50\n",
      "1754/1754 [==============================] - ETA: 0s - loss: 0.3899 - accuracy: 0.8218INFO:tensorflow:Assets written to: best_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1754/1754 [==============================] - 241s 137ms/step - loss: 0.3899 - accuracy: 0.8218 - val_loss: 0.3944 - val_accuracy: 0.8231 - lr: 2.5000e-04\n",
      "Epoch 42/50\n",
      "1754/1754 [==============================] - ETA: 0s - loss: 0.3879 - accuracy: 0.8223INFO:tensorflow:Assets written to: best_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1754/1754 [==============================] - 251s 143ms/step - loss: 0.3879 - accuracy: 0.8223 - val_loss: 0.3954 - val_accuracy: 0.8241 - lr: 2.5000e-04\n",
      "Epoch 43/50\n",
      "1754/1754 [==============================] - ETA: 0s - loss: 0.3883 - accuracy: 0.8231"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1754/1754 [==============================] - 227s 129ms/step - loss: 0.3883 - accuracy: 0.8231 - val_loss: 0.3916 - val_accuracy: 0.8224 - lr: 2.5000e-04\n",
      "Epoch 44/50\n",
      "1754/1754 [==============================] - ETA: 0s - loss: 0.3872 - accuracy: 0.8243INFO:tensorflow:Assets written to: best_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1754/1754 [==============================] - 246s 140ms/step - loss: 0.3872 - accuracy: 0.8243 - val_loss: 0.3896 - val_accuracy: 0.8267 - lr: 2.5000e-04\n",
      "Epoch 45/50\n",
      "1754/1754 [==============================] - ETA: 0s - loss: 0.3872 - accuracy: 0.8229"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1754/1754 [==============================] - 225s 128ms/step - loss: 0.3872 - accuracy: 0.8229 - val_loss: 0.3931 - val_accuracy: 0.8223 - lr: 2.5000e-04\n",
      "Epoch 46/50\n",
      "1754/1754 [==============================] - ETA: 0s - loss: 0.3854 - accuracy: 0.8240"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1754/1754 [==============================] - 226s 129ms/step - loss: 0.3854 - accuracy: 0.8240 - val_loss: 0.3909 - val_accuracy: 0.8236 - lr: 2.5000e-04\n",
      "Epoch 47/50\n",
      "1754/1754 [==============================] - ETA: 0s - loss: 0.3859 - accuracy: 0.8251INFO:tensorflow:Assets written to: best_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1754/1754 [==============================] - 239s 136ms/step - loss: 0.3859 - accuracy: 0.8251 - val_loss: 0.3891 - val_accuracy: 0.8269 - lr: 2.5000e-04\n",
      "Epoch 48/50\n",
      "1754/1754 [==============================] - ETA: 0s - loss: 0.3846 - accuracy: 0.8252"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1754/1754 [==============================] - 229s 130ms/step - loss: 0.3846 - accuracy: 0.8252 - val_loss: 0.3905 - val_accuracy: 0.8247 - lr: 2.5000e-04\n",
      "Epoch 49/50\n",
      "1754/1754 [==============================] - ETA: 0s - loss: 0.3855 - accuracy: 0.8251"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1754/1754 [==============================] - 222s 127ms/step - loss: 0.3855 - accuracy: 0.8251 - val_loss: 0.3908 - val_accuracy: 0.8246 - lr: 2.5000e-04\n",
      "Epoch 50/50\n",
      "1754/1754 [==============================] - ETA: 0s - loss: 0.3855 - accuracy: 0.8241"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1754/1754 [==============================] - 219s 125ms/step - loss: 0.3855 - accuracy: 0.8241 - val_loss: 0.3930 - val_accuracy: 0.8226 - lr: 2.5000e-04\n",
      "finished training\n",
      "Predict on test dataset\n",
      "evaluate on test dataset\n",
      "220/220 [==============================] - 89s 408ms/step - loss: 0.3867 - accuracy: 0.8253\n",
      "test loss, test acc: [0.3867431581020355, 0.8253215551376343]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f527da89a1e440f2bc9e55da7f17268d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='143.361 MB of 143.361 MB uploaded (6.715 MB deduped)\\r'), FloatProgress(value=1.0,"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td></td></tr><tr><td>epoch/epoch</td><td></td></tr><tr><td>epoch/learning_rate</td><td></td></tr><tr><td>epoch/loss</td><td></td></tr><tr><td>epoch/lr</td><td></td></tr><tr><td>epoch/val_accuracy</td><td></td></tr><tr><td>epoch/val_loss</td><td></td></tr><tr><td>test_accuracy</td><td></td></tr><tr><td>test_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>0.82408</td></tr><tr><td>epoch/epoch</td><td>49</td></tr><tr><td>epoch/learning_rate</td><td>0.00025</td></tr><tr><td>epoch/loss</td><td>0.38552</td></tr><tr><td>epoch/lr</td><td>0.00025</td></tr><tr><td>epoch/val_accuracy</td><td>0.82264</td></tr><tr><td>epoch/val_loss</td><td>0.39304</td></tr><tr><td>test_accuracy</td><td>0.82532</td></tr><tr><td>test_loss</td><td>0.38674</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">mobilenetv2_0.25_96_c3_o2_t4l1024.MV1</strong> at: <a href=\"https://wandb.ai/susbrock/mobilenetv2/runs/od1ga73i\" target=\"_blank\">https://wandb.ai/susbrock/mobilenetv2/runs/od1ga73i</a><br/>Synced 7 W&B file(s), 0 media file(s), 92 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230225_205903-od1ga73i\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()\n",
    "history, model, run_id = train_model_wandb(model, train_ds, val_ds, test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_model = wandb.restore('/best_model/saved_model', run_path=f\"{ENTITY}/{PROJECT}/{run_id}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversion to TFLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "#models_tflite_trained_path = models_dir.joinpath(model_name, f\"{model_name}_trained.tflite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert the model to the TensorFlow Lite format without quantization\n",
    "# converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "# # converter = tf.lite.TFLiteConverter.from_saved_model(models_path)\n",
    "# tflite_model = converter.convert()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Save the model.\n",
    "# with open(models_tflite_trained_path, \"wb\") as f:\n",
    "#     f.write(tflite_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversion to TFLite with INT8 quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_iter = test_ds.as_numpy_iterator()\n",
    "\n",
    "# for i in range(1):\n",
    "#     sample = next(sample_iter)[0]\n",
    "# print(\"Number of samples: {}\".format(sample.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def representative_data_gen():\n",
    "#     for i in range(100):\n",
    "#       yield([test_ds[i].reshape(1, 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# representative_data_gen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repr_ds = test_ds.unbatch()\n",
    "\n",
    "# def representative_data_gen():\n",
    "#   for i_value, o_value in repr_ds.batch(1).take(48):\n",
    "#     yield [i_value]\n",
    "    \n",
    "# converter_opt = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "# # set the optimization flag\n",
    "# converter_opt.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "# # enforce integer only quantization\n",
    "# converter_opt.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "# converter_opt.inference_input_type = tf.uint8\n",
    "# converter_opt.inference_output_type = tf.uint8\n",
    "\n",
    "# # provide a representative dataset for quantization\n",
    "# converter_opt.representative_dataset = representative_data_gen\n",
    "\n",
    "# tflite_model_opt = converter_opt.convert()\n",
    "\n",
    "# # Save the model.\n",
    "# with open(models_tflite_opt_path, 'wb') as f:\n",
    "#   f.write(tflite_model_opt)\n",
    "# models_tflite_opt_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_tflite_quant_INT8(model, data_generator):\n",
    "#     converter_opt = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "#     # set the optimization flag\n",
    "#     converter_opt.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "#     # enforce integer only quantization\n",
    "#     converter_opt.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "#     converter_opt.inference_input_type = tf.uint8\n",
    "#     converter_opt.inference_output_type = tf.uint8\n",
    "\n",
    "#     # provide a representative dataset for quantization\n",
    "#     converter_opt.representative_dataset = data_generator\n",
    "\n",
    "#     tflite_model_opt = converter_opt.convert()\n",
    "\n",
    "#     return tflite_model_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf_opt = convert_tflite_quant_INT8(model, data_generator)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the TensorFlow Lite models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_image = test_ds.take(1)\n",
    "# test_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #test_gen = test_ds.unbatch().batch(1)\n",
    "# test_gen = test_ds.as_numpy_iterator()\n",
    "# #test_gen = test_gen.next() \n",
    "# #test_image = test_gen.take(1)\n",
    "# test_image, test_label = next(test_gen)\n",
    "# test_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tflite_predict(model_path, test_image):\n",
    "    # Initialize the interpreter\n",
    "    interpreter = tf.lite.Interpreter(model_path=str(model_path))\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    input_details = interpreter.get_input_details()[0]\n",
    "    output_details = interpreter.get_output_details()[0]\n",
    "\n",
    "    # Check if the input type is quantized, then rescale input data to uint8\n",
    "    if input_details['dtype'] == np.uint8:  # was np.uint8\n",
    "        input_scale, input_zero_point = input_details[\"quantization\"]\n",
    "        test_image = test_image / input_scale + input_zero_point\n",
    "        \n",
    "    test_image = test_image.astype(input_details[\"dtype\"])\n",
    "    interpreter.set_tensor(input_details[\"index\"], test_image)\n",
    "    #interpreter.set_tensor(input_details[\"index\"], np.expand_dims(test_image[0], axis=0)) # only needed when input shape (96, 96, 3)\n",
    "    interpreter.invoke()\n",
    "    output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
    "    prediction = output.argmax()\n",
    "    print(f\"Prediction: Class {prediction} derived from {output}\")\n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tflite_result = tflite_predict(models_tflite_opt_path, test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tflite_predict_on_dataset(model_path, dataset):\n",
    "    # find length of dataset\n",
    "    test_gen = dataset.as_numpy_iterator()\n",
    "    num_images = len(list(test_gen))\n",
    "\n",
    "    predictions = []\n",
    "    y_trues = []\n",
    "\n",
    "    test_gen = dataset.as_numpy_iterator()\n",
    "    accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "    \n",
    "    # iterate over the complete test_set\n",
    "    for i in range(num_images):\n",
    "        test_image, y_true = next(test_gen)\n",
    "        prediction = tflite_predict(model_path, test_image)\n",
    "        predictions.append(prediction)\n",
    "        y_trues.append(y_true[0])\n",
    "        #accuracy.update_state(y_true, prediction) # TODO: correct accuracy\n",
    "        print(f\"{i}, {test_image.shape} - true label: {y_true[0]} vs {tflite_result}\")\n",
    "\n",
    "    #accuracy = (np.sum(predictions == y_trues) * 100) / num_images\n",
    "    print(f\"Accuracy: {accuracy.result()} - (Number of test samples: {num_images})\")\n",
    "    return predictions, y_trues    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preds, trues = tflite_predict_on_dataset(models_tflite_opt_path, test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy = (np.sum(preds == trues) * 100) / num_test_images\n",
    "# accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data using `evaluate`\n",
    "# print(\"Evaluate on test data\")\n",
    "# results = model.evaluate(test_ds, batch_size=BATCH_SIZE)\n",
    "# print(\"test loss, test acc:\", results)\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "# print(\"Generate predictions for 3 samples\")\n",
    "# predictions = model.predict(x_test[:3])\n",
    "# print(\"predictions shape:\", predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entity = \"susbrock\"\n",
    "\n",
    "\n",
    "# run = api.run(f\"{entity}/{PROJECT}/{run_id}\")\n",
    "# run.summary[\"test_accuracy\"] = results[1]\n",
    "# run.summary[\"test_loss\"] = results[0]\n",
    "# run.summary.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_predictions = model.predict(test_ds)\n",
    "# test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_pred_ids = test_predictions.argmax(axis=1)\n",
    "# len(top_pred_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_pred_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_true = [y for x, y in test_ds]\n",
    "# y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_true = np.concatenate([y for x, y in test_ds], axis=0)\n",
    "# len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion_mtx = tf.math.confusion_matrix(y_true, top_pred_ids, num_classes=classes)\n",
    "#     # list(ds_test.map(lambda x, y: y)),\n",
    "#     # predict_class_label_number(test_data),\n",
    "#     # num_classes=len(label_names))\n",
    "    \n",
    "# confusion_mtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_confusion_matrix(cm, labels):\n",
    "  plt.figure(figsize=(6, 6))\n",
    "  sns.heatmap(cm, xticklabels=labels, yticklabels=labels, \n",
    "              annot=True, fmt='g')\n",
    "  plt.xlabel('Prediction')\n",
    "  plt.ylabel('Label')\n",
    "  plt.show()\n",
    "  return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_plot = show_confusion_matrix(confusion_mtx, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_plot.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code reserved for troubleshooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def mobilenet_v1_keras(input_shape, classes=classes, alpha=alpha):\n",
    "#     model = tf.keras.applications.mobilenet.MobileNet(\n",
    "#         input_shape=input_shape,\n",
    "#         alpha=alpha,\n",
    "#         depth_multiplier=1,\n",
    "#         dropout=0.001,\n",
    "#         include_top=True,\n",
    "#         weights=None, #'imagenet'\n",
    "#         input_tensor=None,\n",
    "#         pooling=None,\n",
    "#         classes=classes,\n",
    "#         classifier_activation='softmax',\n",
    "#         #**kwargs\n",
    "#     )\n",
    "\n",
    "#     #model._name = model.name + \"_keras\" # model.name cannot be overritten\n",
    "\n",
    "#     return model\n",
    "#     #model = mobilenet_v1_keras((IMG_WIDTH, IMG_HEIGHT, 3), classes=classes, alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #os.environ[\"WANDB_MODE\"] = \"online\"\n",
    "# def train_model(model):\n",
    "\n",
    "#         # solve issue from: https://github.com/wandb/wandb/issues/3536\n",
    "#         # if len(wandb.patched[\"tensorboard\"]) > 0:\n",
    "#         #         wandb.tensorboard.unpatch()\n",
    "                \n",
    "#         # Configure Tensorboard root log directory to read the debugging information\n",
    "#         #wandb.tensorboard.patch(root_logdir=root_logdir)\n",
    "#         # wandb.tensorboard.patch(root_logdir=\"wandb.run.dir\")\n",
    "        \n",
    "#         # wandb.init(\n",
    "#         #         # Set the project where this run will be logged\n",
    "#         #         project=PROJECT, \n",
    "#         #         # Track hyperparameters and run metadata\n",
    "#         #         #config={\n",
    "#         #         #\"learning_rate\": LR,\n",
    "#         #         #\"epochs\": EPOCHS,\n",
    "#         #         #},\n",
    "#         #         sync_tensorboard=True\n",
    "#         #         )\n",
    "\n",
    "\n",
    "\n",
    "#         # config = wandb.config\n",
    "#         # # Specify the configuration variables\n",
    "#         # config.batch_size = BATCH_SIZE\n",
    "#         # config.dropout =DROPOUT\n",
    "#         # config.learn_rate = LR\n",
    "#         # #config.decay = 1e-6\n",
    "#         # #config.momentum = 0.9\n",
    "#         # config.epochs = EPOCHS\n",
    "#         # config.classes = classes\n",
    "        \n",
    "\n",
    "#         # enable Tensorflow Debugging\n",
    "#         #tf.debugging.experimental.enable_dump_debug_info(\"./logs/debug\", \n",
    "#         #        tensor_debug_mode=\"FULL_HEALTH\", circular_buffer_size=-1)\n",
    "\n",
    "#         #model = mobilenet\n",
    "#         model.compile(optimizer='adam',\n",
    "#                         loss='sparse_categorical_crossentropy',\n",
    "#                         metrics=['accuracy'])\n",
    "\n",
    "#         logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "#         #tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir= wandb.run.dir, histogram_freq=10, update_freq=\"epoch\") #, profile_batch=\"10, 20\")\n",
    "#         tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir= logdir) #, histogram_freq=1)\n",
    "#         #wandb_callback = WandbCallback()# input_type=\"image\", labels=labels) #, validation_data = val_ds.as_numpy_iterator())\n",
    "\n",
    "#         early_stopping = EarlyStopping(monitor=\"val_accuracy\", patience= early_stopping_patience)\n",
    "\n",
    "#         #checkpoint = ModelCheckpoint(\"my_tiny_model\", save_weights_only=True)\n",
    "\n",
    "#         callbacks =[\n",
    "#                 #tensorboard_callback,\n",
    "#                 #wandb_callback,\n",
    "#                 #WandbMetricsLogger(),\n",
    "#                 #checkpoint,\n",
    "#                 #early_stopping\n",
    "#         ]\n",
    "\n",
    "#         history = model.fit(train_ds,\n",
    "#                 epochs=EPOCHS, \n",
    "#                 validation_data=val_ds, \n",
    "#                 callbacks=callbacks\n",
    "#         )\n",
    "\n",
    "#         # wandb.log({\n",
    "#         #         \"loss\": history.history[\"loss\"],\n",
    "#         #         \"accuracy\": history.history[\"accuracy\"],\n",
    "#         #         \"val_loss\": history.history[\"val_loss\"],\n",
    "#         #         \"val_accuracy\": history.history[\"val_accuracy\"],                                \n",
    "#         # })\n",
    "        \n",
    "#         #wandb.finish()\n",
    "#         return history, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tiny_cnn_6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0faa6c31b20b8f809b81d6d7d22a84ccd9f354666f54133d1793fa4c65539801"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
