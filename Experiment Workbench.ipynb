{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Workbench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, math, datetime\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import PIL\n",
    "import PIL.Image\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Conv2D,DepthwiseConv2D, MaxPooling2D, AvgPool2D, GlobalAveragePooling2D, BatchNormalization, Concatenate\n",
    "from tensorflow.keras.layers import ReLU\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler\n",
    " \n",
    "# Import the necessary MLTK APIs\n",
    "from mltk.core import view_model, summarize_model, profile_model\n",
    "\n",
    "# import workbench.config.config\n",
    "from workbench.config.config import initialize\n",
    "from workbench.utils.utils import create_filepaths\n",
    "from workbench.utils.utils import parse_model_name\n",
    "\n",
    "#from dotenv import load_dotenv\n",
    "import wandb\n",
    "from wandb.keras import WandbMetricsLogger, WandbModelCheckpoint\n",
    "#import deeplake\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 2.10.0\n",
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "print(\"Tensorflow version \" + tf.__version__)\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# Confirm that TensorFlow can access GPU\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if not device_name:\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "\n",
    "# Add learning rate schedule\n",
    "\n",
    "# tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DANGER ZONE: Disable warning messages\n",
    "\n",
    "import absl.logging\n",
    "absl.logging.set_verbosity(absl.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/api_docs/python/tf/random/set_seed  \n",
    "\n",
    "Operations that rely on a random seed actually derive it from two seeds: the global and operation-level seeds. This sets the global seed.\n",
    "\n",
    "Its interactions with operation-level seeds is as follows:\n",
    "\n",
    "1. If neither the global seed nor the operation seed is set: A randomly picked seed is used for this op.  \n",
    "2. If the global seed is set, but the operation seed is not: The system deterministically picks an operation seed in conjunction with the global seed so that it gets a unique random sequence. Within the same version of tensorflow and user code, this sequence is deterministic. However across different versions, this sequence might change. If the code depends on particular seeds to work, specify both global and operation-level seeds explicitly.  \n",
    "3. If the operation seed is set, but the global seed is not set: A default global seed and the specified operation seed are used to determine the random sequence.  \n",
    "4. If both the global and the operation seed are set: Both seeds are used in conjunction to determine the random sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_1 = 1\n",
    "seed_2 = 15\n",
    "seed_3 = 30\n",
    "seed_4 = 42\n",
    "seed_5 = 75\n",
    "\n",
    "seed = seed_1\n",
    "\n",
    "# set the random seeds\n",
    "os.environ[\"TF_CUDNN_DETERMINISTIC\"]= \"1\"\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed) # setting tensorflow global seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "global model_name\n",
    "#model_name = \"efficientNetB0_1_96_c3_o3_keras\"\n",
    "#model_name = \"mobilenetv1_0.1_96_c3_o3_keras\"\n",
    "model_name = \"shufflenetv1_0.2_96_c3_o3_g1\"\n",
    "#model_name = \"MobilenetV3small_1_96_c3_o3_keras\"#, \"MobilenetV3large_1_224_c3_o3_keras\"# ,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:\\tinyml\\tiny_cnn\\models\n"
     ]
    }
   ],
   "source": [
    "models_path, models_summary_path, models_image_path, models_layer_df_path, models_tf_path, models_tflite_path, models_tflite_opt_path = create_filepaths(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(models_tf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "global base_model_name\n",
    "global alpha\n",
    "global resolution\n",
    "global channels\n",
    "global classes\n",
    "global variation\n",
    "global early_stopping_patience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name, alpha, resolution, channels, classes, variation = model_name.split(\"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = float(alpha)\n",
    "resolution = int(resolution)\n",
    "classes = int(classes.strip(\"o\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "# Start a Tensorboard session\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#os.environ['WANDB_NOTEBOOK_NAME'] = 'Experiment Workbench'\n",
    "\n",
    "IMG_HEIGHT = resolution\n",
    "IMG_WIDTH = resolution\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 30\n",
    "#LOGGING_STEPS = 64\n",
    "MOMENTUM = 0.9\n",
    "LR = 0.001\n",
    "DROPOUT = 0.2\n",
    "early_stopping_patience = 30\n",
    "\n",
    "PROJECT = base_model_name\n",
    "#PROJECT = \"tiny_cnn troubleshooting\"\n",
    "\n",
    "shuffle_seed = seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the Lemon Quality Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = Path.cwd().joinpath(\"datasets\", \"lemon_dataset\")\n",
    "dataset_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('i:/tinyml/tiny_cnn/datasets/lemon_dataset')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemon_quality_dataset(dataset_path, img_width, img_height, batch_size, channels, normalize=True):\n",
    "    \"\"\" Fetches the lemon quality dataset and prints dataset info. It normalizes the image data to range [0,1] by default.\n",
    "\n",
    "    Args: \n",
    "        dataset_path (Path): the file location of the dataset. Subfolders \"train\", \"test\", and \"val\" are expected.\n",
    "        normalize (boolean): Normalizes the image data to range [0, 1]. Default: True\n",
    "\n",
    "    Returns:\n",
    "        (train_ds, val_ds, test_ds, class_names) (tuple(tf.datasets)): Tensorflow datasets for train, validation and test.\n",
    "    \n",
    "    \"\"\"\n",
    "    if dataset_path.exists():\n",
    "        try:\n",
    "            train_dir = dataset_path.joinpath(\"train\")\n",
    "            val_dir = dataset_path.joinpath( \"val\")\n",
    "            test_dir = dataset_path.joinpath( \"test\")\n",
    "        except:\n",
    "            print(f\"Please check the folder structure of {dataset_path}.\")\n",
    "            raise\n",
    "\n",
    "    channels = int(channels.strip(\"c\"))\n",
    "    if channels==1:\n",
    "        color_mode = \"grayscale\"\n",
    "    else:\n",
    "        color_mode = \"rgb\" \n",
    "    print(f\"Color mode: {color_mode}\")\n",
    "\n",
    "    # create the labels list to avoid inclusion of .ipynb checkpoints\n",
    "    #labels = [\"bad_quality\", \"empty_background\", \"good_quality\"]\n",
    "\n",
    "    print(\"Preparing training dataset...\")        \n",
    "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        train_dir,\n",
    "        subset=None,\n",
    "        seed=shuffle_seed,\n",
    "        image_size=((img_height, img_width)),\n",
    "        #labels=labels,\n",
    "        batch_size=batch_size,\n",
    "        #color_mode=color_mode,\n",
    "        shuffle=True\n",
    "        )\n",
    "    \n",
    "\n",
    "    class_names = train_ds.class_names\n",
    "\n",
    "\n",
    "    print(\"Preparing validation dataset...\")    \n",
    "    val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        val_dir,\n",
    "        subset=None,\n",
    "        seed=shuffle_seed,\n",
    "        image_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        #color_mode=color_mode,\n",
    "        shuffle=True\n",
    "        )\n",
    "    \n",
    "\n",
    "    print(\"Preparing test dataset...\")    \n",
    "    test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        test_dir,\n",
    "        subset=None,\n",
    "        seed=shuffle_seed,\n",
    "        image_size=(img_height, img_width),\n",
    "        batch_size=1,\n",
    "        #color_mode=color_mode,\n",
    "        shuffle=False\n",
    "        )\n",
    "    \n",
    "    # Create a data augmentation stage with horizontal flipping, rotations, zooms\n",
    "    data_augmentation = keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "            tf.keras.layers.RandomRotation(0.1),\n",
    "            tf.keras.layers.RandomZoom(0.1),\n",
    "        ]\n",
    "        )\n",
    "\n",
    "    #train_ds= train_ds.map(lambda x, y: (data_augmentation(x), y), num_parallel_calls=tf.data.AUTOTUNE )\n",
    "\n",
    "    \n",
    "    # Normalize the data to the range [0, 1]\n",
    "    if normalize:\n",
    "        normalization_layer = tf.keras.layers.Rescaling(1./255, offset=-1)\n",
    "\n",
    "        train_ds= train_ds.map(lambda x, y: (normalization_layer(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        val_ds= val_ds.map(lambda x, y: (normalization_layer(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        test_ds= test_ds.map(lambda x, y: (normalization_layer(x), y)) #, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    print (f\"Class names: {class_names}\")\n",
    "    print(f\"Train: {train_ds.element_spec}\")\n",
    "    print(f\"Normalize: {normalize}\")\n",
    "    return (train_ds, val_ds, test_ds, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Color mode: rgb\n",
      "Preparing training dataset...\n",
      "Found 2021 files belonging to 3 classes.\n",
      "Preparing validation dataset...\n",
      "Found 252 files belonging to 3 classes.\n",
      "Preparing test dataset...\n",
      "Found 255 files belonging to 3 classes.\n",
      "Class names: ['bad_quality', 'empty_background', 'good_quality']\n",
      "Train: (TensorSpec(shape=(None, 96, 96, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))\n",
      "Normalize: True\n"
     ]
    }
   ],
   "source": [
    "train_ds, val_ds, test_ds, labels = get_lemon_quality_dataset(dataset_path, IMG_WIDTH, IMG_HEIGHT, BATCH_SIZE, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_names = labels\n",
    "# print(class_names)\n",
    "\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# for images, labels in train_ds.take(1):\n",
    "#   for i in range(9):\n",
    "#     ax = plt.subplot(3, 3, i + 1)\n",
    "#     plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "#     plt.title(class_names[labels[i]])\n",
    "#     plt.axis(\"off\")'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ParallelMapDataset element_spec=(TensorSpec(shape=(None, 96, 96, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for image_batch, labels_batch in train_ds:\n",
    "#     print(image_batch.shape)\n",
    "#     print(labels_batch.shape)\n",
    "#     print(labels_batch)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes = len(labels)\n",
    "# print(f\"The dataset contains {classes } classes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = os.path.join(\"logs\", model_name, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "root_logdir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wandb.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# optimize the data flow\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "#train_ds = train_ds.cache().prefetch(AUTOTUNE)\n",
    "train_ds = train_ds.prefetch(AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#api = wandb.Api()\n",
    "api = wandb.Api(timeout=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code taken from https://www.tensorflow.org/guide/keras/custom_callback#examples_of_keras_callback_applications\n",
    "\n",
    "class EarlyStoppingAtMinLoss(keras.callbacks.Callback):\n",
    "    \"\"\"Stop training when the loss is at its min, i.e. the loss stops decreasing.\n",
    "\n",
    "  Arguments:\n",
    "      patience: Number of epochs to wait after min has been hit. After this\n",
    "      number of no improvement, training stops.\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self, patience=0):\n",
    "        super(EarlyStoppingAtMinLoss, self).__init__()\n",
    "        self.patience = patience\n",
    "        # best_weights to store the weights at which the minimum loss occurs.\n",
    "        self.best_weights = None\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        # The number of epoch it has waited when loss is no longer minimum.\n",
    "        self.wait = 0\n",
    "        # The epoch the training stops at.\n",
    "        self.stopped_epoch = 0\n",
    "        # Initialize the best as infinity.\n",
    "        self.best = np.Inf\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = logs.get(\"loss\")\n",
    "        if np.less(current, self.best):\n",
    "            self.best = current\n",
    "            self.wait = 0\n",
    "            # Record the best weights if current results is better (less).\n",
    "            self.best_weights = self.model.get_weights()\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                self.stopped_epoch = epoch\n",
    "                self.model.stop_training = True\n",
    "                print(\"Restoring model weights from the end of the best epoch.\")\n",
    "                self.model.set_weights(self.best_weights)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.stopped_epoch > 0:\n",
    "            print(\"Epoch %05d: early stopping\" % (self.stopped_epoch + 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStoppingAtMaxValAccuracy(keras.callbacks.Callback):\n",
    "    \"\"\"Stop training when the loss is at its min, i.e. the loss stops decreasing.\n",
    "\n",
    "  Arguments:\n",
    "      patience: Number of epochs to wait after max has been hit. After this\n",
    "      number of no improvement, training stops.\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self, patience=30):\n",
    "        super(EarlyStoppingAtMaxValAccuracy, self).__init__()\n",
    "        self.patience = patience\n",
    "        # best_weights to store the weights at which the minimum loss occurs.\n",
    "        self.best_weights = None\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        # The number of epoch it has waited when loss is no longer minimum.\n",
    "        self.wait = 0\n",
    "        # The epoch the training stops at.\n",
    "        self.stopped_epoch = 0\n",
    "        # Initialize the best as infinity.\n",
    "        self.best = 0\n",
    "        self.best_epoch = 0\n",
    "        self.best_epoch_loss = np.Infinity\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = logs.get(\"val_accuracy\")\n",
    "        if np.greater(current, self.best):\n",
    "            self.best = current\n",
    "            self.best_epoch = epoch\n",
    "            self.best_epoch_loss = logs.get(\"val_loss\")\n",
    "            self.wait = 0\n",
    "            # Record the best weights if current results is better (less).\n",
    "            self.best_weights = self.model.get_weights()\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                self.stopped_epoch = epoch\n",
    "                self.model.stop_training = True\n",
    "                print(\"Restoring model weights from the end of the best epoch.\")\n",
    "                self.model.set_weights(self.best_weights)\n",
    "\n",
    "        metrics = dict()\n",
    "        metrics[\"best_epoch\"] = self.best_epoch\n",
    "        metrics[\"best_val_accuracy\"] = self.best\n",
    "        metrics[\"best_epoch_loss\"] = self.best_epoch_loss\n",
    "\n",
    "        wandb.log(metrics)\n",
    "\n",
    "\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.stopped_epoch > 0:\n",
    "            print(\"Epoch %05d: early stopping\" % (self.stopped_epoch + 1))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_MODE\"] = \"online\"\n",
    "#os.environ[\"WANDB_MODE\"] = \"offline\"\n",
    "def train_model_wandb(model):\n",
    "\n",
    "        # solve issue from: https://github.com/wandb/wandb/issues/3536\n",
    "        if len(wandb.patched[\"tensorboard\"]) > 0:\n",
    "                wandb.tensorboard.unpatch()\n",
    "                \n",
    "        # Configure Tensorboard root log directory to read the debugging information\n",
    "        wandb.tensorboard.patch(root_logdir=root_logdir)\n",
    "        # wandb.tensorboard.patch(root_logdir=\"wandb.run.dir\")\n",
    "        \n",
    "        # Generate run ids\n",
    "        id = wandb.util.generate_id()\n",
    "\n",
    "        run = wandb.init(\n",
    "                # Set the project where this run will be logged\n",
    "                project=PROJECT, \n",
    "                id = id, \n",
    "                resume=\"allow\",\n",
    "                sync_tensorboard=True\n",
    "                )\n",
    "\n",
    "        # Specify the configuration variables\n",
    "        config = wandb.config\n",
    "        \n",
    "        config.batch_size = BATCH_SIZE\n",
    "        #config.dropout =DROPOUT\n",
    "        config.learn_rate = LR\n",
    "        config.momentum = MOMENTUM\n",
    "        #config.decay = 1e-6\n",
    "        config.epochs = EPOCHS\n",
    "        config.classes = classes\n",
    "        config.id = id\n",
    "        config.seed = seed\n",
    "        config.architecture = model_name\n",
    "        \n",
    "\n",
    "        # enable Tensorflow Debugging\n",
    "        #tf.debugging.experimental.enable_dump_debug_info(\"./logs/debug\", \n",
    "        #        tensor_debug_mode=\"FULL_HEALTH\", circular_buffer_size=-1)\n",
    "\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=LR, momentum=MOMENTUM)\n",
    "        config.optimizer = optimizer._name\n",
    "\n",
    "        model.compile(optimizer=optimizer,\n",
    "                        loss='sparse_categorical_crossentropy',\n",
    "                        metrics=['accuracy'])\n",
    "\n",
    "        logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir= wandb.run.dir, histogram_freq=10, update_freq=\"epoch\") #, profile_batch=\"10, 20\")\n",
    "        #tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir= logdir, histogram_freq=1)\n",
    "        #wandb_callback = WandbCallback()# input_type=\"image\", labels=labels) #, validation_data = val_ds.as_numpy_iterator())\n",
    "\n",
    "        def lr_schedule(epoch):\n",
    "                \"\"\"\n",
    "                Returns a custom learning rate that decreases as epochs progress.\n",
    "                \"\"\"\n",
    "                learning_rate = LR\n",
    "                if epoch > 20:\n",
    "                        learning_rate = 0.0001\n",
    "                tf.summary.scalar('learning rate', data=learning_rate, step=epoch)\n",
    "                return learning_rate\n",
    "\n",
    "        lr_callback = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "        best_model_path = Path(wandb.run.dir).joinpath(f\"best_model\")\n",
    "\n",
    "        checkpoint = WandbModelCheckpoint(best_model_path,\n",
    "                monitor=\"val_accuracy\",\n",
    "                save_best_only=True,\n",
    "                save_freq=\"epoch\")\n",
    "\n",
    "        global early_stopping_patience\n",
    "        early_stopping = EarlyStopping(monitor=\"val_accuracy\", patience=early_stopping_patience)\n",
    "\n",
    "        callbacks =[\n",
    "                tensorboard_callback,\n",
    "                lr_callback,\n",
    "                #wandb_callback,\n",
    "                WandbMetricsLogger(),\n",
    "                checkpoint,\n",
    "                #early_stopping,\n",
    "                EarlyStoppingAtMaxValAccuracy()\n",
    "        ]\n",
    "\n",
    "        history = model.fit(train_ds,\n",
    "                epochs=EPOCHS, \n",
    "                validation_data=val_ds, \n",
    "                callbacks=callbacks\n",
    "        )\n",
    "\n",
    "        #wandb.save(\"last_model.h5\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #best_model = keras.models.load_model(best_model_path) # not needed due to \"restore_best_weights=True\"\n",
    "\n",
    "        y_val_true = np.concatenate([y for x, y in val_ds], axis=0)\n",
    "        y_val_pred = model.predict(val_ds).argmax(axis=1)\n",
    "\n",
    "        y_test_true = np.concatenate([y for x, y in test_ds], axis=0)\n",
    "        y_test_pred = model.predict(test_ds).argmax(axis=1)\n",
    "\n",
    "        results = model.evaluate(test_ds, batch_size=BATCH_SIZE)\n",
    "        print(\"test loss, test acc:\", results)\n",
    "        wandb.log({\n",
    "                \"test_loss\" : results[0],\n",
    "                \"test_accuracy\" : results[1]\n",
    "        })\n",
    "\n",
    "        # log data for the confusion matrix\n",
    "        wandb.log({\"conf_mat\" : wandb.plot.confusion_matrix(probs=None,\n",
    "                        y_true=y_test_true, preds=y_test_pred,\n",
    "                        class_names=labels)})\n",
    "\n",
    "\n",
    "        run.finish()\n",
    "        return history, model, run.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msusbrock\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb9a8b0a5a414ed2815e24355dd82130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.9 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>i:\\tinyml\\tiny_cnn\\wandb\\run-20230121_174458-x3fk9re1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/susbrock/shufflenetv1/runs/x3fk9re1\" target=\"_blank\">cerulean-energy-31</a></strong> to <a href=\"https://wandb.ai/susbrock/shufflenetv1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m When using `save_best_only`, ensure that the `filepath` argument contains formatting placeholders like `{epoch:02d}` or `{batch:02d}`. This ensures correct interpretation of the logged artifacts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " 6/64 [=>............................] - ETA: 5s - loss: 2.2773 - accuracy: 0.2917WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0644s vs `on_train_batch_end` time: 0.0788s). Check your callbacks.\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.5729 - accuracy: 0.7768INFO:tensorflow:Assets written to: i:\\tinyml\\tiny_cnn\\wandb\\run-20230121_174458-x3fk9re1\\files\\best_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: i:\\tinyml\\tiny_cnn\\wandb\\run-20230121_174458-x3fk9re1\\files\\best_model\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (i:\\tinyml\\tiny_cnn\\wandb\\run-20230121_174458-x3fk9re1\\files\\best_model)... Done. 0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 85s 662ms/step - loss: 0.5729 - accuracy: 0.7768 - val_loss: 1.0333 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "63/64 [============================>.] - ETA: 0s - loss: 0.1523 - accuracy: 0.9405"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (i:\\tinyml\\tiny_cnn\\wandb\\run-20230121_174458-x3fk9re1\\files\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 4s 65ms/step - loss: 0.1534 - accuracy: 0.9401 - val_loss: 1.0218 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "63/64 [============================>.] - ETA: 0s - loss: 0.1092 - accuracy: 0.9633"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (i:\\tinyml\\tiny_cnn\\wandb\\run-20230121_174458-x3fk9re1\\files\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 5s 72ms/step - loss: 0.1091 - accuracy: 0.9634 - val_loss: 1.0242 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "63/64 [============================>.] - ETA: 0s - loss: 0.0714 - accuracy: 0.9777"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (i:\\tinyml\\tiny_cnn\\wandb\\run-20230121_174458-x3fk9re1\\files\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 4s 65ms/step - loss: 0.0716 - accuracy: 0.9772 - val_loss: 0.9576 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "63/64 [============================>.] - ETA: 0s - loss: 0.0644 - accuracy: 0.9772INFO:tensorflow:Assets written to: i:\\tinyml\\tiny_cnn\\wandb\\run-20230121_174458-x3fk9re1\\files\\best_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: i:\\tinyml\\tiny_cnn\\wandb\\run-20230121_174458-x3fk9re1\\files\\best_model\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (i:\\tinyml\\tiny_cnn\\wandb\\run-20230121_174458-x3fk9re1\\files\\best_model)... Done. 0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 32s 511ms/step - loss: 0.0646 - accuracy: 0.9772 - val_loss: 0.9144 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "63/64 [============================>.] - ETA: 0s - loss: 0.0573 - accuracy: 0.9777INFO:tensorflow:Assets written to: i:\\tinyml\\tiny_cnn\\wandb\\run-20230121_174458-x3fk9re1\\files\\best_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: i:\\tinyml\\tiny_cnn\\wandb\\run-20230121_174458-x3fk9re1\\files\\best_model\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (i:\\tinyml\\tiny_cnn\\wandb\\run-20230121_174458-x3fk9re1\\files\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 26s 410ms/step - loss: 0.0624 - accuracy: 0.9767 - val_loss: 0.7414 - val_accuracy: 0.6032 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "63/64 [============================>.] - ETA: 0s - loss: 0.1022 - accuracy: 0.9593INFO:tensorflow:Assets written to: i:\\tinyml\\tiny_cnn\\wandb\\run-20230121_174458-x3fk9re1\\files\\best_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: i:\\tinyml\\tiny_cnn\\wandb\\run-20230121_174458-x3fk9re1\\files\\best_model\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (i:\\tinyml\\tiny_cnn\\wandb\\run-20230121_174458-x3fk9re1\\files\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 25s 400ms/step - loss: 0.1028 - accuracy: 0.9589 - val_loss: 0.6140 - val_accuracy: 0.7222 - lr: 0.0010\n",
      "Epoch 8/30\n",
      "63/64 [============================>.] - ETA: 0s - loss: 0.0443 - accuracy: 0.9896INFO:tensorflow:Assets written to: i:\\tinyml\\tiny_cnn\\wandb\\run-20230121_174458-x3fk9re1\\files\\best_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: i:\\tinyml\\tiny_cnn\\wandb\\run-20230121_174458-x3fk9re1\\files\\best_model\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (i:\\tinyml\\tiny_cnn\\wandb\\run-20230121_174458-x3fk9re1\\files\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 25s 397ms/step - loss: 0.0451 - accuracy: 0.9891 - val_loss: 0.2994 - val_accuracy: 0.8968 - lr: 0.0010\n",
      "Epoch 9/30\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.0931 - accuracy: 0.9654INFO:tensorflow:Assets written to: i:\\tinyml\\tiny_cnn\\wandb\\run-20230121_174458-x3fk9re1\\files\\best_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: i:\\tinyml\\tiny_cnn\\wandb\\run-20230121_174458-x3fk9re1\\files\\best_model\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (i:\\tinyml\\tiny_cnn\\wandb\\run-20230121_174458-x3fk9re1\\files\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 25s 393ms/step - loss: 0.0931 - accuracy: 0.9654 - val_loss: 0.1993 - val_accuracy: 0.9325 - lr: 0.0010\n",
      "Epoch 10/30\n",
      "63/64 [============================>.] - ETA: 0s - loss: 0.0361 - accuracy: 0.9871INFO:tensorflow:Assets written to: i:\\tinyml\\tiny_cnn\\wandb\\run-20230121_174458-x3fk9re1\\files\\best_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: i:\\tinyml\\tiny_cnn\\wandb\\run-20230121_174458-x3fk9re1\\files\\best_model\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (i:\\tinyml\\tiny_cnn\\wandb\\run-20230121_174458-x3fk9re1\\files\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 25s 400ms/step - loss: 0.0360 - accuracy: 0.9871 - val_loss: 0.0594 - val_accuracy: 0.9722 - lr: 0.0010\n",
      "Epoch 11/30\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.0276 - accuracy: 0.9911INFO:tensorflow:Assets written to: i:\\tinyml\\tiny_cnn\\wandb\\run-20230121_174458-x3fk9re1\\files\\best_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: i:\\tinyml\\tiny_cnn\\wandb\\run-20230121_174458-x3fk9re1\\files\\best_model\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (i:\\tinyml\\tiny_cnn\\wandb\\run-20230121_174458-x3fk9re1\\files\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 28s 439ms/step - loss: 0.0276 - accuracy: 0.9911 - val_loss: 0.0666 - val_accuracy: 0.9762 - lr: 0.0010\n",
      "Epoch 12/30\n",
      "63/64 [============================>.] - ETA: 0s - loss: 0.0335 - accuracy: 0.9866INFO:tensorflow:Assets written to: i:\\tinyml\\tiny_cnn\\wandb\\run-20230121_174458-x3fk9re1\\files\\best_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: i:\\tinyml\\tiny_cnn\\wandb\\run-20230121_174458-x3fk9re1\\files\\best_model\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (i:\\tinyml\\tiny_cnn\\wandb\\run-20230121_174458-x3fk9re1\\files\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 25s 396ms/step - loss: 0.0337 - accuracy: 0.9866 - val_loss: 0.0623 - val_accuracy: 0.9802 - lr: 0.0010\n",
      "Epoch 13/30\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.0266 - accuracy: 0.9916"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (i:\\tinyml\\tiny_cnn\\wandb\\run-20230121_174458-x3fk9re1\\files\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 4s 65ms/step - loss: 0.0266 - accuracy: 0.9916 - val_loss: 0.5465 - val_accuracy: 0.7937 - lr: 0.0010\n",
      "Epoch 14/30\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.0153 - accuracy: 0.9951INFO:tensorflow:Assets written to: i:\\tinyml\\tiny_cnn\\wandb\\run-20230121_174458-x3fk9re1\\files\\best_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: i:\\tinyml\\tiny_cnn\\wandb\\run-20230121_174458-x3fk9re1\\files\\best_model\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (i:\\tinyml\\tiny_cnn\\wandb\\run-20230121_174458-x3fk9re1\\files\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 25s 400ms/step - loss: 0.0153 - accuracy: 0.9951 - val_loss: 0.0167 - val_accuracy: 0.9960 - lr: 0.0010\n",
      "Epoch 15/30\n",
      "63/64 [============================>.] - ETA: 0s - loss: 0.0351 - accuracy: 0.9886"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (i:\\tinyml\\tiny_cnn\\wandb\\run-20230121_174458-x3fk9re1\\files\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 5s 83ms/step - loss: 0.0350 - accuracy: 0.9886 - val_loss: 0.2902 - val_accuracy: 0.8968 - lr: 0.0010\n",
      "Epoch 16/30\n",
      "63/64 [============================>.] - ETA: 0s - loss: 0.0223 - accuracy: 0.9931"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (i:\\tinyml\\tiny_cnn\\wandb\\run-20230121_174458-x3fk9re1\\files\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 5s 82ms/step - loss: 0.0236 - accuracy: 0.9921 - val_loss: 0.0848 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 17/30\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.0490 - accuracy: 0.9832"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (i:\\tinyml\\tiny_cnn\\wandb\\run-20230121_174458-x3fk9re1\\files\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 5s 69ms/step - loss: 0.0490 - accuracy: 0.9832 - val_loss: 1.0592 - val_accuracy: 0.7222 - lr: 0.0010\n",
      "Epoch 18/30\n",
      "63/64 [============================>.] - ETA: 0s - loss: 0.0387 - accuracy: 0.9861"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (i:\\tinyml\\tiny_cnn\\wandb\\run-20230121_174458-x3fk9re1\\files\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 5s 74ms/step - loss: 0.0386 - accuracy: 0.9861 - val_loss: 0.8502 - val_accuracy: 0.7619 - lr: 0.0010\n",
      "Epoch 19/30\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.0195 - accuracy: 0.9946"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (i:\\tinyml\\tiny_cnn\\wandb\\run-20230121_174458-x3fk9re1\\files\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 5s 70ms/step - loss: 0.0195 - accuracy: 0.9946 - val_loss: 0.3871 - val_accuracy: 0.8532 - lr: 0.0010\n",
      "Epoch 20/30\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.0168 - accuracy: 0.9946"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (i:\\tinyml\\tiny_cnn\\wandb\\run-20230121_174458-x3fk9re1\\files\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 5s 70ms/step - loss: 0.0168 - accuracy: 0.9946 - val_loss: 0.0681 - val_accuracy: 0.9683 - lr: 0.0010\n",
      "Epoch 21/30\n",
      "63/64 [============================>.] - ETA: 0s - loss: 0.0350 - accuracy: 0.9891"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (i:\\tinyml\\tiny_cnn\\wandb\\run-20230121_174458-x3fk9re1\\files\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 7s 110ms/step - loss: 0.0358 - accuracy: 0.9886 - val_loss: 1.3353 - val_accuracy: 0.7262 - lr: 0.0010\n",
      "Epoch 22/30\n",
      "63/64 [============================>.] - ETA: 0s - loss: 0.0909 - accuracy: 0.9722"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (i:\\tinyml\\tiny_cnn\\wandb\\run-20230121_174458-x3fk9re1\\files\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 5s 77ms/step - loss: 0.0907 - accuracy: 0.9723 - val_loss: 0.1063 - val_accuracy: 0.9603 - lr: 1.0000e-04\n",
      "Epoch 23/30\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.0290 - accuracy: 0.9906"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (i:\\tinyml\\tiny_cnn\\wandb\\run-20230121_174458-x3fk9re1\\files\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 5s 75ms/step - loss: 0.0290 - accuracy: 0.9906 - val_loss: 0.1045 - val_accuracy: 0.9563 - lr: 1.0000e-04\n",
      "Epoch 24/30\n",
      "63/64 [============================>.] - ETA: 0s - loss: 0.0280 - accuracy: 0.9906"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (i:\\tinyml\\tiny_cnn\\wandb\\run-20230121_174458-x3fk9re1\\files\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 4s 65ms/step - loss: 0.0283 - accuracy: 0.9906 - val_loss: 0.0824 - val_accuracy: 0.9722 - lr: 1.0000e-04\n",
      "Epoch 25/30\n",
      "63/64 [============================>.] - ETA: 0s - loss: 0.0208 - accuracy: 0.9916"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (i:\\tinyml\\tiny_cnn\\wandb\\run-20230121_174458-x3fk9re1\\files\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 4s 63ms/step - loss: 0.0210 - accuracy: 0.9916 - val_loss: 0.0518 - val_accuracy: 0.9802 - lr: 1.0000e-04\n",
      "Epoch 26/30\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.0243 - accuracy: 0.9921"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (i:\\tinyml\\tiny_cnn\\wandb\\run-20230121_174458-x3fk9re1\\files\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 4s 65ms/step - loss: 0.0243 - accuracy: 0.9921 - val_loss: 0.0295 - val_accuracy: 0.9921 - lr: 1.0000e-04\n",
      "Epoch 27/30\n",
      "63/64 [============================>.] - ETA: 0s - loss: 0.0150 - accuracy: 0.9955"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (i:\\tinyml\\tiny_cnn\\wandb\\run-20230121_174458-x3fk9re1\\files\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 4s 65ms/step - loss: 0.0178 - accuracy: 0.9946 - val_loss: 0.0304 - val_accuracy: 0.9921 - lr: 1.0000e-04\n",
      "Epoch 28/30\n",
      "63/64 [============================>.] - ETA: 0s - loss: 0.0180 - accuracy: 0.9955"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (i:\\tinyml\\tiny_cnn\\wandb\\run-20230121_174458-x3fk9re1\\files\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 4s 64ms/step - loss: 0.0180 - accuracy: 0.9955 - val_loss: 0.0481 - val_accuracy: 0.9841 - lr: 1.0000e-04\n",
      "Epoch 29/30\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.0165 - accuracy: 0.9960"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (i:\\tinyml\\tiny_cnn\\wandb\\run-20230121_174458-x3fk9re1\\files\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 5s 72ms/step - loss: 0.0165 - accuracy: 0.9960 - val_loss: 0.0432 - val_accuracy: 0.9881 - lr: 1.0000e-04\n",
      "Epoch 30/30\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.0116 - accuracy: 0.9975"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (i:\\tinyml\\tiny_cnn\\wandb\\run-20230121_174458-x3fk9re1\\files\\best_model)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 5s 73ms/step - loss: 0.0116 - accuracy: 0.9975 - val_loss: 0.0323 - val_accuracy: 0.9921 - lr: 1.0000e-04\n",
      "8/8 [==============================] - 2s 22ms/step\n",
      "255/255 [==============================] - 5s 15ms/step\n",
      "255/255 [==============================] - 5s 19ms/step - loss: 0.0541 - accuracy: 0.9804\n",
      "test loss, test acc: [0.05414583161473274, 0.9803921580314636]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1abbab93538462bbcc264f419df81c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='68.542 MB of 68.542 MB uploaded (0.380 MB deduped)\\r'), FloatProgress(value=1.0, mâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>â–â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>best_epoch</td><td>â–â–â–â–â–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>best_epoch_loss</td><td>â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–†â–…â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>best_val_accuracy</td><td>â–â–â–â–â–‚â–ƒâ–…â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>epoch</td><td>â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>loss</td><td>â–ˆâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–</td></tr><tr><td>lr</td><td>â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–â–â–â–â–â–â–â–</td></tr><tr><td>test_accuracy</td><td>â–</td></tr><tr><td>test_loss</td><td>â–</td></tr><tr><td>val_accuracy</td><td>â–â–â–â–â–‚â–ƒâ–…â–‡â–‡â–ˆâ–ˆâ–ˆâ–…â–ˆâ–‡â–ˆâ–…â–…â–†â–ˆâ–…â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>val_loss</td><td>â–†â–†â–†â–†â–†â–…â–„â–ƒâ–‚â–â–â–â–„â–â–‚â–â–‡â–…â–ƒâ–â–ˆâ–â–â–â–â–â–â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.99753</td></tr><tr><td>best_epoch</td><td>13</td></tr><tr><td>best_epoch_loss</td><td>0.01674</td></tr><tr><td>best_val_accuracy</td><td>0.99603</td></tr><tr><td>epoch</td><td>29</td></tr><tr><td>loss</td><td>0.01161</td></tr><tr><td>lr</td><td>0.0001</td></tr><tr><td>test_accuracy</td><td>0.98039</td></tr><tr><td>test_loss</td><td>0.05415</td></tr><tr><td>val_accuracy</td><td>0.99206</td></tr><tr><td>val_loss</td><td>0.03226</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">cerulean-energy-31</strong>: <a href=\"https://wandb.ai/susbrock/shufflenetv1/runs/x3fk9re1\" target=\"_blank\">https://wandb.ai/susbrock/shufflenetv1/runs/x3fk9re1</a><br/>Synced 7 W&B file(s), 1 media file(s), 32 artifact file(s) and 7 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230121_174458-x3fk9re1\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()\n",
    "history, model, run_id = train_model_wandb(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wandb sync i:\\tinyml\\tiny_cnn\\wandb\\offline-run-20221227_091238-1vzrst0a"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversion to TFLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Susanne\\AppData\\Local\\Temp\\tmpv4x7m00v\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Susanne\\AppData\\Local\\Temp\\tmpv4x7m00v\\assets\n"
     ]
    }
   ],
   "source": [
    "# Convert the model to the TensorFlow Lite format without quantization\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "# converter = tf.lite.TFLiteConverter.from_saved_model(models_path)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open(models_tflite_trained_path, \"wb\") as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversion to TFLite with INT8 quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_iter = test_ds.as_numpy_iterator()\n",
    "\n",
    "# for i in range(1):\n",
    "#     sample = next(sample_iter)[0]\n",
    "# print(\"Number of samples: {}\".format(sample.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def representative_data_gen():\n",
    "#     for i in range(100):\n",
    "#       yield([test_ds[i].reshape(1, 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# representative_data_gen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Susanne\\AppData\\Local\\Temp\\tmpt3eh5j5c\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Miniconda\\envs\\tiny_cnn_3\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py:766: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n"
     ]
    }
   ],
   "source": [
    "repr_ds = test_ds.unbatch()\n",
    "\n",
    "def representative_data_gen():\n",
    "  for i_value, o_value in repr_ds.batch(1).take(48):\n",
    "    yield [i_value]\n",
    "\n",
    "\n",
    "# def representative_data_gen():\n",
    "#     for i in range(100):\n",
    "#       yield([test_ds[i].reshape(1, 1)])\n",
    "\n",
    "# def representative_data_gen():\n",
    "#   for data in test_ds.batch(1).take(100):\n",
    "#     yield [tf.dtypes.cast(data, tf.float32)]\n",
    "\n",
    "# def representative_data_gen():\n",
    "#     for i in range(BATCH_SIZE):\n",
    "#         yield([np.expand_dims(sample[i], axis=0)])\n",
    "\n",
    "# testing dataset\n",
    "# def representative_data_gen():\n",
    "#     for _ in range(100):\n",
    "#       data = np.random.rand(1, 96, 96, 3)\n",
    "#       yield [data.astype(np.float32)]\n",
    " \n",
    "    \n",
    "converter_opt = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "\n",
    "# set the optimization flag\n",
    "converter_opt.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "# enforce integer only quantization\n",
    "converter_opt.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter_opt.inference_input_type = tf.uint8\n",
    "converter_opt.inference_output_type = tf.uint8\n",
    "\n",
    "# provide a representative dataset for quantization\n",
    "#converter_opt.representative_dataset = tf.lite.RepresentativeDataset(representative_data_gen)\n",
    "converter_opt.representative_dataset = representative_data_gen\n",
    "\n",
    "tflite_model_opt = converter_opt.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open(models_tflite_opt_path, 'wb') as f:\n",
    "  f.write(tflite_model_opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiling model in simulator ...\n",
      "Using Tensorflow-Lite Micro version: b13b48c (2022-06-08)\n",
      "Searching for optimal runtime memory size ...\n",
      "Determined optimal runtime memory size to be 143360\n"
     ]
    }
   ],
   "source": [
    "# MLTK profile model reads the mode from a path - only works for MLTK models! / Model must be trained first\n",
    "\n",
    "profiling_results = profile_model(str(models_tflite_opt_path), accelerator=None, build=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the TensorFlot Lite models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset element_spec=(TensorSpec(shape=(None, 96, 96, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_image = test_ds.take(1)\n",
    "test_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[-0.33998156, -0.33213842, -0.35174626],\n",
       "         [-0.35225183, -0.3444087 , -0.36401653],\n",
       "         [-0.36940867, -0.36156553, -0.38117337],\n",
       "         ...,\n",
       "         [-0.3047487 , -0.30222118, -0.33359373],\n",
       "         [-0.2494638 , -0.24554223, -0.27691478],\n",
       "         [-0.3516084 , -0.34768683, -0.37905937]],\n",
       "\n",
       "        [[-0.42683822, -0.42683822, -0.43468136],\n",
       "         [-0.3713541 , -0.3713541 , -0.37919724],\n",
       "         [-0.36905634, -0.36513478, -0.38474262],\n",
       "         ...,\n",
       "         [-0.30284923, -0.30284923, -0.34206492],\n",
       "         [-0.29128367, -0.28344053, -0.32657778],\n",
       "         [-0.29404104, -0.2861979 , -0.32933515]],\n",
       "\n",
       "        [[-0.51554835, -0.51554835, -0.5233915 ],\n",
       "         [-0.41674322, -0.41674322, -0.42458636],\n",
       "         [-0.3831188 , -0.3831188 , -0.39096195],\n",
       "         ...,\n",
       "         [-0.3132046 , -0.3132046 , -0.35242033],\n",
       "         [-0.309375  , -0.3039828 , -0.34589458],\n",
       "         [-0.22161454, -0.21859676, -0.2530024 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.7803155 , -0.7920803 , -0.8273744 ],\n",
       "         [-0.819087  , -0.8308517 , -0.86614585],\n",
       "         [-0.8183823 , -0.830147  , -0.8654412 ],\n",
       "         ...,\n",
       "         [-0.28642768, -0.29819238, -0.3334865 ],\n",
       "         [-0.29486823, -0.30663294, -0.34192705],\n",
       "         [-0.28938413, -0.28938413, -0.3285998 ]],\n",
       "\n",
       "        [[-0.7735294 , -0.7852941 , -0.82058823],\n",
       "         [-0.8007353 , -0.8125    , -0.8477941 ],\n",
       "         [-0.78943014, -0.80119485, -0.83648896],\n",
       "         ...,\n",
       "         [-0.5060355 , -0.5178002 , -0.5530943 ],\n",
       "         [-0.3642463 , -0.376011  , -0.41130513],\n",
       "         [-0.21966904, -0.21966904, -0.2588848 ]],\n",
       "\n",
       "        [[-0.77282476, -0.78458947, -0.8198836 ],\n",
       "         [-0.76387864, -0.77564335, -0.8109375 ],\n",
       "         [-0.76741725, -0.77918196, -0.8144761 ],\n",
       "         ...,\n",
       "         [-0.45900732, -0.47077203, -0.50606614],\n",
       "         [-0.43328732, -0.44505203, -0.48034614],\n",
       "         [-0.28388476, -0.28388476, -0.32310045]]]], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test_gen = test_ds.unbatch().batch(1)\n",
    "test_gen = test_ds.as_numpy_iterator()\n",
    "#test_gen = test_gen.next() \n",
    "#test_image = test_gen.take(1)\n",
    "test_image = next(test_gen)[0]\n",
    "test_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_images = len(list(test_gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 96, 96, 3)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tflite_predict(model_path, test_image):\n",
    "    # Initialize the interpreter\n",
    "    interpreter = tf.lite.Interpreter(model_path=str(model_path))\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    input_details = interpreter.get_input_details()[0]\n",
    "    output_details = interpreter.get_output_details()[0]\n",
    "\n",
    "    # Check if the input type is quantized, then rescale input data to uint8\n",
    "    if input_details['dtype'] == np.uint8:  # was np.uint8\n",
    "        input_scale, input_zero_point = input_details[\"quantization\"]\n",
    "        test_image = test_image / input_scale + input_zero_point\n",
    "        \n",
    "    test_image = test_image.astype(input_details[\"dtype\"])\n",
    "    interpreter.set_tensor(input_details[\"index\"], test_image)\n",
    "    #interpreter.set_tensor(input_details[\"index\"], np.expand_dims(test_image[0], axis=0)) # only needed when input shape (96, 96, 3)\n",
    "    interpreter.invoke()\n",
    "    output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
    "    prediction = output.argmax()\n",
    "    print(f\"Prediction: Class {prediction} derived from {output}\")\n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Class 0 derived from [255   0   0]\n"
     ]
    }
   ],
   "source": [
    "tflite_result = tflite_predict(models_tflite_opt_path, test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tflite_predict_on_dataset(model_path, dataset):\n",
    "    # find length of dataset\n",
    "    test_gen = dataset.as_numpy_iterator()\n",
    "    num_images = len(list(test_gen))\n",
    "\n",
    "    predictions = []\n",
    "    y_trues = []\n",
    "\n",
    "    test_gen = dataset.as_numpy_iterator()\n",
    "    accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "    \n",
    "    # iterate over the complete test_set\n",
    "    for i in range(num_images):\n",
    "        test_image, y_true = next(test_gen)\n",
    "        prediction = tflite_predict(model_path, test_image)\n",
    "        predictions.append(prediction)\n",
    "        y_trues.append(y_true[0])\n",
    "        accuracy.update_state(y_true, prediction)\n",
    "        print(f\"{i}, {test_image.shape} - true label: {y_true[0]} vs {tflite_result}\")\n",
    "\n",
    "    #accuracy = (np.sum(predictions == y_trues) * 100) / num_images\n",
    "    print(f\"Accuracy: {accuracy.result()} - (Number of test samples: {num_images})\")\n",
    "    return predictions, y_trues    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Class 0 derived from [255   0   0]\n",
      "0, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   0]\n",
      "1, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   1]\n",
      "2, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   0]\n",
      "3, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [248   0   8]\n",
      "4, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   0]\n",
      "5, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   0]\n",
      "6, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [240   0  16]\n",
      "7, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [245   0  11]\n",
      "8, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   1]\n",
      "9, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   0]\n",
      "10, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   0]\n",
      "11, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   0]\n",
      "12, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   0]\n",
      "13, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   0]\n",
      "14, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   1]\n",
      "15, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   0]\n",
      "16, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   0]\n",
      "17, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [240   0  16]\n",
      "18, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   1]\n",
      "19, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   0]\n",
      "20, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   0]\n",
      "21, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   0]\n",
      "22, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   0]\n",
      "23, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [213   0  43]\n",
      "24, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   0]\n",
      "25, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   0]\n",
      "26, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   1]\n",
      "27, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   0]\n",
      "28, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   0]\n",
      "29, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   0]\n",
      "30, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   0]\n",
      "31, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   0]\n",
      "32, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   0]\n",
      "33, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   1]\n",
      "34, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   0]\n",
      "35, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   0]\n",
      "36, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   0]\n",
      "37, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   0]\n",
      "38, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   0]\n",
      "39, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [252   0   4]\n",
      "40, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 2 derived from [ 91   0 165]\n",
      "41, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   0]\n",
      "42, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [253   0   3]\n",
      "43, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [204  20  32]\n",
      "44, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [250   0   6]\n",
      "45, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   0]\n",
      "46, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   1]\n",
      "47, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [244   0  12]\n",
      "48, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   0]\n",
      "49, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   0]\n",
      "50, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   1]\n",
      "51, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [221   0  35]\n",
      "52, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   0]\n",
      "53, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [246   0  10]\n",
      "54, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   1]\n",
      "55, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   0]\n",
      "56, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [230   0  26]\n",
      "57, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   0]\n",
      "58, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   0]\n",
      "59, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   0]\n",
      "60, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [253   0   3]\n",
      "61, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [254   2   0]\n",
      "62, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   0]\n",
      "63, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   0]\n",
      "64, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   0]\n",
      "65, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   0]\n",
      "66, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   0]\n",
      "67, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 2 derived from [ 95   0 161]\n",
      "68, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   0]\n",
      "69, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [254   0   2]\n",
      "70, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   0]\n",
      "71, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 2 derived from [ 29   0 227]\n",
      "72, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   0]\n",
      "73, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   0]\n",
      "74, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   0]\n",
      "75, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [252   0   3]\n",
      "76, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   0]\n",
      "77, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   0]\n",
      "78, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   0]\n",
      "79, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   0]\n",
      "80, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [210   0  46]\n",
      "81, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   0]\n",
      "82, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [250   0   6]\n",
      "83, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   0]\n",
      "84, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [252   0   4]\n",
      "85, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   0]\n",
      "86, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   0]\n",
      "87, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [251   0   5]\n",
      "88, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   0]\n",
      "89, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   0]\n",
      "90, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   0]\n",
      "91, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   0]\n",
      "92, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   0]\n",
      "93, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   1]\n",
      "94, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 0 derived from [255   0   0]\n",
      "95, (1, 96, 96, 3) - true label: 0 vs 0\n",
      "Prediction: Class 1 derived from [  0 255   0]\n",
      "96, (1, 96, 96, 3) - true label: 1 vs 0\n",
      "Prediction: Class 1 derived from [  0 255   0]\n",
      "97, (1, 96, 96, 3) - true label: 1 vs 0\n",
      "Prediction: Class 1 derived from [  0 255   0]\n",
      "98, (1, 96, 96, 3) - true label: 1 vs 0\n",
      "Prediction: Class 1 derived from [  0 255   0]\n",
      "99, (1, 96, 96, 3) - true label: 1 vs 0\n",
      "Prediction: Class 1 derived from [  0 255   0]\n",
      "100, (1, 96, 96, 3) - true label: 1 vs 0\n",
      "Prediction: Class 1 derived from [  0 255   0]\n",
      "101, (1, 96, 96, 3) - true label: 1 vs 0\n",
      "Prediction: Class 1 derived from [  0 255   0]\n",
      "102, (1, 96, 96, 3) - true label: 1 vs 0\n",
      "Prediction: Class 1 derived from [  0 255   0]\n",
      "103, (1, 96, 96, 3) - true label: 1 vs 0\n",
      "Prediction: Class 1 derived from [  0 255   0]\n",
      "104, (1, 96, 96, 3) - true label: 1 vs 0\n",
      "Prediction: Class 1 derived from [  0 255   0]\n",
      "105, (1, 96, 96, 3) - true label: 1 vs 0\n",
      "Prediction: Class 1 derived from [  0 255   0]\n",
      "106, (1, 96, 96, 3) - true label: 1 vs 0\n",
      "Prediction: Class 1 derived from [  0 255   0]\n",
      "107, (1, 96, 96, 3) - true label: 1 vs 0\n",
      "Prediction: Class 1 derived from [  0 255   0]\n",
      "108, (1, 96, 96, 3) - true label: 1 vs 0\n",
      "Prediction: Class 1 derived from [  0 255   0]\n",
      "109, (1, 96, 96, 3) - true label: 1 vs 0\n",
      "Prediction: Class 1 derived from [  0 255   0]\n",
      "110, (1, 96, 96, 3) - true label: 1 vs 0\n",
      "Prediction: Class 1 derived from [  0 255   0]\n",
      "111, (1, 96, 96, 3) - true label: 1 vs 0\n",
      "Prediction: Class 1 derived from [  0 255   0]\n",
      "112, (1, 96, 96, 3) - true label: 1 vs 0\n",
      "Prediction: Class 1 derived from [  0 255   0]\n",
      "113, (1, 96, 96, 3) - true label: 1 vs 0\n",
      "Prediction: Class 1 derived from [  0 255   0]\n",
      "114, (1, 96, 96, 3) - true label: 1 vs 0\n",
      "Prediction: Class 1 derived from [  0 255   0]\n",
      "115, (1, 96, 96, 3) - true label: 1 vs 0\n",
      "Prediction: Class 1 derived from [  0 255   0]\n",
      "116, (1, 96, 96, 3) - true label: 1 vs 0\n",
      "Prediction: Class 1 derived from [  0 255   0]\n",
      "117, (1, 96, 96, 3) - true label: 1 vs 0\n",
      "Prediction: Class 1 derived from [  0 255   0]\n",
      "118, (1, 96, 96, 3) - true label: 1 vs 0\n",
      "Prediction: Class 1 derived from [  0 255   0]\n",
      "119, (1, 96, 96, 3) - true label: 1 vs 0\n",
      "Prediction: Class 1 derived from [  0 255   0]\n",
      "120, (1, 96, 96, 3) - true label: 1 vs 0\n",
      "Prediction: Class 1 derived from [  0 255   0]\n",
      "121, (1, 96, 96, 3) - true label: 1 vs 0\n",
      "Prediction: Class 1 derived from [  0 255   0]\n",
      "122, (1, 96, 96, 3) - true label: 1 vs 0\n",
      "Prediction: Class 1 derived from [  0 255   0]\n",
      "123, (1, 96, 96, 3) - true label: 1 vs 0\n",
      "Prediction: Class 1 derived from [  0 255   0]\n",
      "124, (1, 96, 96, 3) - true label: 1 vs 0\n",
      "Prediction: Class 1 derived from [  0 255   0]\n",
      "125, (1, 96, 96, 3) - true label: 1 vs 0\n",
      "Prediction: Class 1 derived from [  0 255   0]\n",
      "126, (1, 96, 96, 3) - true label: 1 vs 0\n",
      "Prediction: Class 1 derived from [  0 255   0]\n",
      "127, (1, 96, 96, 3) - true label: 1 vs 0\n",
      "Prediction: Class 1 derived from [  0 255   0]\n",
      "128, (1, 96, 96, 3) - true label: 1 vs 0\n",
      "Prediction: Class 1 derived from [  0 255   0]\n",
      "129, (1, 96, 96, 3) - true label: 1 vs 0\n",
      "Prediction: Class 1 derived from [  0 255   0]\n",
      "130, (1, 96, 96, 3) - true label: 1 vs 0\n",
      "Prediction: Class 1 derived from [  0 255   0]\n",
      "131, (1, 96, 96, 3) - true label: 1 vs 0\n",
      "Prediction: Class 1 derived from [  0 255   0]\n",
      "132, (1, 96, 96, 3) - true label: 1 vs 0\n",
      "Prediction: Class 1 derived from [  0 255   0]\n",
      "133, (1, 96, 96, 3) - true label: 1 vs 0\n",
      "Prediction: Class 1 derived from [  0 255   0]\n",
      "134, (1, 96, 96, 3) - true label: 1 vs 0\n",
      "Prediction: Class 1 derived from [  0 255   0]\n",
      "135, (1, 96, 96, 3) - true label: 1 vs 0\n",
      "Prediction: Class 1 derived from [  0 255   0]\n",
      "136, (1, 96, 96, 3) - true label: 1 vs 0\n",
      "Prediction: Class 1 derived from [  0 255   0]\n",
      "137, (1, 96, 96, 3) - true label: 1 vs 0\n",
      "Prediction: Class 1 derived from [  0 255   0]\n",
      "138, (1, 96, 96, 3) - true label: 1 vs 0\n",
      "Prediction: Class 1 derived from [  0 255   0]\n",
      "139, (1, 96, 96, 3) - true label: 1 vs 0\n",
      "Prediction: Class 1 derived from [  0 255   0]\n",
      "140, (1, 96, 96, 3) - true label: 1 vs 0\n",
      "Prediction: Class 1 derived from [  0 255   0]\n",
      "141, (1, 96, 96, 3) - true label: 1 vs 0\n",
      "Prediction: Class 2 derived from [ 16   0 240]\n",
      "142, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "143, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "144, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "145, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "146, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "147, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "148, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "149, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "150, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "151, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "152, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "153, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  1   0 255]\n",
      "154, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "155, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "156, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "157, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [ 83   0 173]\n",
      "158, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "159, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 0 derived from [190   0  66]\n",
      "160, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "161, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "162, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "163, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "164, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "165, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "166, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  3   0 253]\n",
      "167, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "168, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "169, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  2   0 253]\n",
      "170, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "171, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "172, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "173, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "174, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "175, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "176, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "177, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "178, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  1   0 255]\n",
      "179, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "180, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "181, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "182, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "183, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "184, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "185, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "186, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "187, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "188, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "189, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  1   0 255]\n",
      "190, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "191, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "192, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "193, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "194, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "195, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "196, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "197, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "198, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [ 27   0 229]\n",
      "199, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "200, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "201, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  1   0 255]\n",
      "202, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "203, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "204, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "205, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "206, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "207, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "208, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "209, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "210, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "211, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "212, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "213, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "214, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "215, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "216, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "217, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "218, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "219, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "220, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "221, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "222, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "223, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [ 15   0 241]\n",
      "224, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 0 derived from [240   0  16]\n",
      "225, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "226, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "227, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "228, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "229, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "230, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "231, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "232, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "233, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "234, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  9   0 247]\n",
      "235, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  1   0 255]\n",
      "236, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "237, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  1   0 255]\n",
      "238, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "239, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "240, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  5   0 251]\n",
      "241, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  7   1 248]\n",
      "242, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "243, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "244, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "245, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "246, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 0 derived from [235   0  21]\n",
      "247, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "248, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  1   0 255]\n",
      "249, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [ 87   0 169]\n",
      "250, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "251, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  0   0 255]\n",
      "252, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 2 derived from [  4   0 252]\n",
      "253, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Prediction: Class 0 derived from [251   0   5]\n",
      "254, (1, 96, 96, 3) - true label: 2 vs 0\n",
      "Accuracy: 0.0 - (Number of test samples: 255)\n"
     ]
    }
   ],
   "source": [
    "preds, trues = tflite_predict_on_dataset(models_tflite_opt_path, test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = (np.sum(preds == trues) * 100) / num_test_images\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Helper function to evaluate a TFLite model on all images\n",
    "# def evaluate_model(tflite_file, model_type):\n",
    "#   global test_images\n",
    "#   global test_labels\n",
    "\n",
    "#   test_image_indices = range(test_images.shape[0]) # TODO: is this correct?\n",
    "#   predictions = tflite_predict(tflite_file, test_image_indices)\n",
    "\n",
    "#   accuracy = (np.sum(test_labels== predictions) * 100) / len(test_images)\n",
    "\n",
    "#   print('%s model accuracy is %.4f%% (Number of test samples=%d)' % (\n",
    "#       model_type, accuracy, len(test_images)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # code copied from: https://www.tensorflow.org/lite/performance/post_training_integer_quant\n",
    "\n",
    "# # Helper function to run inference on a TFLite model\n",
    "# def run_tflite_model(tflite_file, test_image_indices):\n",
    "#   global test_images\n",
    "\n",
    "#   # Initialize the interpreter\n",
    "#   interpreter = tf.lite.Interpreter(model_path=str(tflite_file))\n",
    "#   interpreter.allocate_tensors()\n",
    "\n",
    "#   input_details = interpreter.get_input_details()[0]\n",
    "#   output_details = interpreter.get_output_details()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "  # predictions = np.zeros((len(test_image_indices),), dtype=int)\n",
    "  # for i, test_image_index in enumerate(test_image_indices):\n",
    "  #   test_image = test_images[test_image_index]\n",
    "  #   test_label = test_labels[test_image_index]\n",
    "\n",
    "  #   # Check if the input type is quantized, then rescale input data to uint8\n",
    "  #   if input_details['dtype'] == np.uint8:\n",
    "  #     input_scale, input_zero_point = input_details[\"quantization\"]\n",
    "  #     test_image = test_image / input_scale + input_zero_point\n",
    "\n",
    "  #   test_image = np.expand_dims(test_image, axis=0).astype(input_details[\"dtype\"])\n",
    "  #   interpreter.set_tensor(input_details[\"index\"], test_image)\n",
    "  #   interpreter.invoke()\n",
    "  #   output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
    "\n",
    "  #   predictions[i] = output.argmax()\n",
    "\n",
    "  # return predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test data\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"d:\\Miniconda\\envs\\tiny_cnn_3\\lib\\site-packages\\keras\\engine\\training.py\", line 1727, in test_function  *\n        return step_function(self, iterator)\n    File \"d:\\Miniconda\\envs\\tiny_cnn_3\\lib\\site-packages\\keras\\engine\\training.py\", line 1713, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"d:\\Miniconda\\envs\\tiny_cnn_3\\lib\\site-packages\\keras\\engine\\training.py\", line 1701, in run_step  **\n        outputs = model.test_step(data)\n    File \"d:\\Miniconda\\envs\\tiny_cnn_3\\lib\\site-packages\\keras\\engine\\training.py\", line 1667, in test_step\n        self.compute_loss(x, y, y_pred, sample_weight)\n    File \"d:\\Miniconda\\envs\\tiny_cnn_3\\lib\\site-packages\\keras\\engine\\training.py\", line 1052, in compute_loss\n        return self.compiled_loss(\n    File \"d:\\Miniconda\\envs\\tiny_cnn_3\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"d:\\Miniconda\\envs\\tiny_cnn_3\\lib\\site-packages\\keras\\losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"d:\\Miniconda\\envs\\tiny_cnn_3\\lib\\site-packages\\keras\\losses.py\", line 272, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"d:\\Miniconda\\envs\\tiny_cnn_3\\lib\\site-packages\\keras\\losses.py\", line 1990, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"d:\\Miniconda\\envs\\tiny_cnn_3\\lib\\site-packages\\keras\\backend.py\", line 5529, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 3) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [28], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Evaluate the model on the test data using `evaluate`\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEvaluate on test data\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m results \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mevaluate(test_ds, batch_size\u001b[39m=\u001b[39;49mBATCH_SIZE)\n\u001b[0;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mtest loss, test acc:\u001b[39m\u001b[39m\"\u001b[39m, results)\n",
      "File \u001b[1;32md:\\Miniconda\\envs\\tiny_cnn_3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileqchzdmm8.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__test_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"d:\\Miniconda\\envs\\tiny_cnn_3\\lib\\site-packages\\keras\\engine\\training.py\", line 1727, in test_function  *\n        return step_function(self, iterator)\n    File \"d:\\Miniconda\\envs\\tiny_cnn_3\\lib\\site-packages\\keras\\engine\\training.py\", line 1713, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"d:\\Miniconda\\envs\\tiny_cnn_3\\lib\\site-packages\\keras\\engine\\training.py\", line 1701, in run_step  **\n        outputs = model.test_step(data)\n    File \"d:\\Miniconda\\envs\\tiny_cnn_3\\lib\\site-packages\\keras\\engine\\training.py\", line 1667, in test_step\n        self.compute_loss(x, y, y_pred, sample_weight)\n    File \"d:\\Miniconda\\envs\\tiny_cnn_3\\lib\\site-packages\\keras\\engine\\training.py\", line 1052, in compute_loss\n        return self.compiled_loss(\n    File \"d:\\Miniconda\\envs\\tiny_cnn_3\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"d:\\Miniconda\\envs\\tiny_cnn_3\\lib\\site-packages\\keras\\losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"d:\\Miniconda\\envs\\tiny_cnn_3\\lib\\site-packages\\keras\\losses.py\", line 272, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"d:\\Miniconda\\envs\\tiny_cnn_3\\lib\\site-packages\\keras\\losses.py\", line 1990, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"d:\\Miniconda\\envs\\tiny_cnn_3\\lib\\site-packages\\keras\\backend.py\", line 5529, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 3) are incompatible\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data using `evaluate`\n",
    "print(\"Evaluate on test data\")\n",
    "results = model.evaluate(test_ds, batch_size=BATCH_SIZE)\n",
    "print(\"test loss, test acc:\", results)\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "# print(\"Generate predictions for 3 samples\")\n",
    "# predictions = model.predict(x_test[:3])\n",
    "# print(\"predictions shape:\", predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entity = \"susbrock\"\n",
    "\n",
    "\n",
    "# run = api.run(f\"{entity}/{PROJECT}/{run_id}\")\n",
    "# run.summary[\"test_accuracy\"] = results[1]\n",
    "# run.summary[\"test_loss\"] = results[0]\n",
    "# run.summary.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.07146961241960526, 0.9725490212440491]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(test_ds, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.07146961241960526, 0.9725490212440491]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'accuracy']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255/255 [==============================] - 4s 14ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[9.99985576e-01, 1.85058454e-07, 1.42443223e-05],\n",
       "       [9.99472201e-01, 2.19188769e-05, 5.05886273e-04],\n",
       "       [9.95712399e-01, 3.46672925e-04, 3.94088728e-03],\n",
       "       [9.99792516e-01, 1.46384230e-06, 2.06044424e-04],\n",
       "       [9.12287831e-01, 1.11508649e-03, 8.65971223e-02],\n",
       "       [9.99769866e-01, 7.42999373e-06, 2.22730276e-04],\n",
       "       [9.99756634e-01, 1.79441020e-06, 2.41542584e-04],\n",
       "       [9.51880872e-01, 6.56784250e-05, 4.80534136e-02],\n",
       "       [9.26768303e-01, 7.82446354e-04, 7.24493489e-02],\n",
       "       [9.97744679e-01, 5.07638606e-06, 2.25019827e-03],\n",
       "       [9.99657989e-01, 1.59539559e-05, 3.26103676e-04],\n",
       "       [9.99945402e-01, 1.05058079e-05, 4.41570519e-05],\n",
       "       [9.99861836e-01, 2.26321481e-05, 1.15518378e-04],\n",
       "       [9.99972820e-01, 2.71869226e-06, 2.44858802e-05],\n",
       "       [9.95697260e-01, 3.65680098e-05, 4.26622247e-03],\n",
       "       [9.93911028e-01, 1.24154531e-03, 4.84736403e-03],\n",
       "       [9.99712646e-01, 1.34641141e-06, 2.86010443e-04],\n",
       "       [9.99807656e-01, 2.10384860e-05, 1.71212145e-04],\n",
       "       [9.53149736e-01, 4.96937246e-05, 4.68006134e-02],\n",
       "       [9.95568097e-01, 6.24493230e-04, 3.80746322e-03],\n",
       "       [9.97621119e-01, 2.12315445e-05, 2.35755369e-03],\n",
       "       [9.99969125e-01, 9.25154438e-07, 2.99464464e-05],\n",
       "       [9.99992728e-01, 1.32146184e-07, 7.13997269e-06],\n",
       "       [9.99798000e-01, 1.67421695e-06, 2.00391296e-04],\n",
       "       [6.83869004e-01, 2.25560780e-05, 3.16108465e-01],\n",
       "       [9.99251783e-01, 2.22371250e-06, 7.45917845e-04],\n",
       "       [9.98141885e-01, 2.80650402e-05, 1.83006632e-03],\n",
       "       [9.95440722e-01, 4.04516992e-04, 4.15478321e-03],\n",
       "       [9.99995947e-01, 1.77482264e-06, 2.31676177e-06],\n",
       "       [9.99600947e-01, 6.17556452e-07, 3.98502219e-04],\n",
       "       [9.99984264e-01, 1.46637421e-06, 1.42694844e-05],\n",
       "       [9.99993801e-01, 3.36536061e-07, 5.79482321e-06],\n",
       "       [9.99975920e-01, 9.14739019e-07, 2.31248505e-05],\n",
       "       [9.96135473e-01, 4.98668851e-06, 3.85954743e-03],\n",
       "       [9.96222496e-01, 1.76124391e-04, 3.60131823e-03],\n",
       "       [9.99842763e-01, 3.93917617e-06, 1.53324625e-04],\n",
       "       [9.99235630e-01, 1.17343625e-05, 7.52530585e-04],\n",
       "       [9.98718023e-01, 2.70770921e-04, 1.01129012e-03],\n",
       "       [9.99919415e-01, 4.54667816e-06, 7.60654293e-05],\n",
       "       [9.98635709e-01, 8.36963081e-05, 1.28060603e-03],\n",
       "       [9.88664746e-01, 6.09849012e-05, 1.12743024e-02],\n",
       "       [2.49001876e-01, 5.17504755e-04, 7.50480592e-01],\n",
       "       [9.99879241e-01, 2.85537476e-06, 1.17829608e-04],\n",
       "       [9.84839201e-01, 6.53152601e-05, 1.50955888e-02],\n",
       "       [7.70403087e-01, 9.62075815e-02, 1.33389235e-01],\n",
       "       [9.88100290e-01, 1.06777030e-03, 1.08318813e-02],\n",
       "       [9.99595821e-01, 3.77853023e-04, 2.63134571e-05],\n",
       "       [9.95766521e-01, 6.12029326e-05, 4.17218124e-03],\n",
       "       [9.16566491e-01, 4.13628732e-05, 8.33921507e-02],\n",
       "       [9.99999881e-01, 1.01498765e-09, 7.89502081e-08],\n",
       "       [9.99889970e-01, 8.33661579e-06, 1.01679689e-04],\n",
       "       [9.99014854e-01, 4.01562602e-06, 9.81107354e-04],\n",
       "       [7.34444976e-01, 3.27404523e-05, 2.65522242e-01],\n",
       "       [9.99986053e-01, 1.48015129e-08, 1.39325239e-05],\n",
       "       [9.63674903e-01, 1.54291731e-04, 3.61707881e-02],\n",
       "       [9.94743109e-01, 1.12297203e-05, 5.24558080e-03],\n",
       "       [9.99996185e-01, 2.37933406e-07, 3.63287586e-06],\n",
       "       [9.16510522e-01, 1.42462377e-04, 8.33470672e-02],\n",
       "       [9.99996901e-01, 2.58214732e-06, 5.13590578e-07],\n",
       "       [9.99967098e-01, 6.81552501e-06, 2.60953766e-05],\n",
       "       [9.99992847e-01, 4.12424981e-07, 6.79857476e-06],\n",
       "       [9.72726643e-01, 1.95397856e-03, 2.53193416e-02],\n",
       "       [9.91556287e-01, 7.74162589e-03, 7.02121935e-04],\n",
       "       [9.99524713e-01, 1.06693151e-05, 4.64597659e-04],\n",
       "       [9.99997020e-01, 1.14163038e-06, 1.77845891e-06],\n",
       "       [9.99981999e-01, 5.53464588e-07, 1.73941899e-05],\n",
       "       [9.99998689e-01, 8.91297418e-08, 1.21531116e-06],\n",
       "       [9.99437988e-01, 4.55716236e-06, 5.57522988e-04],\n",
       "       [1.83221623e-01, 1.27386258e-04, 8.16650987e-01],\n",
       "       [9.99707878e-01, 2.20942711e-05, 2.70074321e-04],\n",
       "       [9.86220598e-01, 9.03930268e-05, 1.36890262e-02],\n",
       "       [9.98874366e-01, 9.64817067e-04, 1.60794400e-04],\n",
       "       [1.30088553e-01, 9.36278593e-05, 8.69817793e-01],\n",
       "       [9.99939680e-01, 3.37130905e-06, 5.69393123e-05],\n",
       "       [9.99977946e-01, 2.76477408e-06, 1.93569122e-05],\n",
       "       [9.99989390e-01, 1.17540083e-06, 9.42232327e-06],\n",
       "       [9.80336130e-01, 1.03112357e-03, 1.86327230e-02],\n",
       "       [9.99824345e-01, 1.82775932e-06, 1.73916225e-04],\n",
       "       [9.99765217e-01, 9.61673322e-06, 2.25108743e-04],\n",
       "       [9.99503493e-01, 1.81279802e-06, 4.94733686e-04],\n",
       "       [9.97662902e-01, 4.67056780e-05, 2.29034456e-03],\n",
       "       [6.77842736e-01, 8.53915408e-04, 3.21303338e-01],\n",
       "       [9.99972582e-01, 3.94057992e-07, 2.70963101e-05],\n",
       "       [9.56519485e-01, 5.73221814e-06, 4.34747860e-02],\n",
       "       [9.99749959e-01, 2.79014898e-06, 2.47300079e-04],\n",
       "       [9.78633463e-01, 5.74164142e-06, 2.13608053e-02],\n",
       "       [9.99953747e-01, 1.23923527e-07, 4.61465715e-05],\n",
       "       [9.99894857e-01, 7.73542779e-05, 2.77326781e-05],\n",
       "       [9.76356149e-01, 3.85036365e-05, 2.36053187e-02],\n",
       "       [9.97709990e-01, 8.43036389e-07, 2.28911545e-03],\n",
       "       [9.98555243e-01, 1.10973315e-05, 1.43369520e-03],\n",
       "       [9.99973297e-01, 2.32727984e-07, 2.64695773e-05],\n",
       "       [9.99999523e-01, 3.11931245e-08, 5.08068524e-07],\n",
       "       [9.99840856e-01, 1.02774393e-07, 1.58986149e-04],\n",
       "       [9.91237700e-01, 4.80344079e-06, 8.75746645e-03],\n",
       "       [9.99989986e-01, 7.15280237e-07, 9.30829447e-06],\n",
       "       [1.09269604e-06, 9.99988079e-01, 1.09059574e-05],\n",
       "       [1.06394657e-06, 9.99975562e-01, 2.34148411e-05],\n",
       "       [3.76018374e-06, 9.99866843e-01, 1.29322550e-04],\n",
       "       [3.54189410e-07, 9.99974728e-01, 2.48664710e-05],\n",
       "       [7.99055033e-06, 9.99975562e-01, 1.64120192e-05],\n",
       "       [3.72847126e-06, 9.99639034e-01, 3.57226498e-04],\n",
       "       [3.25229939e-06, 9.97867107e-01, 2.12969375e-03],\n",
       "       [1.86640204e-06, 9.99553502e-01, 4.44565085e-04],\n",
       "       [4.62051366e-06, 9.99731958e-01, 2.63417605e-04],\n",
       "       [1.52967114e-05, 9.99957800e-01, 2.69297216e-05],\n",
       "       [9.70371502e-07, 9.99971271e-01, 2.77255149e-05],\n",
       "       [2.34244840e-06, 9.99974847e-01, 2.27377768e-05],\n",
       "       [7.34653895e-07, 9.99949217e-01, 5.00493843e-05],\n",
       "       [8.71881821e-06, 9.99988914e-01, 2.37609561e-06],\n",
       "       [1.88077323e-07, 9.99976516e-01, 2.32270886e-05],\n",
       "       [9.24974643e-07, 9.99988437e-01, 1.06384823e-05],\n",
       "       [3.20692343e-05, 9.99961734e-01, 6.23136430e-06],\n",
       "       [4.88920796e-06, 9.99953270e-01, 4.17882511e-05],\n",
       "       [5.32848617e-07, 9.99971151e-01, 2.84161779e-05],\n",
       "       [3.13959663e-06, 9.99991536e-01, 5.33100638e-06],\n",
       "       [4.37956487e-06, 9.99965549e-01, 2.99900094e-05],\n",
       "       [2.68781537e-06, 9.99977469e-01, 1.97665449e-05],\n",
       "       [7.07965819e-06, 9.99986649e-01, 6.29302576e-06],\n",
       "       [1.74708293e-06, 9.99974728e-01, 2.34554300e-05],\n",
       "       [1.15562614e-06, 9.99963403e-01, 3.54331714e-05],\n",
       "       [2.94883807e-06, 9.99868393e-01, 1.28658838e-04],\n",
       "       [1.04917399e-06, 9.99986768e-01, 1.21377061e-05],\n",
       "       [2.38165399e-06, 9.99994755e-01, 2.91949459e-06],\n",
       "       [3.95210719e-07, 9.99986768e-01, 1.28797383e-05],\n",
       "       [1.18228763e-06, 9.99982357e-01, 1.63972036e-05],\n",
       "       [1.17271256e-05, 9.99700904e-01, 2.87420116e-04],\n",
       "       [9.09773462e-06, 9.99801338e-01, 1.89612256e-04],\n",
       "       [7.22234518e-06, 9.99921203e-01, 7.15398273e-05],\n",
       "       [5.18698187e-07, 9.99953508e-01, 4.60135852e-05],\n",
       "       [1.96856561e-07, 9.99940515e-01, 5.92142860e-05],\n",
       "       [5.20815911e-06, 9.99888420e-01, 1.06358922e-04],\n",
       "       [3.66687636e-05, 9.99761283e-01, 2.02049006e-04],\n",
       "       [2.55165196e-06, 9.99957323e-01, 4.02189326e-05],\n",
       "       [6.70611769e-07, 9.99980092e-01, 1.92205534e-05],\n",
       "       [1.39925714e-06, 9.99882102e-01, 1.16454736e-04],\n",
       "       [1.82110534e-06, 9.99989867e-01, 8.30202316e-06],\n",
       "       [5.61533443e-06, 9.99906778e-01, 8.75704573e-05],\n",
       "       [1.47287963e-07, 9.99985456e-01, 1.44700443e-05],\n",
       "       [4.11733436e-06, 9.99971509e-01, 2.43603281e-05],\n",
       "       [6.57296187e-06, 9.99966264e-01, 2.71579938e-05],\n",
       "       [2.00292698e-06, 9.99969721e-01, 2.82020756e-05],\n",
       "       [6.38421923e-02, 4.22226789e-04, 9.35735524e-01],\n",
       "       [2.46209122e-04, 2.20577003e-05, 9.99731719e-01],\n",
       "       [7.09786264e-06, 1.21026460e-05, 9.99980807e-01],\n",
       "       [1.77087786e-04, 3.12652446e-05, 9.99791682e-01],\n",
       "       [9.79852281e-04, 3.49312404e-06, 9.99016643e-01],\n",
       "       [8.21092748e-04, 1.38666528e-05, 9.99165058e-01],\n",
       "       [8.30604622e-05, 7.47310332e-06, 9.99909401e-01],\n",
       "       [2.85596674e-04, 6.74941111e-05, 9.99646902e-01],\n",
       "       [5.50367520e-04, 4.90549110e-06, 9.99444664e-01],\n",
       "       [5.08205485e-05, 8.34675157e-05, 9.99865651e-01],\n",
       "       [2.01667586e-04, 3.45756271e-05, 9.99763787e-01],\n",
       "       [1.83853568e-04, 4.97939473e-04, 9.99318242e-01],\n",
       "       [1.54562003e-03, 1.66613645e-05, 9.98437703e-01],\n",
       "       [1.36786639e-05, 1.92857406e-05, 9.99966979e-01],\n",
       "       [1.02795129e-04, 1.24853050e-05, 9.99884725e-01],\n",
       "       [3.36638514e-05, 8.60477667e-06, 9.99957681e-01],\n",
       "       [2.42980286e-01, 1.36810855e-03, 7.55651593e-01],\n",
       "       [2.19116482e-05, 1.00347925e-05, 9.99968052e-01],\n",
       "       [7.20164239e-01, 1.24158745e-03, 2.78594136e-01],\n",
       "       [4.01435937e-05, 3.65898882e-06, 9.99956250e-01],\n",
       "       [4.82067480e-05, 6.80389985e-06, 9.99945045e-01],\n",
       "       [3.02011467e-04, 1.58286584e-05, 9.99682188e-01],\n",
       "       [6.82011814e-05, 1.44432779e-05, 9.99917388e-01],\n",
       "       [2.06714776e-05, 6.11310588e-06, 9.99973178e-01],\n",
       "       [2.71033196e-06, 3.91355616e-06, 9.99993324e-01],\n",
       "       [9.13405046e-03, 9.81519261e-05, 9.90767777e-01],\n",
       "       [2.23621857e-04, 7.26538246e-07, 9.99775708e-01],\n",
       "       [4.42726487e-05, 9.97156803e-06, 9.99945760e-01],\n",
       "       [4.98066563e-03, 3.12542135e-04, 9.94706810e-01],\n",
       "       [6.06793270e-04, 3.63042382e-05, 9.99356925e-01],\n",
       "       [5.37161286e-05, 5.06500237e-06, 9.99941230e-01],\n",
       "       [2.53139910e-06, 2.93956109e-05, 9.99968052e-01],\n",
       "       [5.14071493e-04, 2.17051042e-04, 9.99268830e-01],\n",
       "       [3.26394620e-05, 1.11954041e-05, 9.99956131e-01],\n",
       "       [8.60579121e-07, 1.22753613e-06, 9.99997854e-01],\n",
       "       [7.57248272e-05, 7.43331498e-07, 9.99923468e-01],\n",
       "       [1.74218094e-05, 7.52082633e-06, 9.99975085e-01],\n",
       "       [4.33373172e-03, 4.26302950e-06, 9.95662034e-01],\n",
       "       [9.84191662e-04, 1.67947874e-05, 9.98999059e-01],\n",
       "       [1.55751259e-05, 1.56814967e-05, 9.99968767e-01],\n",
       "       [3.42602584e-06, 1.68229531e-06, 9.99994874e-01],\n",
       "       [3.60524755e-05, 2.54929319e-06, 9.99961376e-01],\n",
       "       [6.07595837e-07, 9.86199484e-06, 9.99989510e-01],\n",
       "       [4.09085369e-05, 6.60600926e-05, 9.99893069e-01],\n",
       "       [2.64749600e-04, 1.11289637e-05, 9.99724090e-01],\n",
       "       [5.29113633e-04, 3.30396579e-05, 9.99437869e-01],\n",
       "       [2.28997906e-05, 1.33258186e-06, 9.99975801e-01],\n",
       "       [1.94784752e-05, 2.15412729e-06, 9.99978423e-01],\n",
       "       [1.95233151e-03, 4.12732697e-05, 9.98006403e-01],\n",
       "       [5.28781020e-05, 1.90492904e-06, 9.99945164e-01],\n",
       "       [7.27586303e-05, 1.95261800e-05, 9.99907732e-01],\n",
       "       [2.00701732e-04, 5.84293739e-05, 9.99740899e-01],\n",
       "       [2.94420788e-05, 3.94024792e-05, 9.99931097e-01],\n",
       "       [6.00049680e-04, 1.48908832e-04, 9.99251068e-01],\n",
       "       [9.98460655e-06, 4.97144856e-06, 9.99985099e-01],\n",
       "       [5.75496051e-05, 7.39582856e-06, 9.99935031e-01],\n",
       "       [5.90873315e-05, 3.90550986e-06, 9.99937057e-01],\n",
       "       [1.47202328e-01, 8.03332005e-05, 8.52717400e-01],\n",
       "       [1.66303682e-04, 7.85818793e-06, 9.99825895e-01],\n",
       "       [2.10219223e-04, 2.54497263e-06, 9.99787271e-01],\n",
       "       [3.40674887e-03, 3.22866690e-04, 9.96270418e-01],\n",
       "       [1.03425773e-04, 1.12400551e-06, 9.99895453e-01],\n",
       "       [5.63282119e-05, 2.64192022e-06, 9.99940991e-01],\n",
       "       [1.38130602e-07, 1.14152999e-06, 9.99998689e-01],\n",
       "       [1.00580270e-04, 6.61347440e-05, 9.99833226e-01],\n",
       "       [1.60970012e-05, 3.98540396e-06, 9.99979973e-01],\n",
       "       [9.25632139e-06, 9.08893412e-07, 9.99989867e-01],\n",
       "       [3.22135951e-04, 4.41017619e-05, 9.99633789e-01],\n",
       "       [1.59449773e-04, 1.16819983e-05, 9.99828815e-01],\n",
       "       [3.01362830e-04, 1.31154913e-04, 9.99567449e-01],\n",
       "       [1.50164306e-05, 1.49542996e-06, 9.99983430e-01],\n",
       "       [1.33544218e-05, 1.53707219e-06, 9.99985099e-01],\n",
       "       [5.29412646e-05, 3.64704174e-05, 9.99910593e-01],\n",
       "       [3.41042432e-05, 7.79820897e-04, 9.99186099e-01],\n",
       "       [2.15164619e-05, 6.48507194e-05, 9.99913573e-01],\n",
       "       [3.41717373e-06, 3.19615756e-05, 9.99964595e-01],\n",
       "       [1.28690837e-04, 5.08039120e-05, 9.99820530e-01],\n",
       "       [2.08475452e-04, 9.24354117e-06, 9.99782264e-01],\n",
       "       [1.12834300e-06, 1.75721493e-06, 9.99997139e-01],\n",
       "       [1.88334980e-05, 1.36634162e-05, 9.99967456e-01],\n",
       "       [4.39358468e-04, 5.27882912e-06, 9.99555290e-01],\n",
       "       [1.38983480e-03, 3.51898188e-06, 9.98606622e-01],\n",
       "       [3.59218530e-02, 2.47642980e-04, 9.63830471e-01],\n",
       "       [9.21017468e-01, 1.74690736e-04, 7.88078010e-02],\n",
       "       [9.90710646e-07, 8.37178277e-07, 9.99998212e-01],\n",
       "       [1.51663669e-04, 3.74055026e-06, 9.99844551e-01],\n",
       "       [4.13178932e-05, 1.81191808e-05, 9.99940515e-01],\n",
       "       [1.42900579e-04, 5.22644723e-06, 9.99851823e-01],\n",
       "       [3.32645395e-05, 1.12032903e-05, 9.99955535e-01],\n",
       "       [4.06225399e-06, 3.57966542e-06, 9.99992371e-01],\n",
       "       [4.79963965e-05, 1.68460110e-06, 9.99950290e-01],\n",
       "       [2.32778202e-05, 8.62234010e-05, 9.99890447e-01],\n",
       "       [3.39724170e-06, 6.42686587e-07, 9.99995947e-01],\n",
       "       [1.62935313e-02, 1.56277063e-04, 9.83550191e-01],\n",
       "       [5.09181852e-03, 1.31015127e-04, 9.94777203e-01],\n",
       "       [1.25404622e-04, 2.46932473e-06, 9.99872088e-01],\n",
       "       [1.88498129e-03, 8.32634323e-05, 9.98031795e-01],\n",
       "       [1.91018971e-05, 2.69987377e-05, 9.99953866e-01],\n",
       "       [4.80318304e-05, 5.90897398e-04, 9.99361098e-01],\n",
       "       [4.88864165e-03, 1.87602214e-04, 9.94923770e-01],\n",
       "       [1.74199119e-02, 3.33066238e-03, 9.79249418e-01],\n",
       "       [9.68100620e-04, 2.56151852e-05, 9.99006331e-01],\n",
       "       [1.99409391e-04, 3.46759552e-05, 9.99765933e-01],\n",
       "       [9.04496468e-04, 2.80257773e-05, 9.99067485e-01],\n",
       "       [1.00502903e-05, 1.97532845e-05, 9.99970198e-01],\n",
       "       [8.65043879e-01, 2.89824129e-05, 1.34927124e-01],\n",
       "       [1.30511296e-04, 4.58907343e-05, 9.99823630e-01],\n",
       "       [2.11792742e-03, 2.32359139e-06, 9.97879744e-01],\n",
       "       [1.78035870e-01, 1.02095837e-04, 8.21862042e-01],\n",
       "       [8.31901561e-05, 8.35468563e-06, 9.99908447e-01],\n",
       "       [1.96595269e-04, 1.37991274e-05, 9.99789655e-01],\n",
       "       [1.16039859e-02, 1.08664761e-04, 9.88287389e-01],\n",
       "       [9.86873567e-01, 4.12834197e-06, 1.31222662e-02]], dtype=float32)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions = model.predict(test_ds)\n",
    "test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_pred_ids = test_predictions.argmax(axis=1)\n",
    "len(top_pred_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 0], dtype=int64)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_pred_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [y for x, y in test_ds]\n",
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = np.concatenate([y for x, y in test_ds], axis=0)\n",
    "len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=int32, numpy=\n",
       "array([[ 93,   0,   3],\n",
       "       [  0,  46,   0],\n",
       "       [  4,   0, 109]])>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_mtx = tf.math.confusion_matrix(y_true, top_pred_ids, num_classes=classes)\n",
    "    # list(ds_test.map(lambda x, y: y)),\n",
    "    # predict_class_label_number(test_data),\n",
    "    # num_classes=len(label_names))\n",
    "    \n",
    "confusion_mtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.heatmap(confusion_mtx, xticklabels=labels, yticklabels=labels, \n",
    "#               annot=True, fmt='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_confusion_matrix(cm, labels):\n",
    "  plt.figure(figsize=(6, 6))\n",
    "  sns.heatmap(cm, xticklabels=labels, yticklabels=labels, \n",
    "              annot=True, fmt='g')\n",
    "  plt.xlabel('Prediction')\n",
    "  plt.ylabel('Label')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show_confusion_matrix(confusion_mtx, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 0], dtype=int64)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_pred_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code reserved for troubleshooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mobilenet_v1_keras(input_shape, classes=classes, alpha=alpha):\n",
    "    model = tf.keras.applications.mobilenet.MobileNet(\n",
    "        input_shape=input_shape,\n",
    "        alpha=alpha,\n",
    "        depth_multiplier=1,\n",
    "        dropout=0.001,\n",
    "        include_top=True,\n",
    "        weights=None, #'imagenet'\n",
    "        input_tensor=None,\n",
    "        pooling=None,\n",
    "        classes=classes,\n",
    "        classifier_activation='softmax',\n",
    "        #**kwargs\n",
    "    )\n",
    "\n",
    "    #model._name = model.name + \"_keras\" # model.name cannot be overritten\n",
    "\n",
    "    return model\n",
    "    #model = mobilenet_v1_keras((IMG_WIDTH, IMG_HEIGHT, 3), classes=classes, alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ[\"WANDB_MODE\"] = \"online\"\n",
    "def train_model(model):\n",
    "\n",
    "        # solve issue from: https://github.com/wandb/wandb/issues/3536\n",
    "        # if len(wandb.patched[\"tensorboard\"]) > 0:\n",
    "        #         wandb.tensorboard.unpatch()\n",
    "                \n",
    "        # Configure Tensorboard root log directory to read the debugging information\n",
    "        #wandb.tensorboard.patch(root_logdir=root_logdir)\n",
    "        # wandb.tensorboard.patch(root_logdir=\"wandb.run.dir\")\n",
    "        \n",
    "        # wandb.init(\n",
    "        #         # Set the project where this run will be logged\n",
    "        #         project=PROJECT, \n",
    "        #         # Track hyperparameters and run metadata\n",
    "        #         #config={\n",
    "        #         #\"learning_rate\": LR,\n",
    "        #         #\"epochs\": EPOCHS,\n",
    "        #         #},\n",
    "        #         sync_tensorboard=True\n",
    "        #         )\n",
    "\n",
    "\n",
    "\n",
    "        # config = wandb.config\n",
    "        # # Specify the configuration variables\n",
    "        # config.batch_size = BATCH_SIZE\n",
    "        # config.dropout =DROPOUT\n",
    "        # config.learn_rate = LR\n",
    "        # #config.decay = 1e-6\n",
    "        # #config.momentum = 0.9\n",
    "        # config.epochs = EPOCHS\n",
    "        # config.classes = classes\n",
    "        \n",
    "\n",
    "        # enable Tensorflow Debugging\n",
    "        #tf.debugging.experimental.enable_dump_debug_info(\"./logs/debug\", \n",
    "        #        tensor_debug_mode=\"FULL_HEALTH\", circular_buffer_size=-1)\n",
    "\n",
    "        #model = mobilenet\n",
    "        model.compile(optimizer='adam',\n",
    "                        loss='sparse_categorical_crossentropy',\n",
    "                        metrics=['accuracy'])\n",
    "\n",
    "        logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "        #tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir= wandb.run.dir, histogram_freq=10, update_freq=\"epoch\") #, profile_batch=\"10, 20\")\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir= logdir) #, histogram_freq=1)\n",
    "        #wandb_callback = WandbCallback()# input_type=\"image\", labels=labels) #, validation_data = val_ds.as_numpy_iterator())\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor=\"val_accuracy\", patience= early_stopping_patience)\n",
    "\n",
    "        #checkpoint = ModelCheckpoint(\"my_tiny_model\", save_weights_only=True)\n",
    "\n",
    "        callbacks =[\n",
    "                #tensorboard_callback,\n",
    "                #wandb_callback,\n",
    "                #WandbMetricsLogger(),\n",
    "                #checkpoint,\n",
    "                #early_stopping\n",
    "        ]\n",
    "\n",
    "        history = model.fit(train_ds,\n",
    "                epochs=EPOCHS, \n",
    "                validation_data=val_ds, \n",
    "                callbacks=callbacks\n",
    "        )\n",
    "\n",
    "        # wandb.log({\n",
    "        #         \"loss\": history.history[\"loss\"],\n",
    "        #         \"accuracy\": history.history[\"accuracy\"],\n",
    "        #         \"val_loss\": history.history[\"val_loss\"],\n",
    "        #         \"val_accuracy\": history.history[\"val_accuracy\"],                                \n",
    "        # })\n",
    "        \n",
    "        #wandb.finish()\n",
    "        return history, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer='adam',\n",
    "#                 loss='sparse_categorical_crossentropy',\n",
    "#                 metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tiny_cnn_3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "faf61258749f39ee396d68e4f37ccf2f6c623aa999bdf06f87f249fa5d734f8f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
