{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Workbench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, math, datetime\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import PIL\n",
    "import PIL.Image\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Conv2D,DepthwiseConv2D, MaxPooling2D, AvgPool2D, GlobalAveragePooling2D, BatchNormalization, Concatenate\n",
    "from tensorflow.keras.layers import ReLU\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    " \n",
    "# Import the necessary MLTK APIs\n",
    "from mltk.core import view_model, summarize_model, profile_model\n",
    "\n",
    "# import workbench.config.config\n",
    "from workbench.config.config import initialize\n",
    "from workbench.utils.utils import create_filepaths\n",
    "from workbench.utils.utils import parse_model_name\n",
    "\n",
    "#from dotenv import load_dotenv\n",
    "import wandb\n",
    "from wandb.keras import WandbMetricsLogger\n",
    "#import deeplake\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 2.10.0\n",
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "print(\"Tensorflow version \" + tf.__version__)\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# Confirm that TensorFlow can access GPU\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if not device_name:\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DANGER ZONE: Disable warning messages\n",
    "\n",
    "import absl.logging\n",
    "absl.logging.set_verbosity(absl.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# set the random seeds\n",
    "os.environ[\"TF_CUDNN_DETERMINISTIC\"]= \"1\"\n",
    "random.seed(hash(\"setting_random seeds\") % 2**32 -1)\n",
    "np.random.seed(hash(\"improves reproducibility\") % 2**32 -1)\n",
    "tf.random.set_seed(hash(\"by removing stochasticity\") %2 *32 -1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "model_name = \"mobilenetv2_0.2_96_c3_o3_keras\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:\\tinyml\\tiny_cnn\\models\n"
     ]
    }
   ],
   "source": [
    "models_path, models_summary_path, models_image_path, models_layer_df_path, models_tf_path, models_tflite_path, models_tflite_opt_path = create_filepaths(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(models_tf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "global base_model_name\n",
    "global alpha\n",
    "global resolution\n",
    "global channels\n",
    "global classes\n",
    "global variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name, alpha, resolution, channels, classes, variation = model_name.split(\"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = float(alpha)\n",
    "resolution = int(resolution)\n",
    "classes = int(classes.strip(\"o\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a Tensorboard session\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#os.environ['WANDB_NOTEBOOK_NAME'] = 'Experiment Workbench'\n",
    "\n",
    "IMG_HEIGHT = resolution\n",
    "IMG_WIDTH = resolution\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "#LOGGING_STEPS = 64\n",
    "LR = 0.001\n",
    "DROPOUT = 0.5\n",
    "\n",
    "PROJECT = base_model_name\n",
    "SEED = 42\n",
    "tf.random.set_seed(SEED) # global seed for tensorflow random parts, like dropout\n",
    "shuffle_seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the Lemon Quality Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = Path.cwd().joinpath(\"datasets\", \"lemon_dataset\")\n",
    "dataset_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemon_quality_dataset(dataset_path, img_width, img_height, batch_size, normalize=True):\n",
    "    \"\"\" Fetches the lemon quality dataset and prints dataset info. It normalizes the image data to range [0,1] by default.\n",
    "\n",
    "    Args: \n",
    "        dataset_path (Path): the file location of the dataset. Subfolders \"train\", \"test\", and \"val\" are expected.\n",
    "        normalize (boolean): Normalizes the image data to range [0, 1]. Default: True\n",
    "\n",
    "    Returns:\n",
    "        (train_ds, val_ds, test_ds, class_names) (tuple(tf.datasets)): Tensorflow datasets for train, validation and test.\n",
    "    \n",
    "    \"\"\"\n",
    "    if dataset_path.exists():\n",
    "        try:\n",
    "            train_dir = dataset_path.joinpath(\"train\")\n",
    "            val_dir = dataset_path.joinpath( \"val\")\n",
    "            test_dir = dataset_path.joinpath( \"test\")\n",
    "        except:\n",
    "            print(f\"Please check the folder structure of {dataset_path}.\")\n",
    "            raise\n",
    "\n",
    "    print(\"Preparing training dataset...\")        \n",
    "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        train_dir,\n",
    "        subset=None,\n",
    "        seed=shuffle_seed,\n",
    "        image_size=((img_height, img_width)),\n",
    "        batch_size=batch_size)\n",
    "    \n",
    "\n",
    "    class_names = train_ds.class_names\n",
    "\n",
    "\n",
    "    print(\"Preparing validation dataset...\")    \n",
    "    val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        val_dir,\n",
    "        subset=None,\n",
    "        seed=shuffle_seed,\n",
    "        image_size=(img_height, img_width),\n",
    "        batch_size=batch_size)\n",
    "    \n",
    "\n",
    "    print(\"Preparing test dataset...\")    \n",
    "    test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        test_dir,\n",
    "        subset=None,\n",
    "        seed=shuffle_seed,\n",
    "        image_size=(img_height, img_width),\n",
    "        batch_size=batch_size)\n",
    "    \n",
    "    # Create a data augmentation stage with horizontal flipping, rotations, zooms\n",
    "    data_augmentation = keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "            tf.keras.layers.RandomRotation(0.1),\n",
    "            tf.keras.layers.RandomZoom(0.1),\n",
    "        ]\n",
    "        )\n",
    "\n",
    "    #train_ds= train_ds.map(lambda x, y: (data_augmentation(x), y))\n",
    "\n",
    "    \n",
    "    # Normalize the data to the range [0, 1]\n",
    "    if normalize:\n",
    "        normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "\n",
    "        train_ds= train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "        val_ds= val_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "        test_ds= test_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    print (f\"Class names: {class_names}\")\n",
    "    print(train_ds.element_spec)\n",
    "    print(f\"Normalize: {normalize}\")\n",
    "    return (train_ds, val_ds, test_ds, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing training dataset...\n",
      "Found 2021 files belonging to 3 classes.\n",
      "Preparing validation dataset...\n",
      "Found 252 files belonging to 3 classes.\n",
      "Preparing test dataset...\n",
      "Found 255 files belonging to 3 classes.\n",
      "Class names: ['bad_quality', 'empty_background', 'good_quality']\n",
      "(TensorSpec(shape=(None, 96, 96, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))\n",
      "Normalize: True\n"
     ]
    }
   ],
   "source": [
    "train_ds, val_ds, test_ds, labels = get_lemon_quality_dataset(dataset_path, IMG_WIDTH, IMG_HEIGHT, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(train_ds.as_numpy_iterator())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 3 classes.\n"
     ]
    }
   ],
   "source": [
    "classes = len(labels)\n",
    "print(f\"The dataset contains {classes } classes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#element[0].shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mobilenet = tf.keras.applications.mobilenet.MobileNet(\n",
    "#     input_shape=(96,96,3),\n",
    "#     alpha=0.25,\n",
    "#     depth_multiplier=1,\n",
    "#     dropout=DROPOUT,\n",
    "#     include_top=True,\n",
    "#     weights= None, #'imagenet',\n",
    "#     input_tensor=None,\n",
    "#     pooling=None,\n",
    "#     classes=classes,\n",
    "#     classifier_activation='softmax',\n",
    "#     #**kwargs\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODELNAME = mobilenet.name\n",
    "# print(MODELNAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = os.path.join(\"logs\", model_name, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "root_logdir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.13.6'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# optimize the data flow\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.prefetch(AUTOTUNE)\n",
    "val_ds = val_ds.prefetch(AUTOTUNE)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_MODE\"] = \"online\"\n",
    "def train_model():\n",
    "\n",
    "        # solve issue from: https://github.com/wandb/wandb/issues/3536\n",
    "        if len(wandb.patched[\"tensorboard\"]) > 0:\n",
    "                wandb.tensorboard.unpatch()\n",
    "                \n",
    "        # Configure Tensorboard root log directory to read the debugging information\n",
    "        wandb.tensorboard.patch(root_logdir=root_logdir)\n",
    "        # wandb.tensorboard.patch(root_logdir=\"wandb.run.dir\")\n",
    "        \n",
    "        wandb.init(\n",
    "                # Set the project where this run will be logged\n",
    "                project=PROJECT, \n",
    "                # Track hyperparameters and run metadata\n",
    "                #config={\n",
    "                #\"learning_rate\": LR,\n",
    "                #\"epochs\": EPOCHS,\n",
    "                #},\n",
    "                sync_tensorboard=True\n",
    "                )\n",
    "\n",
    "\n",
    "\n",
    "        config = wandb.config\n",
    "        # Specify the configuration variables\n",
    "        config.batch_size = BATCH_SIZE\n",
    "        config.dropout =DROPOUT\n",
    "        config.learn_rate = LR\n",
    "        #config.decay = 1e-6\n",
    "        #config.momentum = 0.9\n",
    "        config.epochs = EPOCHS\n",
    "        config.classes = classes\n",
    "        \n",
    "\n",
    "        # enable Tensorflow Debugging\n",
    "        #tf.debugging.experimental.enable_dump_debug_info(\"./logs/debug\", \n",
    "        #        tensor_debug_mode=\"FULL_HEALTH\", circular_buffer_size=-1)\n",
    "\n",
    "        #model = mobilenet\n",
    "        model.compile(optimizer='adam',\n",
    "                        loss='sparse_categorical_crossentropy',\n",
    "                        metrics=['accuracy'])\n",
    "\n",
    "        logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir= wandb.run.dir, histogram_freq=10, update_freq=\"epoch\") #, profile_batch=\"10, 20\")\n",
    "        #tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir= logdir, histogram_freq=1)\n",
    "        #wandb_callback = WandbCallback()# input_type=\"image\", labels=labels) #, validation_data = val_ds.as_numpy_iterator())\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor=\"val_accuracy\", patience=50)\n",
    "\n",
    "        checkpoint = ModelCheckpoint(\"my_tiny_model\", save_weights_only=True)\n",
    "\n",
    "        callbacks =[\n",
    "                #tensorboard_callback,\n",
    "                #wandb_callback,\n",
    "                WandbMetricsLogger(),\n",
    "                #checkpoint,\n",
    "                #early_stopping\n",
    "        ]\n",
    "\n",
    "        history = model.fit(train_ds,\n",
    "                epochs=EPOCHS, \n",
    "                validation_data=val_ds, \n",
    "                callbacks=callbacks\n",
    "        )\n",
    "\n",
    "        # wandb.log({\n",
    "        #         \"loss\": history.history[\"loss\"],\n",
    "        #         \"accuracy\": history.history[\"accuracy\"],\n",
    "        #         \"val_loss\": history.history[\"val_loss\"],\n",
    "        #         \"val_accuracy\": history.history[\"val_accuracy\"],                                \n",
    "        # })\n",
    "        \n",
    "        wandb.finish()\n",
    "        return history, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msusbrock\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>i:\\tinyml\\tiny_cnn\\wandb\\run-20221210_090144-ngfxam3e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/susbrock/mobilenetv2/runs/ngfxam3e\" target=\"_blank\">fancy-morning-14</a></strong> to <a href=\"https://wandb.ai/susbrock/mobilenetv2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " 6/64 [=>............................] - ETA: 12s - loss: 1.1259 - accuracy: 0.4010WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0705s vs `on_train_batch_end` time: 0.1270s). Check your callbacks.\n",
      "64/64 [==============================] - 198s 447ms/step - loss: 0.8403 - accuracy: 0.5512 - val_loss: 1.0910 - val_accuracy: 0.4444\n",
      "Epoch 2/50\n",
      "61/64 [===========================>..] - ETA: 26s - loss: 0.5241 - accuracy: 0.7541"
     ]
    }
   ],
   "source": [
    "#wandb.finish()\n",
    "history, model = train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#history.history[\"loss\"]\n",
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('tiny_cnn_5')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4b362e3f7439b35b0bc0fa2e0c3a5b3cc0154c779cd4216b658aa3553d61d067"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
